{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tpreviews import GetReviews #to use the tpreviews module place the tprevies.py file in the working directory\n",
    "import pandas as pd\n",
    "from afinn import Afinn\n",
    "\n",
    "#ONLY EXECUTE ONCE!! We gathered the data on 24-08-2018\n",
    "\n",
    "#d = GetReviews() \n",
    "#d['www.jensens.com'] = '469070ae0000640005000ddd'\n",
    "#print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#retured_dict = d.gather_data('dk') #Here we specify that we want reviews in Danish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(retured_dict) \n",
    "#df.to_csv(\"Trustpilot_Jensens_180824.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read .csv and remove the reviewer name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"Trustpilot_Jensens_180824.csv\")\n",
    "\n",
    "#remove the reviewer name and save again\n",
    "\n",
    "#data = data.drop('reviewerName', axis = 1)\n",
    "#data.to_csv(\"Trustpilot_Jensens_180824_nonames.csv\", index = False)\n",
    "\n",
    "#read the new data\n",
    "data = pd.read_csv(\"Trustpilot_Jensens_180824_nonames.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3922, 6) \n",
      "\n",
      "Index(['headline', 'inLanguage', 'datePublished', 'reviewBody', 'ratingValue',\n",
      "       'Company'],\n",
      "      dtype='object') \n",
      "\n",
      "headline         object\n",
      "inLanguage       object\n",
      "datePublished    object\n",
      "reviewBody       object\n",
      "ratingValue       int64\n",
      "Company          object\n",
      "dtype: object \n",
      "\n",
      "                               headline inLanguage         datePublished  \\\n",
      "0  Helt fantastisk middagsoplevelse på…         da  2018-08-23T12:28:51Z   \n",
      "1                        Fin til prisen         da  2018-08-22T11:16:41Z   \n",
      "2                           God service         da  2018-08-22T11:14:21Z   \n",
      "3                 Ganske fin til prisen         da  2018-08-22T11:13:58Z   \n",
      "4                       Skidt oplevelse         da  2018-08-22T06:25:14Z   \n",
      "\n",
      "                                          reviewBody  ratingValue  \\\n",
      "0  Helt fantastisk middagsoplevelse på Jensens Bø...            5   \n",
      "1         Ganske fin mad og okay service til prisen.            3   \n",
      "2  Ok god service. Bøffen var lidt saltet og sala...            3   \n",
      "3               Ganske fin til prisen. Venlig tjener            3   \n",
      "4  Vi har nu spist hos jer. Reklamens magt, om ma...            1   \n",
      "\n",
      "           Company  \n",
      "0  www.jensens.com  \n",
      "1  www.jensens.com  \n",
      "2  www.jensens.com  \n",
      "3  www.jensens.com  \n",
      "4  www.jensens.com  \n"
     ]
    }
   ],
   "source": [
    "print(data.shape, '\\n')\n",
    "print(data.columns, '\\n') #no reviewer name anymore!\n",
    "print(data.dtypes, '\\n')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Compute new variables for descriptives and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Afinn scores for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn = Afinn()\n",
    "scores = [] #initiate score list\n",
    "\n",
    "for i in range(0, data.shape[0]):\n",
    "    score = afinn.score(str(data['reviewBody'].iloc[i])) #Compute an afinn score for each review in the dataset\n",
    "    scores.append(score) #Append score to score list\n",
    "\n",
    "data['Afinn_score'] = scores #Create a new variable with the afinn scores in our dataset\n",
    "data['Afinn_binary'] = data['Afinn_score'].apply(lambda x: 1 if x>0 else 0) #Dichotomize  \n",
    "#(perhaps determine cutpoint where afinn score has rating 3?--> no then fit will be perfect for afinn....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3922.000000\n",
       "mean        0.188169\n",
       "std         0.390897\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         1.000000\n",
       "Name: Afinn_binary, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Afinn_binary'].describe() #See if dichotomization worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3922.000000\n",
      "mean        0.359001\n",
      "std         0.479769\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "Name: sept14, dtype: float64\n",
      "count    3922.000000\n",
      "mean        0.261346\n",
      "std         0.439424\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "Name: before_sept14, dtype: float64\n",
      "count    3922.000000\n",
      "mean        0.380418\n",
      "std         0.485552\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "Name: after_sept14, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#First compute time variables\n",
    "data['datetime'] = pd.to_datetime(data['datePublished'].astype(str))\n",
    "data['year'] = data['datetime'].dt.year\n",
    "data['month'] = data['datetime'].dt.month\n",
    "data['day'] = data['datetime'].dt.day\n",
    "\n",
    "#convert ratings to -1, 0, 1\n",
    "def func(x):\n",
    "    if  x <= 2:\n",
    "        return -1\n",
    "    elif x == 3:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "data['rating_c'] = data['ratingValue'].apply(func) #split in 3 \n",
    "data['rating_binary'] = data['rating_c'].apply(lambda x: 1 if x>0 else 0) #dichotomize ratings to 0-1\n",
    "\n",
    "\n",
    "#Set the index without throwing away variables\n",
    "data.set_index(['year', 'month', 'day'], inplace = True, drop = False)\n",
    "\n",
    "#Compute day means, dat sums, day counts, other variables\n",
    "data['daymean_afinn'] = data.groupby(level=['year', 'month', 'day']).mean()['Afinn_score'] #daymean of Afinn_score\n",
    "data['daymean_rating'] = data.groupby(level=['year', 'month', 'day']).mean()['rating_c'] #daymean of ratings\n",
    "data['no_rev'] = data.groupby(level=['year', 'month', 'day']).count()['reviewBody'] #number of reviews per day\n",
    "data['len_rev'] = data['reviewBody'].apply(len) #length of a review\n",
    "data['daysum_afinn'] = data.groupby(level=['year', 'month', 'day']).sum()['Afinn_score'] #compute summed dayscore\n",
    "data['daysum_rating'] = data.groupby(level=['year', 'month', 'day']).sum()['rating_c'] \n",
    "\n",
    "#Make a variable indicating whether the review is from sept 2014, before and after.\n",
    "data['sept14'] = ((data['datetime'] > '2014-08-31') & (data['datetime'] < '2014-10-01')).replace(False, 0)\n",
    "data['before_sept14'] = (data['datetime'] < '2014-09-01').replace(False, 0)\n",
    "data['after_sept14'] = (data['datetime'] > '2014-09-30').replace(False, 0)\n",
    "\n",
    "print(data['sept14'].describe()) #35.9% of all reviews written in sept 2014\n",
    "print(data['before_sept14'].describe())#26% of all reviews written before sept 2014\n",
    "print(data['after_sept14'].describe())#38% of all review written after sept 2014\n",
    "\n",
    "\n",
    "#Create categorical variable with before during and after 2014 values\n",
    "time = data[['before_sept14', 'sept14', 'after_sept14']].idxmax(axis=1)\n",
    "time = time.astype(dtype = 'category')\n",
    "\n",
    "time = time.cat.rename_categories(['After Sept 2014', 'Before Sept 2014', 'Sept 2014']) #rename categories\n",
    "time= time.cat.reorder_categories(['Before Sept 2014', 'Sept 2014', 'After Sept 2014']) #reorder categories\n",
    "data['time_cat'] = time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "## Descriptives and figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the cumulative mean of the ratings and plot those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jolien\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  warnings.warn(\"This figure includes Axes that are not compatible \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAw4AAAFeCAYAAAAsU0iTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xd8FHX+x/HXd3fTQ4Ak1AAJHelKqIoigr17lhNFwXJ4YjlFD+Wn53n2s50Nxd5BPUBFbNgOFVFKQFroNZQUIL3tfn9/bFgJScgGkmzK+/l4zGN2Z74z8/kmlPnMfIux1iIiIiIiInI4jkAHICIiIiIidZ8SBxERERERqZQSBxERERERqZQSBxERERERqZQSBxERERERqZQSBxERERERqZQSBxERERERqVStJw7GmO+NMfnGmOySJbm2YxARERERkaoJ1BuHidbayJKle4BiEBERERERP6mpkoiIiIiIVCpQicPDxpg0Y8xPxpgRAYpBRERERET8ZKy1tXtBYwYDq4BC4DLgOaC/tXbDIeWuB64H6Nmz54CVK1fWapwiIiIiIo2E8adQrb9xsNYutNZmWWsLrLVvAj8BZ5ZTbpq1NtFamxgWFlbbYYqIiIiIyEHqQh8Hi59ZjoiIiIiIBEatJg7GmGbGmNOMMaHGGJcxZgxwIvBlbcYhIiIiIiJV46rl6wUBDwA9ADewBjjfWqu5HERERERE6rBaTRystanAwNq8poiIiIiIHL260MdBRERERETqOCUOIiIiIiJSKSUOIiIiIiJSKSUOIiIiIiJSKSUOIiIiIiJSKSUOIiIiIiJSKSUOIiIiIiJSKSUOIiIiIiJSKSUOIiIiIiJSKSUOIiIiUkp6ejpnnnkm3bt3p2/fvlx44YWkpqaWKXf11VfTrl07+vfvT//+/XnwwQd9+3bv3s2pp55Kt27d6NevHwsXLqzNKohIDVDiICIiIqUYY7jzzjtJTk5m+fLldO7cmcmTJ5dbdvLkySQlJZGUlMSUKVN82++66y5OPPFE1q5dy/PPP8+YMWOw1tZWFUSkBihxEBERkVKio6MZMWKE7/uQIUPYsmVLlc7xwQcfMGHCBABOOOEEQkNDWbRoUXWGKSK1TImDiIiIVMjj8TB16lTOPffccvc/+eST9OnTh/PPP5/Vq1cD3qZO1lpiY2N95Tp06MC2bdtqJWYRqRmuQAcgIiIidddNN91EZGQkEydOLLPvwQcfpE2bNjgcDt566y1OP/10Nm7cWOVrTJs2jWnTpgGwZs0aevTocdRxizQkmzdvJi0tLdBhYOpDe8PExESr15siIiK1a9KkSSxfvpxPP/2UkJCQSsvHxMSwZMkS4uPjiYiIYMuWLb63Dr179+b1119n4MCBhz1HYmKimjSJHKIW/l4YfwrpjYOIiIiUMWXKFBYvXsxnn31WYdKwY8cO4uLiAPjyyy9xOp2+7xdffDEvvvgi//d//8ePP/5IXl4eAwYMOLqgCgpg2DAwBhyO0uvq3lZT5z3085EsR3v8wYu13sXj+eNzTW/btQsiI+G666Bfv6P7MyG1SomDiIiIlLJy5UoeeughunXrxrBhwwDo2LEjs2bNon///sydO5e2bdty1VVXsXv3bhwOB1FRUXzyySe4XN5bi0ceeYQrrriCN998k7CwMN5++20cjmroWtmmTdkb0YNvUg98drv9K3e4bVUtfyTbDl0amvKSncJC777nn4cdO6Bt28DGKH5T4iAiIiKl9OrVq8KhU5OSknyf582bV+E5Wrdufdj9RyQkBObMqd5z1kXlJRT+PNE/kqWiNxjVsa0ixcVwwQXe3+W8eTB2bO39bOWoKHEQERERqUsO3IA3VC4XvPyy9+1Rdnago5Eq0HCsIiIiIlK7IiK869zcwMYhVaLEQURERERqV3i4d52TE9g4pEqUOIiIiIhI7XI6vX1WlDjUK0ocRERERKT2NW0K+/YFOgqpAiUOIiIiIlL7WreG3bsDHYVUgRIHEREREal9rVopcahnlDiIiIiISO1r0wZSUgIdhVSBEgcRERERqX3dusG2bZCWFuhIxE9KHERERESk9p1wgnd9330BDUP8p8RBRERERGrf0KHe9fPPg7WBjUX8osRBRERERGpfcDC8+KL385o1gY1F/KLEQUREREQCIzHRu167NrBxiF+UOIiIiIhIYLRs6V1rWNZ6QYmDiIiIiARG06bedWZmYOMQvyhxEBERkTLS09M588wz6d69O3379uXCCy8kNTW1TLkbb7yRHj160K9fP44//ngWLVrk2zdixAg6depE//796d+/P6+//nptVkHqg7Aw7zovL7BxiF+UOIiIiEgZxhjuvPNOkpOTWb58OZ07d2by5Mllyp1xxhn8/vvvLFu2jLvuuotLL7201P5nnnmGpKQkkpKSGDduXG2FL/VFUBA4nUoc6gklDiIiIlJGdHQ0I0aM8H0fMmQIW7ZsKVPu7LPPJigoCIChQ4eyfft2PB5PbYUpDUGTJpCdHegoxA9KHEREROSwPB4PU6dO5dxzzz1sueeee46zzjoLh+OP24s77riDPn36cMUVV7Bjx45yj5s2bRqJiYkkJiaW2xxKGrioKPVxqCeUOIiIiMhh3XTTTURGRjJx4sQKy0yfPp333nuPqVOn+ra9/fbbrF69mqSkJHr06FGmGdMB119/PYsWLWLRokW0aNGi2uOXOk6JQ72hxEFEREQqNGnSJNatW8eMGTNKvUk42KxZs5gyZQpffvklrVq18m1v3749AE6nk1tuuYVffvlFzZikrKgo2L8/0FGIH5Q4iIiISLmmTJnC4sWLmT17NiEhIeWWmTNnDrfddhtffvklCQkJvu3FxcXsPmhs/vfff58+ffpUmHxIIxYdDRkZgY5C/OAKdAAiIiJS96xcuZKHHnqIbt26MWzYMAA6duzIrFmz6N+/P3PnzqVt27aMGzeO4OBg/vSnP/mO/eabbwgNDeWss86isLAQay1xcXFMnz49UNWRuiw2FpKSAh2F+EGJg4iIiJTRq1cvrLXl7ks66CbvcJ2ZD57TQaRCsbGQlgbWgjGBjkYOQ+8LRURERCRwYmMhPx9ycwMdiVRCiYOIiIiIBE5srHedlhbYOKRSShxEREREJHCUONQbShxEREREJHAOJA7p6YGNQyqlxEFEREREAkdvHOoNJQ4iIiIiEjgtW3rXK1cGNg6plBIHEREREQmc5s2hQwd46CHvkKxSZylxEBEREZHASkz0rjURXJ2mxEFEREREAuvhh73rL74IbBxyWEocRERERCSwunb1ru++O7BxyGEFLHEwxnQ1xuQbY94JVAwiIiIiUgcY88fn/PzAxSGHFcg3Ds8DvwXw+iIiIiJS1yxeHOgIpAIBSRyMMZcB+4BvAnF9EREREalj1q3zrjUsa53lV+JgjPEYY9wVLMXGmHRjzNfGmFP9OFcUcD9w+9EGLyIiIiINRKdOEBUFy5YFOhKpgL9vHP4FbANSgTeAR4E3S75vB94GWgCfG2PO9uNcr1prtx2ukDHmemPMImPMotTUVD/DFBEREZF6yeGA/v1h3rxARyIV8DdxyAc2AQnW2mustXdba8cDHYHNeBOI44CvgAq7wxtj+gOjgKcqu6C1dpq1NtFam9iiRQs/wxQREZGjlZ6ezplnnkn37t3p27cvF154IeU9xMvNzeXSSy+lS5cu9OjRgzlz5vi1T6RCo0fD2rUalrWO8jdxmAA8Za0t1c3dWpuHNwmYYK31AK8AfQ9znhFAArDVGLMLmARcZIxZUsW4RUREpIYYY7jzzjtJTk5m+fLldO7cmcmTJ5cp9/jjj9OkSRPWr1/Pp59+yrXXXkt2dnal+0QqdPPN3vXnnwc2DimXv4lDSyCogn3BQEzJ5zTAVFAOYBrQGehfsrwIfAac5mccIiIiUsOio6MZMWKE7/uQIUPYsmVLmXIzZsxgwoQJAHTt2pXExEQ+L7nhO9w+kQpFRcGwYfDDD4GORMrhb+KwCLjPGNPm4I3GmLbAP0r2A8QDKRWdxFqba63ddWABsoF8a606MYiIiNRBHo+HqVOncu6555bZt3XrVuLj433fO3TowLZt2yrdd6hp06aRmJhIYmJiuU2ipJHZssXbQfo3jdpf1/ibONwCtAM2GWO+M8bMMMZ8B2wE2gIl75XoArzn78WttfdZa6+oSsAiIiJSe2666SYiIyOZOHFijV3j+uuvZ9GiRSxatAj1axRmzvSuBw2CG28MbCxSil+Jg7V2Cd6k4EnAA/QpWT8BdLXWJpWUu9da+48ailVERERq0aRJk1i3bh0zZszA4Sh7y9ChQ4dSTZi2bt1K+/btK90ncliDBkFKSQOW998PbCxSit8TwFlr00tGUzrFWtuzZD3FWptekwGKiIhI7ZsyZQqLFy9m9uzZhISElFvm4osv5qWXXgJg3bp1/Pbbb5x++umV7hOpVJs2cO+9sH8/pKUFOhopEZCZo0VERKTuWrlyJQ899BApKSkMGzaM/v37c8EFFwDQv39/UkqeBt9xxx3s27ePLl26cPbZZzNt2jSaNGlS6T4Rv5xyCng86utQh7j8LWiMuQr4M9ABCD1kt7XWdq7OwERERCQwevXqhbW23H1JSUm+zxEREXz44YflljvcPhG/dO3qXW/cGNg4xMevxMEYcw/wT2AFkAQU1GRQIiIiItLItW7tXb//vjpJ1xH+vnG4BviPtfZvNRmMiIiIiAgApmRqsJ9+gpwciIgIbDzidx+HGODTmgxERERERKSUxx/3riMjAxuHAP4nDj8A/WoyEBERERGRUm677Y/P11wTuDgE8L+p0q3ATGNMOjAXyDi0gLXWU52BiYiIiEgjZwzs3OkdnvW11yA/H+Lj4eSTYfToQEfX6PibOKwtWb9ewX5bhXOJiIiIiPindWvIyIDx4+Hbb2HXLnj4YdizBzTTeK3y92b/frzJgYiIiIhI7WreHGbN8n5+9VW49lrYu1eJQy3zK3Gw1t5Xw3GIiIiIiFQuOtq7zssLbByNkJoXiYiIiEj9ER7uXefmBjaOrCxIS/PObg1wYNLE3bu9fTEcjj+W1FQoKvK+JcnMBLcbiou9y6hRMHx44OpRBRUmDsaYe4FXrLUpJZ8Px1pr/1W9oYmIiIiIHOJA4pCTU33ndLu959u1CwoKvDf4e/fCG2/A5s3e/fv2ed9y5Od712539Vw7LKz+Jw7AfcAXQErJ58OxgBIHEREREalZB+Z0yMz0/5jiYliyBFatgm++gaVLweXyLvn5sHJlxce2bAlDh3r7WYSFeZfQUGja1LvP4fhjsjpjIDjY26HbWu/bCI8HmjXzJjyRkRATA06n99oHH1sPVJg4WGsd5X0WEREREQmYmBjvesOGsvs8Hm+S8Omn8Oij3uZE2dmwffsfZSIivEO6xsf/0WTojDO8SUBkpLfDdfPm3pv96GhISKhXN/c1ya8+DsaYDsBOa21ROftcQFtr7dbqDk5EREREpJQDIyndeSfccQfs2AF/+xu0agXPPfdHuR49oFcvaNLEe4zbDQMGwCWXeJ/2S5X5+1PbBAwFfi1nX7+S7c7qCkpEREREpFxhYX98fvRRmDzZ+9np9CYGq1bBtGlw0UWly8pR87cJ0uHezwQBmjVaRERERGrHTTd51weShr/9DQoLYdEi72hLV1yhpKEGVJg4GGOaGWM6GWM6lWyKO/D9oKUXcBWwq1aiFRERkVoxadIkOnbsiDGGFStWlFtm7Nix9O/f37c4HA4++eQTAO677z5atmzp23fjjTfWZvjS0D3zDJx33h/fn3zS29FYatThmirdAvwD74hJFviognKmpJyIiIg0EOeffz633HILww8zTORbb73l+7xs2TJGjhzJaaed5ts2duxYHn/88RqNUxqx2bMhPf2PUZakxh0ucZgNbMabGLwGPAAc2n29AFhlrV1eI9GJiIhIQJxwwglVKv/qq68yZswYQkJCaigikXIcGGFJasXhhmNdBiwDMMZYYI61Nr22AhMREZH6obCwkPfee4958+aV2j59+nS++uorWrduzT//+U+GDh1a7vHTpk1j2rRpAKSmptZ4vCJyZIw9MD12HZaYmGgXLVoU6DBEREQanYSEBObMmUPv3r0rLPPBBx/wyCOPsGTJEt+2Xbt2ERMTQ1BQEF9//TVjxoxh9erVxFTyhDg2NpaEhITqCr9cKSkpvs9t27at0WvVFY2xztBw6r1582bS0tJq8hJ+TVTh9yC2xpjewDVAdyD0kN3WWnuK/7GJeBUWe7jxvSWMHRrP8K4tAh2OiIgcgddee43x48eX2ta6dWvf59GjR9O+fXtWrFjBSSeddNhz1fDNEQDmoMm8Dr6xbMgaY52h8da7pvjV/dwYMxhYBJwBnAY0BzoBI4Au+JmliBxq1c5Mvl61mxvfXcKW9JxAhyMiIlW0fft25s+fz+WXX15q+44dO3yfk5KS2Lx5M927d6/t8ESkGvn7xuEhYCZwJVAEXGOtXWKMGQm8jbfjtEiVrdixH4Bij+Uvby9m1l+PJyxYcwmKiATazTffzMyZM9m1axejRo0iJiaGlStXcuaZZ3L//feTmJgIwJtvvsk555xDdHR0qePvvvtuFi9ejNPpJDg4mLfffrvUW4hA+sc/Gt9gkI2xztB4611T/OrjYIxJxTtfwxdAMTDYWvtbyb4bgKuttYNrKkj1cWi47pq5nLm/7+Lpy/oz/o3fOK9fW566tH+pV4siIiIiUqP8uvHyd6aMICDHWusBMoA2B+1LBiruMSVyGCt2ZNI7LoqTu7fktlHdmJ2Uwps/bw50WCIiIiJyCH8Thw1AXMnn5cB4Y4zDGOMAxqGZo+UIFBZ7SN6VRe+2TQG48eQujDqmJQ98tprfNmcEODoREakrDtc6wu12l1ve4/H4PhcUFFBQUFCmbHFxMXl5eeTm5lJcXFzmPPn5+VhrcbvdZGVlsX//fgoKCiqMJSMjg7y8PH+rdcQqqvPBP6cjrXNBQYGvztnZ2X7VOT8//yhqU7YeFW0/8Ds9mMfj8R3j8Xh89T70PEVFRb56l3ee3NxcwPvzyczMZP/+/RQVFVUYS0ZGBoWFhVWqW0Pgb+IwB29HaPD2dzgDyAT2ApcDT1Z7ZNLgrduTRaHbQ684b+LgcBieuKQ/7ZqH8dd3l7Ans/r+IRIRkfpnzZo1DBkyhB49enDCCSewcePGUvunTJlCUFAQycnJpbbfd999vPzyywA8++yz9OrViz59+nDzzTf7bigLCgq48cYbGTx4MAMHDmTSpEmlbjY9Hg+nnHIK69at46uvvuKkk04iNjaWu+++u9xYf/nlF2JjY5kyZcpR1fm2224jISEBl8tVpl6V1fnAXBiHq/PEiRNL1flgh9b5xBNPPGydFy5cWC11TktL4/TTT+eYY47h2GOP5eKLLyY9/Y+pw6y1XHnllbhcrlJJjLWWcePG8fXXXwNwzz33+Op9//33+8rt3buXK664gqFDhzJo0CAee+yxUtcvLi6mT58+ZGdn8+6773L88cfTvHlzXnrppXLjnTVrFrGxsb6fd2PiV+Jgrf2Htfb6ks/zgCHA08CrwBnW2udrLkRpqFbuyASgd9so37amYUG8dGUi2fnF/PXdJRQWl30qICIiDZ/H4+GGG27gtttuIzk5mb/85S9cd911vv2//vorS5YsoVOnTqWOc7vdzJ07lwsvvJDk5GT+/e9/s2LFCtauXcvGjRt5++23Ae+kczt37mT58uWsXLmS9evX8/777/vOk5aWRnZ2Nt26daNnz57MmDGDe++9l+Dg4DKx5uTkMGnSJK699tqjnjn74osvZv78+XTo0KHMPn/r/Nhjj1VY55SUlFJ1fu+990rVOSsry1fn6dOnH7bOt99+O9dee225+6vC4XAwZcoUVq9eTVJSEgkJCUyePNm3/+OPPyY0NBSXq/SYPkVFRfz888+MHj2ar7/+ms8//5z169ezcuVKZs2axfz58wF44IEHaNmyJUlJSSxdupQZM2awYMEC33mWLVtGz549iYyMZOjQocyZM4fx48cTFBRUJtZdu3bx6KOPcvHFF5e7v6GrNHEwxgQZY84zxnQ8sM1au9Ra+3/W2tustV/VbIjSUK1I2U9EsJOEmIhS27u3bsJjf+rLoi17eWju6gBFJyIigbRnzx6WLVvGJZdcAsBll13GkiVL2LdvHzk5Odx666289NJLuN3uUm8Ktm3bhtPppEWLFsycOZNLLrmE0NBQrLVcd911zJgxA4DVq1czYsQI33EnnXRSqZvozz77jHPPPReA+Ph4unbtSnFxcZlmLtZa7r//fq666iri4uLKbQZTFUOHDiUuLq5MU5vc3Fy/63zxxRcTGuqdcuu6667jgw8+8LvO55xzjq/O3bp1O2ydx44dWy11jo6OZvjw4b7vgwYNYuvWrYD3Rv3hhx/mqaeeKtO0asGCBSQmJmKM4b///S/jxo0DwOVycdVVV/l+12vWrOHkk08GICgoiOOPP553333Xd55PPvmE888/H4Bu3boRHx9PUVFRmd9BcXExt99+O//6178IDw8/6nrXR5UmDtbaIuADIKHGo5FGZcWO/fRq2xSHo2xH/nP6teWaEzryxs+bmbV0ewCiExGRQNq+fTtxcXG+7y6Xi7i4ODZv3sx9993H+PHjfU/lDx6J7+CbwB07dtCuXTtfmbi4ON/8EkOGDGH27NlkZGSwZ88ePv74Y98+ay0ff/yxL3E4oLwR/xYsWMCqVau47rrrMMbgcPjbCrxiB65z8I3rfffdxzXXXONXndu3b+/bFxcXx/bt28vUOTU1tdw6n3feeX7X+frrr6+2Oh9QXFzMiy++yAUXXADALbfcwoMPPkiTJk3KxPPxxx8f9nd9cL2nT59OdnY2W7Zs4auvvvLV2+Px8Nlnn3H22Wf7zmutxRhTJnGYNWsWkZGRnHrqqRhjcDob3/Dx/v6mNwItazIQaVzcHsuqnZn0iouqsMzkM3owuGM0d838nVUpmbUYnYiI1EXWWnJycli8eDHXXnutr3PqgU6s1lo+/fRT381keTd/B248x44dy+jRoxk9ejR//vOfGT58uK/JTU5ODitWrGDgwIFlrn+w/Px8rr32Wh588EFSU1PJzs72LdXpxx9/ZPHixVxzzTW+uh5tnS+77LJSdc7NzfW7ztdddx0PPvigrzlXTk5OtdTZWstNN91EdHQ0EyZMYPr06YSGhjJq1CgKCwsxxvj6OHg8Hr744gvOPPPMcm/0D2wD+Pvf/05cXBwnnngiEyZM4NRTT/XVe+vWrTidTlq1anXY2Pbs2cPdd9/NPffcQ2pqKnl5eezfv79WOsPXJf5OAPcYMMUY8621NrUmA5LG4etVu8kv8jAgvnmFZYKcDp67/DjOefZHJryzmE8nnkDT8MbXnlBEpDFq3759qdmn3W43KSkp/PDDD6xbt87Xzn/79u2ceeaZvPHGGxx77LFs376dHj16ANCxY0c2bNjgO8emTZuIj4/3fb/nnnu45557AG87+L59+wIwb948Ro0aVSYml8tV6ul6amoqhYWFXHzxxb6Rdg549tlnj7juhz7l//HHH1m7dm2Fdd62bVut1jk/P79Mna21R1VngEmTJrFlyxbmzJkDwPz58/n+++999bbW0r9/f+bOnUtRURFt27b1vYlISEgoU++EhAQAgoODeeqpp3z7rrvuOl+9Z8+ezYUXXlgqjgNvEw5+o7B+/XocDgcjR44EvInE999/j9vtLtUfo8E7MHzX4Ra8s0NvB7KBeSXf3zpoedOf8xzpMmDAACsNR1Gx2458/Ds78vHvbFGxu9Lyi7dk2C53f2avfm2hdbs9tRChiIjUBSeffLJ99913rcfjsW+++aYdNWpUmTIJCQl27dq11lpr33nnHXv77bf79m3atMl26NDBpqSk2OzsbDtq1Cg7Y8YMa621eXl5vu0rVqywnTp1sqtXr7bWWjtu3Dj7+eef+85TXFxs9+7da++44w5744032n379tmioqIysTzwwAP2rrvuOup6u91um5CQYFeuXFnu/srq3L59e5uSkmJzcnLsqFGj7PTp08vUeeXKlYets9vt9tV54sSJNV7nyZMn25EjR9q8vDzr8Xisx1P2/3tjjC+GBx54wD733HO+ff/73/9s37597f79+216errt06eP/eWXX6y11u7bt8+mpqba7Oxs++OPP9r27dvbjIwMa621I0eOtKtWrfKdp7Cw0GZkZNhLL73UPvbYY3bv3r3W7S57rzJ+/Hj7yiuvHHW96xC/7sn9feNwAlAEpAKdS5ZS+cfRpzDSWPx3yXY2pObw4hUDcDkrby13XIfm3HtOL+6ZvYL/fLOOv43uVgtRiohIoE2dOpWrrrqK+++/n+joaN/oQAdzuVy+TrOzZ8/m5ptv9u1LSEjgn//8JyeddBIAZ599tq+zdWZmJqNHj8bhcOBwOHjmmWfo0aMHxcXFfPfdd7z44ou+8/zyyy+MGTPG10xm7ty5vPzyy5xyyimlmse4XK6jHmHo5ptvZtasWezZs4dRo0YRGxvL8uXLj6rOl156aZXrvGDBAl+drbV89tlnFdb5aEcXWrlyJY899hjdu3dn6NChGGPo1KkTH330ka+MtRaXy0VRURFOp5NPPvmk1P7hw4dz2WWXMWDAAACuvfZaBg8eDMDmzZu57LLLcLlcRERE8NFHH9G8eXPS09PZsWMHxxxzjO88M2fO5O9//zt5eXnMnz+fqVOn8vnnn9O9e3dfva21hIWFHVWd6ytz8C+/rkpMTLSLFi0KdBhSDfKL3Iz49/e0aRbKzBuGldvpqjzWWu74aDkfLd7Oq1clcsoxh2+LKCIijUthYSG9e/dm7dq1R3WehQsX8p///KfUaEN1VXXW+emnny41HG1dtnPnTs455xyO9t7wgw8+YOnSpTz88MPVFFm95tcNmRIHqVUv/bCBhz9fw/TrhzCkU0yVjs0vcvOnF39mS3oun048gYTYiMoPEhERqQaH3i/5++Crvju43o2xztBo6u1XJatv/CyRSuzPK+KF7zcwonuLKicNAKFBTqaOGYDTYZjwzmJyC4srP0hERKQaGGNKLY0vgjlOAAAgAElEQVRFY69zY6q3P5Q4SK158YcNZOYXcedpPY74HO2jw3nmsmNJ3p3FXTN/L/NUQERERERqhhIHqRW7M/N5/adNnNevLT3bVjx3gz9O7NaCSad25+OkFF7/aXP1BCgiIiIih6XEQWrF0/PW4fZYbhvdvVrOd8NJnRndsxUPzV3Nr5syKj9ARERERI6KEgepcRtSs/lg0TYuH9SBDjHh1XJOh8PwxCX96BAdzl/fXcLuzPxqOa+IiIiIlK/KiYMxpqUxpsOhS00EJw3DE18lE+JyMHFk12o9b1RoEC9eOYDcwmL++u4SCos91Xp+EREREfmDX4mDMSbKGPO6MSYX2AlsKmcRKWPZtn3M/X0X1w7vRIsmIdV+/m6tmvDvP/Vj8Za9PPDZqmo/v4iIiIh4+Ttz9PPARcCrwO9AQY1FJA2GtZZHv1hDdEQw1w3vWGPXOatvG5K2deTl+Zvo374ZFx7XrsauJSIiItJY+Zs4nAbcYa19viaDkYZl/ro0ft6Qzr1n96RJ6NFNR1+Zv5/eg9937Oeumb/TvXUTerVtWqPXExEREWls/O3jYIDkmgxEGhaPx/u2oV3zMMYMqfkuMC6ng+cuP47oiGAmvLOYfbmFNX5NERERkcbE38RhOnBOTQYiDcuc33eyMiWT20Z3I8TlrJVrxkaG8MKY49i9v4Bbpifh9mhyOBEREZHq4m/i8BVwtjHmNWPMn4wxIw9dajJIqV+K3B6e+CqZHq2bcF7/uFq99rEdmvOPc3vyw9pU/jNvba1eW0RERKQh87ePw8cl647A1Qdtt3ibMVmgdh4rS503/bdtbEnP5bWrE3E6TK1f//JBHVi2bR/PfLuevu2aMapnq1qPQURERKSh8TdxOLm6LmiMeQc4BYgAdgGPWWtfqa7zS2DlFhbzzDfrGJQQzcndWwYkBmMM95/Xm9U7s/jbjCQ+uekEOsZGBCQWERERkYbCr8TBWvtDNV7zYeAaa22BMaYH8L0xZqm1dnE1XkMC5LUfN5GaVcCLVwzAmNp/23BAaJCTqVccxznP/siEtxcz68ZhhAf7myeLiIiIyKGqPHP00bLWrrTWHpgHwpYsnWs7Dql+e3MKeemHjYzu2YoB8c0DHQ7tmofzzJ+PZd2eLP5v1opAhyMiIiJSr/mdOBhjehtjnjLGzDXGfHvI8k1VLmqMeaFkFuo1eGeinltOmeuNMYuMMYtSU1OrcnoJkOe/W09OYTF3ntY90KH4DO/aggkndWbm0h1sSssJdDgiIiIi9ZZfiYMxZjCwCDgD72RwzYFOwAigC94O0n6z1v4VaAIMB2ZSzkzU1tpp1tpEa21iixYtqnJ6CYAd+/J4a8EWLjquHV1bNQl0OKVcfXwCQU7D2wu2BDoUERERkXrL3zcOD+G9we+FN0m4xlqbAIzCO5rSA1W9sLXWba39EWgH3FDV46VueerrtWDg1tHdAh1KGS2bhHJmnzZ8uGgbOQXFgQ5HREREpF7yN3HoC7yDtz8ClAy9aq39Fm/S8PBRxOBCfRzqtbW7s5i5ZDtjh8QT1yws0OGUa+zQBLIKipm1dEegQxERERGpl/xNHIKAHGutB8gA2hy0Lxno7c9JjDEtjTGXGWMijTFOY8xpwJ+Bb6sStNQtj32RTESwixtP7hLoUCp0XIdm9I6L4q0Fm7FWM0qLiIiIVJW/icMG4MAUwMuB8cYYhzHGAYzDOx+DPyzeZknbgb3A48Ct1tqPD3uU1FmLNmcwb/Vu/nJSJ5pHBAc6nAoZYxg7NIG1u7P5ZWNGoMMRERERqXf8TRw+xdsRGrz9Hc4AMvHe/F8OPOnPSay1qdbak6y1zay1UdbaPtbal6sYs9QR1loe/WINLZqEMP6EjoEOp1Ln9mtL8/Ag3lqwOdChiIiIiNQ7/k4Ad99Bn+cZY4YCFwFhwBfW2q9qJjypy75ds4ffNu/lX+f3rheTq4UGObl0YAdenr+RlH15tK2j/TFERERE6qIjmgDOWrvEWjvFWnubkobGye2xPPZFMgkx4Vw2sH2gw/HbmMEdsNby7kINzSoiIiJSFVVKHIwxfY0xE40x/zDGtC7Z1sUYU7cG7pcaN3vpDpJ3Z3H7qd0Jctb6BORHrH10OKcc04r3f91GfpE70OGIiIiI1Bv+TgAXYoz5EFgKPAPcC7Qt2f0YMKVmwpO6qKDYzZNfr6V3XBRn9WlT+QF1zFVDE8jIKWTu7zsDHYqIiIhIveHvo+IH8U72diXQitIzRX+OdzZpaSTe/WUrO/bl8ffTe+BwVGnS8Drh+C4xdGoRwZuaSVpERETEb/4mDn8G/s9a+x7eeRwOtglIqM6gpO7Kyi/iue/Wc3yXGIZ3bRHocI6IMYarhiawbNs+krbtC3Q4IiIiIvWCv4lDDLD6MOcIqZ5wpK57ef4mMnIK+fvpPQIdylG5aEA7IkNcvPXz5kCHIiIiIlIv+Js4bAKGVrBvEN7Zo6WBS80q4JX5GzmrTxv6tmsW6HCOSmSIi4uOi2PO8p2kZRdUyznzCtXZWkRERBoufxOHt4DJxpgxwIHpga0x5mTgb8BrNRGc1C3PfbuOgmIPt5/aLdChVIsrhyZQ6PYw47dtR32uxVv20v/+r7hn9gqstdUQnYiIiEjd4m/i8BjwGfA2f/Rx+BGYh3cCuGdrIDapQ7am5/Ler1u5dGB7OrWIDHQ41aJLy0hO6BLLO79sodjtOeLzpGUXcOO7S3A6DG//soWH5q5W8iAiIiINjl+Jg7XWba29DDgJeAJ4Be+wrCOttWNqMD6pI574Ohmnw3DLKV0DHUq1Gjs0np378/l61e4jOr7Y7eGm95ayN7eQDycMZezQeF6ev4mn5q2r5khFREREAstVlcLW2vnA/BqKReqolSn7+TgphRtGdKZVVGigw6lWpxzTirhmYby5YDNnHMGcFE98vZYFG9N5/OJ+9GrblPvOiSKv0M0z36wjLMjJDSM6V3/QIiIiIgFQpcTBGGOANkCZu0dr7cbqCkrqlse+SKZpWBATTmp4N8FOh+GKIfE8+sUakndl0b21/5Ogf7VyF1O/38CfB3XgTwPaAeBwGB65qC/5xR4e/WINYUEOrj6+Y02FLyIiIlJr/J05OsYYMx3IB7YB68pZpAFasCGdH9am8tcRnWkaFhTocGrEZQPbE+Jy8P6vW/0+ZnNaDrd/sIy+7Zryj3N6ltrndBievKQfp/ZsxX2frqrSeUVERETqKn/fOLwKnAw8B6wBCmssIqkzrLU88sUa2jQN5aphCYEOp8Y0jwime+smbEzL8at8XqGbCe8sxuk0vDDmOEKDnGXKBDkdPHv5sfzl7cXcNfN3nMZwycD21R26iIiISK3xN3E4GbjFWvtGDcYidcyXK3exbNs+Hr2oT7k3xw1JdESwX/M5WGuZMut3kndn8ca4QbRrHl5h2RCXkxevGMD1by/m7zOX43AYX5MmERERkfrG3+FYM4AjG3ZG6qVit4fHvkymc4sILjqu4d/sxkSEkJFd+Yu0dxduZebSHdx6SjdO6tai0vKhQU6mXTmAE7rEcsdHy5i5ZHt1hCsiIiJS6/xNHJ4FJpR0jpZG4KPF29mYmsMdp/XA5fT3j0n9FRMZTFpO4WHnX0jato/7P13FiO4tuGlkF7/P7U0eEhnaKYZJHy7j46Qd1RGyiIiISK3yq6mStfZJY0xbYJUxZh6wt2wR+49qj04CIr/IzdPz1nFsh2ac1qtVoMOpFdERwRQWe8gpdBMZUvavRUZOIX99ZzEtmoTw9KX9cTiqlkOHBTt59aqBjHvjV/42IwmHMZzTr211hS8iIiJS4/xKHIwxZwI3AiFA93KKWECJQwPx5s+b2ZWZz9OX9aexvGSKiQgGICO7sEzi4PZYbpm+lLScQv47YRjNwoOP6BphwU5eu3ogV7/2G7eWJA9n9a363BEiIiIigeBvG5Qngd+AfkCItdZxyNKwe842Ivvzinjh+w2M6N6CIZ1iAh1OrYmJ9CYD6TllO0j/Z95a5q9L4/5ze9GnXdOjuk54sIvXxw3k2PbNuHn6Ur5YsbNKx3s8ljW7Mvl0WQoFxe6jikWkphW7PWTkaBA+EZGGwt9RlToAN1trf6/JYCTwXvxhA5n5Rdx5Wo9Ah1KroiNCAEg/pIP0t2t288y367kksR2XDepQLdeKCHHxxvhBjH11IRPfW8rzYwyn9Wpdbtlit4dVOzP5dVMGCzdl8NvmDPblFgFwVt82PHvZsVVuNiWB99gXa/hi5S6mXz+Elk0a1mzsBzw9by1Pz/NO8dOueRiDEqI579g4TugSi1N/ZkVE6iV/E4elgBpkN3C7M/N5/adNnNevLT3bRgU6nFrla6p00NPRrem53Do9iV5to7j/vN7Ver3IEBdvjh/Ela/+ysT3ljB1zABG9WxFQbGb37fvZ+GmDH7dlMHiLXvJLigGID4mnNHHtGJwpxi2ZeTyn2/W0ToqlHvO7lnJ1aSuWLgxnUe/WMOSrfsAGPTgNwC8d91gOreIpFVUw0kipv+6DYBBCdFERwTz8bIUZi7dQZMQFyf3aMngTtG0ax5Os7AgEmIiaBreMCeYFBFpSPxNHG4G3jTGrLPW/lSTAUngPD1vHW6P5fZTy+vG0rD90VTJmzjkF7m54d3FAEwdM6BG5rFoEhrEW9cM4spXFnLDu4s5tkNzlm3bR0GxB4BurSI5/9i2DOoYw6CEaFo3/eOm0lrL/rwiXv1xE22ahnLt8E7VHl9jYK1lydZ9dIqNoHnEkfVdOZzNaTlk5BbSs00Ur/64iX9/mezbd0ybKFbvzATg8pcXljru+0kj2Jvr7W/TtVWTUvsKit14PBDicrBoy176tW9KiKvutRbdlZnP8V1iePfaIQCkZxfwfXIqn6/YyffJe/hkWUqp8u2ahxEa5CTI6SDY5eCkrrGEBbuIjQymddNQBiZE1+v5ZKy1ZOQUUuS2tGwSojeFIlIv+Zs4zAaigP8ZY3KAfYfst9ba+GqNTGrV8u37mP7bVq4elkD76IonNWuowoNdhAY5yCjp43DvxytYmZLJq1cl0iGm5n4eUaFBvHXNYG6dvpQ9WQWMGRzPoI7RDExoTkxkSIXHGWO45+ye7M7M54HPVtMyKpRzNUpTlSVt28dFU38GYOU/TyOinBG1jtSezHxGPP59qW2tokI4vkssN43sSsfYCDwey/Id+0nelcnf//tHS9BDjwM4rVcrsvKL+XlDeqntLoehf/tmvHJV4hF33K8pBzf9i4kM4aIB7bhoQDustWzfm8eerHzW7s5mS3ouu/bnUeS2pGYXkJZdwDPfri91rqhQF91bNyEhJoLQICc79uWxLSMXCwQ7HQzrHEPvuKasTNnPkE4x9GwbRUxECMEuB9bagA70MP6N3/h2zR7f98gQFyd2i6VTbCThIU4iQ1y0ax5G22ZhOI0hI6cQl9NBQkz4Yf8dEBGpbeZw49b7ChnzBt6RkypkrR1XTTGVkZiYaBctWlRTp2/0it0ezn/hJ/ZkFjDv9pOICm2cTQaOf+RbBneMZnCnaP7+39+5aWSXOv/2Jb/IzdhXfyVp2z7eHD+IoZ0r7tCemV9EsNNRr5/aVrePFm9n0ofLfN83PHRmtbW/T5j8WZlt/7vj5MMmojv355GyL48FG9L5cPF2tqTnAt6mdOmHdDK+amg8admFLNyUQVp2AS6HYXCnaIrclmNaN6Fvu2b0aNOEXm2PrkP/kepy91z+clIn7jiC/lLWWtJzCgkLcpKeXciqnZl8n7yHjak5bEzLobDYTbvm4bSKCmH1zix2ZeZXeK7IEBeFxR5aNQ0hrlkYcc3CaR8dRnxMOO2ahzOgQ/Mae/rv9lg63z3X933UMa04sVssv23ey6LNGezOzMdTyX/Bcc3CCAlyEBHsoll4EK2jQmnTNJSosCDCg110WLqAqKy9hHZOICK+Pc07tSO8eWB+5yJSr/n1D6G/8zhcfVShSJ329i9bWLEjk+cuP7bRJg3gba60ZOte5vy+k+FdY7l1VLdAh1Sp0CAn08YO4E8vLuD6txfx4YSh9Ghdun/K6p2ZvDJ/E58s28H5/eP498X9AhRt3bN+T3ap7zv25lXLG6Yb3lns+7zp4TP9ftrdpmkYbZqGMSA+mokju1JQ7Ca/0EPT8CB+3ZRBq6gQUrMKSEyI9h3j9liWbt3LV6t289P6NFameDvTwxbAO0dJ22ahNAkJYsnWvbw1fhBNw4OICg0iKiyIiGBnjTyN91iL4wjPa4whtuRJe0SIiw4x4Zzeu/wBBA7Yk5XPut3ZzFu9m5O7t2Tb3lzSsgpJzynA6fA+xd+xN4+fN6Sxa2k+B56ZBTkNXVs2oUvLSLq2jKR9dDgea4kMcZFX5Obn9elkFxbTs00UnWIjMMb43ggWuz04jCk38bC2dNLwxa3DfX83xw5N8JUpKPawN7eQrem5pGUX4raW6PBgCordbEzNYcnWvTiMIbewmIzcItbuTmVPVoEv/pdmPs0J637xXWdTm050TNlwRD93EZHK+PXGIdD0xqHm7Nqfz6gnf+C4+Oa8OW5go5m3oTxXv/4r3yen0rZpKHNuHk50DbR5ryk79uVx4Qs/YTDM/Osw2jQN5fu1qbw6fxM/rk8jLMhJTGQw+UUefptySqP+PR/s0LcCnVpE8O3tI47qnJn5RfS97ysAku4dXevNh/KL3BQUe9iUlsMXK3axbncWbmv5Pjm1wmMSYsIJcTkJC3YSHRFMWLCTnm2iOKlbC3q2iTqiJ/IJkz/j5lO6ctvoupeA5xe5WbMri2n/20CbpmGs35PN+j3Z7NiXV6ZsWJD3Z1LePmOgbdMwhnSKoVl4EOHBTgZ3jMEY+O+S7cxc4p0lftH/jfIlQtWh2O2drDKv0E1exj6KNm4mb/0GOt1yPW6Hk2a5mdV2LRFpNKrvjYM0XPfPWUmR28O/zuvV6G8mW0eFEuQ0vHDFgHqVNIC3OcPrVw/ikpcWcMWrC3Eaw7o92bSKCuHO07tz+aAOfLVqN3d+tJzr317MzSO7HvWcFPVZYbGH5F1ZZbYf6ZwDmflFOI0hPNjJGU/PB+D1qwcGpM9BaJCT0CAn/ds3o3/7Zr7tbo9l0eYMityWzPwiMvOK2J9XxHfJe/hlYwYhLgdtm4VR5PaQXVDMZ8t38u8vk4mNDCY2MoShnWNIjI9mb24hmflFbE3P5ay+bRgQ35zw4NL/lRx4IFVX+/8e+Pm8MGZAqe05BcWk7MvDGMgv8hAW7KRlkxCahAaRmV/EptQcft2UgcdaVqZksmt/Pi6n4fMVO8kt9M6r8iyl+2YsvPuUak0aAFxOB03DHDQNC4KmraFjazhlCAtnf0q3/31RrdcSETmYEodG7Ls1e5j7+y4mndqN+JiIQIcTcH8b3Y0rhsTTO65+3lD3bBvFS1cOYNwbv9GlRSRPXtKPs/u2JdjlnefxwmPjSNmXx2s/buKcVbsZ0b0FN43swoD46ErO3LBYaznt6f+xKS0HgNtHd2NFyn5+3ZRB5xaR7M7MZ/BD33Db6G6MHRpf6c3//rwi+v3T+4bh2A7NfE+mD9ffJBCcDsPgciZ1/MtJnSlyewhylp4PdE9WPvPXpvHD2lS+Wb2bNbuyeP2nzaXKTP9tW8k5OmEtNA8PxumAs/t6O+ofaVOlQIkoZxSrA6JCg+jXvhn9DkrGDmatZWNaDlvTc8FAXqGbFk1CaneIXYcDYz21dz0RaXTUVKmRyit0M/qpHwgNcjL35uG+m0up/3IKigk/TLv1rPwi3v5lC6/M30RGTiFDO8Vw08guDO0c0yjeOm1IzeaUJ37giiEduG54JzpEh2OM4do3FzFv9e4y5X+bMooWTSp+YvynqT+zaMveUtsGxDfnvzcMq/bYA8VaS26ht819iyYhhAY52Lk/nx/XpfHYl2socluCXQ4Ki0vftE46tRsTR3YNUNSNzy/nXEHPeR8TlVf2bZqI1B3WWr5L3sPKHZmsT81m7NAEBsQ3D3RYaqokFXvm23Vs35vHjOuHKGloYCobUrRJaBB/HdGFq4cl8N7CrUz730Yuf2UhA+KbM3FkF0Z0a9GgE4gLnvdORTOiW8tSb9riK+gUPfDBefRr34ypY46jbbMwANbsymR/bhGDO8Ww7pAO1kCDShrA21k5IsRVqnlbs/BgjmkTxbjjE8gpdBMV6mJfbhEbUrP5YNE2Plq8nYRYvcmsVQ4HDo/eOIjUZXuy8n2Tf4K3qfEZvdsEMKKq0RuHRih5VxZnPTOf84+N43GNsNPo5Re5+XDRNqZ+v4GU/fn0iWvKxJFdGH1MqwYxSVVGTiEzl2xn6vcbePXqgVz12q/szyti+X2nlhpFLCu/iD4lnZorct85PRnRvaVvnoVz+rXl02UpXD64A9n5xXRqEcFNI7tW25CuIlXxy/lX0efzD4koyA10KCJSji9W7OSm95dS5LZcNrA9U846hiZ1ZzRLv/7j8jtxMMbEAbcDJwLRwLnW2hXGmFuBBdbahYc9wVFQ4lB9PB7LpdMWsG5PNt/ePqLedQKWmlNY7GHW0u288P0GtqTn0r1VE24c2YVz+rap128gzvzPfFaVzNDcOiqUXZn5TDy5C5NOKztHxwNzVvHKj5t477rBLNyYwX++WefXNWb+dRjHdQj4a2Zp5H65cBx950wnvLDsCFAiElhPfJXMs9+up1l4EC+MOY5hnWMDHdKhqq+pkjGmFzAfcAMLgGOBA3ec8cAg4PKqxyi17cPF2/ht814eu6ivkgYpJdjl4NKBHbjouHbMWb6T575bz83vL8Xt8XDBse0CHd4RO5A0AL6JwppX8Gd/ylnHMGZIPB1jIxjWOZabT+nK/83+nRHdWzJl1grSsgt8ZZuFB7EvtwiAYyvoMCtSq4zB1INWBCKNTXp2Ac9+6x1xbeHdpxDiqr8Tsfrbx+EJYDVwGpAPHDxm4c/Ao9Ucl9SA9OwCHv58DYMSovnTgPp7Iyg1y+V0cP6xcZzbry3DH/uO137czEndWtbLRPPNnzeXu71PBSNnGWPoeFC7fKfD8PCFfQE4rVdrflqfxphXFvLSlQM4rVdr9mTlg6Vev5GRhsM6HBiUOIjUJVn5RQx4YB4At43uVq+TBvA/cTgB+LO1NtsYc2iNdwOHn9JT6oSH5q4hO7+YBy7o3SDarkvNcjgMk8/owe0fLOPsZ+bzwhUDSs0LUFf9tD6NVSmZPPPtOrLyiwFo2zSUb24fgcMB63ZnH/GQu8d3iWXzI2f5vrdsUotDbYpUxhgcGo5VpE45MFrfvWf3ZPwJHQMczdHzdzidw/1LFAuoQWUdt2BDOv9dsp3rT+xEtwrGKRc51Dn92vLRDUMxxnDxiz/z1oLN1NUBFfIK3SzcmM6YVxby4NzVvqQB4MEL+hAW7CTE5ay383SIVMrhQC8cROqWA8NUn9a7YTxj9zdx+BUYV8G+S4CfqiccqQkFxW6mzP6d9tFh3KQx1aWK+rZrxmc3n8Dwri249+OV3DI9iZyC4jLlAp1QjHryBy6d9kuZ7Xec1p2Te7QMQEQitUxvHETqnN82e+f5iQiu302UDvC3qdK/gHnGmK+A9/A+0xhljLkFuADvSEtSR037YSMbU3N4fdxAwhrIH1ypXc3Cg3llbCJTf9jAE18ls2pnJi9ecRxdWnrfXn27Zje3fbAMayE2MpjYyBBaNAk5aB1MiyYhdGnRhPbRYdXWJ8DtsTiMt4/BgRmbDzZ2aDzXNIBXwyJ+cThw1NE3giKNkbWWL1fsIq5ZGM3C618/wfL4lThYa38wxpwPPA28VrL5EWAzcH5NDsUqR2dzWg7Pfrees/q04eTueuoqR87hMNx4chf6t2/Gze8v5YLnf+aDCUNJ3pXFpA+X0a1VExITmpOWXUBqVgErUzJJyyog65C3E01CXfRqG8WQTjFcf2InwoOrPg9lXqGbz37fyRNfJdMyKpSPbzzet+/ywR14b+FWAMYMjic0SMmyNBIOBw61VRKpMzLzi8kqKObKofGBDqXa+P0/trX2M+AzY0wXoCWQbq1NrrHI5KhZa7nn4xUEOx3ce07PQIcjDcTxXWL55KYTuPCFn/jT1J/JKXQzpFM0L49NLHcim/wiN6lZBezJyid5VzYrU/azIiWTp+etY+aSHTxyYR+GdanaeNb3fLyCjxZvB2Dn/nwe+XwNAMO7xvLQBX1o3zycj5N20LVl5NFXWKTe8L7Jsx4PxuFvS2QRqSnZJQ/OOkSHBziS6uPvPA59rbXLAay164H1NRqVVItPl+9k/ro07junJ62iNPqLVJ+4ZmF8NGEYkz5cRosmITx+cb8Kn+yHBjlpHx1O++hwBsRH+7b/sjGdyf9dzuWvLOTywR2464wefs+guXBTeqnvL/6wAYAuJYnCDSM6c8OIzkdSNZH6qyRZsB6LUd4gEnALNnj/r2oZFRLgSKqPv28ckowxvwNvAe9Za3fWYExSDfbnFfGvOavoE9eUK4cmBDocaYDaR4cz4y9Dj/j4IZ1i+PyWE3ny62Re/XET363Zw6MX9eXEbi0Oe9x3yXvYllH+QG7jj1d/BmnESobZ9rjdOOr5WPFVYa1la0YuS7fuY3N6Dgs3ZrA/rwinw/j6QB0YgbzQ7aGgyENBsYeCYjce6/2xOYzBAIVuS2GxG4fD4HI4CHIaXE5DkMPBzv35JMRGEOJy+PpXFXssWfnFuD2WHfvyfNcJcTkJCXIQ4nIQ7HIQ4nISGuTAaf6YacPlMAS7HAS7nAQ7DUFOB0FOb/kgp4NgZ8nv03rnlHE5DB4LkSHOkjg95BYWk1fk9sV/oK7mQJ1K6m/wfneX9IFxGkNuoRuPtd64SurrchhczgNrb0wuh8HpKPlc8rNwOf/YFux0EBrkLKmLw1evIKeDZmFBhAY5ySooIiOnkIIijzcmvLEBvu8HPrs9lv15RezLLSKnsJjcgmJyi9zkFBSTnV+MBV9dC/anP3IAABwkSURBVIrdvhgOXH9fbhH/396dR8lV1gkf//6qt6Sz7yZANhaBBIMQUBAEwQ1BkQGdAVRAURzlfX1Hz8w4ArLqoKLCcRxGRlncGJwZdkdWBWRxYcBIgiEQQoKJZF+7k/RSz/vHrYQmJF3VnaquXr6fc+7p6rq37v3dPKnq51fP1lR4zdbWdtryiXyCfD7Rls/T2p5oyycaanME2fUSkE+JlLJBvKnwuLU9m2ygpvBvkIvsZ822n7kglwtq4tVjanLB1tY8S9Y08/yKTQxrqOWQyaMq+h7oSaUmDqcDZwL/DFwZEb8kSyJuSyk1Vyo4dd9V9z7H6k1buf6sw6hxzQb1UoPra7jgxAM58U2T+Pv/nMO5Nz3Jb750fKeLzZ1zw+93uW+vftQcLHXZthaHfjxAevmGLTy9ZC0PL1jJ4tXNLN+whYUrm7bvj4DxwxoY0lDLlNGN5HeoCI6qzRUq9DXU1+TI5YKUEvmUVS7rarLKfj6lrILZnqctn2htz7PHqMH87+K1HLzXSBpqs9EkuQj2GV9LbS7HwpWbGFxXw6FTRrG1rZ0trXla2vK0tOfZ3NK+PVHJKspZEtPallWSW9vytLZnW/aaLIGJQgW1tT1Pez4RQFNL+/aKckNtjqGDakmpQ8W3cC+Jws/tzyVqckFKWcIztKGWXA62tGbn3navbYXH+V7236gmFwxtqCUiSwK2tuUZPriO1kIy2FL4NxpSX8PwwXUMrq9hUG0NtTVZhT9XqNwPqsveJ63tWcJQX5vbnmRBh4SLbEHUIPu3bS/822x73J5PbG1rp72QlLQX9rXlE3U1OaaNHcLb9xvH+2dN6jcDo6H0wdG3ALdExFiyJOIjwI+ATRFxG/CjlNIDlQtTXfGHl9fx498u5qwjpnLQns5Zr97v4L1GcuWpB3HqtU/wzm89zLlHT+NjR0xlaMPrP6JGNdbRWF+7fRalw6aO2j7dnTSQbZutLN/eXuVIimtpyzP/lQ3MW7aBupocQ+prtlfgWtvzrNrUwsqNW1m5aQtrm7JvnhevbmbRqleThOljh7D/xGEctc9YWtrzvO+giRw+bXSfX5m3mJRSj6xWn88nWvN52tpfTSa2JVHbn8tnSVFLW/ZNfmt7VoFva0+0tLezrrmV5pZ2hg+qZdSQehrra8jnOyRzZIlN4c5IKft/PKqxjpGN2fFDGmpprK/JWgiK3HfHmfZUGV2aziSltAr4DvCdiNgX+ChwLllrRNenRlHZtbXn+dKtzzB+WANfePd+1Q5HKtmhU0Zz62eO5DsPPs/X73mO6x55kXOPmsbHjpzK8MLYh80t7azb3MrZR05jZGMdb50+hsb6Go7++q+qHL1UfanDGIfeJqWsK88df1jGr+avYN6yDWxuLZ7gjGqs217h3Gf8UM44fDKzp45in/FDSx4T1d/0VKU4lwsacjXs5PubXsseFpXXrf8OEdEIHF7YxgOvXw1KVXHTE4t59i8b+NczDxmwH6rquw6ZPIobzjmcOS+v4zu/fJ6r7lvAdY+8yAkzJzJ2WD3t+ezbqX0nDOV9B00EsmRZEmzra5Gq+J5IKXHvvOX8eW0za5pa+Mv6LSxYvpHFq5u3zzAzrKGWvzpkDw6dMoqZe4xgcF0Nm7a20VroHlMTwdhh9YwZ0kB9raO8pd6k5MQhshT3nWStDB8EhgJPAJ8FbqlIdOqSv6zfzLfue45j3ziOE/rJ0uYamGbtNZLvn3UYc5eu5zu/fJ4H5y9nbXPr9mbog/Z4tQtebY0VCwnYPgVr6uHVo9c3t/LQghXc+tRSfvPiara2ZdevzQWjhtSz97ghnHbonkwZ08iMSSM4bOoou5JIfVSp07F+AzgDmAgsBL5JNq7hxQrGpi665M55tOUTl5880w9l9Qsz9xjB9z46G8j6227c0kZ7Sp0OnpYGrNZWAPI92OLQ1p7n//zH0zyyYCWQDUw+/7h9OHnWHgwfXOvfIqmfKbXF4ePAz8iShccrGI+66YFnl3PvvOX8/Xve6Mwy6pdyuWBE48673z32xeNo2mqPSQ1sh95wDQBNK1YxdGzPTP942d3P8siClZx6yJ5cevKMnU5oIKn/KPUdPjGl1FLRSNRtzS1tXHznPPYdP5RPHj292uFIPW6PkYOrHYJUdU9+6BMccct1NAyt/IrpKSVueOwlfvjEYoYNquWqD73J1gVpACh1OtayJA0R0QD8K9lYidFkK1B/KaX0i3Kcf6C65sHnWbpuMz877wgHkknSAJWbvBcAbYUuS5V07FUPsXh1M7OnjOLi988waZAGiF0mDhHxInBKSmlORCwCOpvfLaWU9i7xei8DxwBLgPcBP4uIg1JKL5UetraZ/8oGfvDrRXx49p4cPm10tcORJFVJ1GZd+dq3bq3oddY0tbB4dTNH7j2Gn5z7FpMGaQDprMXhYWBDh8e7PTF0SqkJuKTDU3cXkpJDgZd29/wDTT6f+NKtzzBsUC3/dMIB1Q5HklRNdYXEoaWy431+t2gNAOcds7dJgzTA7DJxSCmd0+Hx2ZW4eERMAPYD5u1k36eATwFMnjy5Epfv82558mWeWrKOb5z2JkY5y4wkDWi5uuxPer6lsl2VfvPiagAOmDisoteR1PuU1CE+Ir4cEZN2sW9iRHy5qxeOiDrgJ8BNKaX5O+5PKV2XUpqdUpo9bty4rp6+31u1aStX/mI+b5k2mtMO3bPa4UiSqm1bi0MFxzis39zKjY+/xIxJwxk/bFDFriOpdyp1JO3FwK5qp5MK+0sWETngR0ALcH5XXqvMV3/+J5pb2vjKKa7ZIEmCXG2hxaG1cl2V5i1bD8BfH7ZXxa4hqfcqNXHorGY6Cih5JFZhBeofABOAU1NKlZ/+oZ95/IVV3Pr0Us57+97sM96mYkkSRH3lB0df90i27ut7Z76hYteQ1Ht1NqvSscBxHZ46LyJO2uGwwcCJ7GSMQieuBQ4A3plS2tyF1wnY2tbOhbfPZfLoRs4/bp9qhyNJ6iW2j3GoUIvDhi2tPLFwNYPqcnZTkgaozmZVOga4sPA4Aefs5JgW4Fng/5ZysYiYApxH1kLxSocuNuellH5SyjkGun976EVeXNXETR8/nEF1NdUOR5LUS0RhjEM5B0fn84kVG7fy1JK1fOYnTwFwx2ffVrbzS+pbOptV6VLgUoCIyANvTSn9bncullJaTOfdntSJRaua+O5DL3DSmyZyzH4OGJckvSpXn82uV44WhyWrm7nrj8v46W+XsHTdazsHzNpr5G6fX1LfVOrK0S5HXGUpJS66fS4NNTm+fNKB1Q5HktTLvDoda0tJx69pamHOy+tYtWkrq5taWL1pK+uaW3lm6Xrmv7IRgCOmj+G8Y6azz7ihHDZtNHU1VgekgaykxKGjiBgPvK5zY0ppSVki0k7dOWcZj76wistOnsH44fYtlSS9Vq4wODoVpmNd39zKqqatbNjcyoYtbaxrbmH5hi0sXNHEg/NXsGrTawdRD6rLMXJwPZPHNHLhiQfwnhlvYK/RjT1+H5J6r5ISh8L0qVeQjU/YVRulHe4rZH1zK5ff/Syz9hzBmW+ZUu1wJEm9UK4wxqHxwn/iH+9+ilv2O3qnx9XVBCfMnMj0cUN46/Qx7DFyMGOG1tNY3+XvEiUNMKV+Svw/4LPA18gSiK8AeeDMws8rKxKdAPj6vfNZ09TCjeccTk3OISKSpNfb1uKw919e5KQlTzL9c5/kDSMGMXxQHcMH1zJ8UB1jhjYwYnCdf0skdUupicM5wGXA1WSJw20ppaci4grgPmByheIb8J5aspaf/m4J5xw5jZl7jKh2OJKkXqqmkDgAHP3kA+y8vUGSuq/UUU7TgSdTSu1AG9n6DRQWb7sa+HhlwhvY2trzfOnWZ5gwbBCff/d+1Q5HktSLVXLFaEmC0hOH9bw6IHoZ8MYO+2qB0eUMSpkbHnuJ+a9s5JIPHMjQBvueSpJ2rbU5mzb1pQlTqxuIpH6r1Nro08CBwL2F7dKI2EzW+vAV4KnKhDdwLV23mW8/sIDj9h/Pe2a8odrhSJJ6uWnvOoo5Bx3B0Ku+Vu1QJPVTpSYOV5N1VwK4GDgE2LbS82Lg/DLHNeBdcuc88ilx6Qdm0GGFbUmSdqphSCOz/vh4tcOQ1I+VugDc/R0evxIRhwN7A43AnwpjHVQm9817hfufXc4XT9jfObQlSZLUK3Sr43xKKQEvlDkWAU1b27jkznm8ccIwPnHUtGqHI0mSJAGdJA4R8faunCil9Mjuh6OrH1jAsvVb+K/T30xdTalj1yVJkqTK6qzF4SEglXCOKBznytG76dllG7j+sZf4m8P2YvZUJ6qSJElS79FZ4vCOHotC5POJC25/hhGD6/jiCftXOxxJkiTpNXaZOKSUHu7JQAa6m3+/hKeXrOObH5rFyMb6aocjSZIkvYad6HuBlRu38rVfzOeI6WP4q0P2qHY4kiRJ0uuUNKtSRPyyyCEppXR8GeIZkK74+bNsac1zxSkzXbNBkiRJvVKpLQ45skHQHbexwNuA/Qq/qxsefX4Vd/xhGZ8+Zjp7jxta7XAkSZKknSp1Abhjd/Z8ROwN3A58tYwxDRhbWtu56I65TBnTyGfesU+1w5EkSZJ2abfGOKSUFgJXAt8oTzgDy7UPLWTRqiau+OBMBtU5m60kSZJ6r3IMjl5J1l1JXbBw5SaufWghH5g1iaP3HVftcCRJkqRO7VbiEBGjgc8DC8sTzsCQUuKi2+fSUJfjwpMOqHY4kiRJUlGlzqq0iNevIl0PTCg8PrWcQfV3t/9hKY8vXM3lH5zJ+GGDqh2OJEmSVFRJiQPwMK9PHLYAi4H/LIx1UAnWNbdwxd1/4uC9RnLm4ZOrHY4kSZJUklJnVTq7wnEMGF+75znWbW7lh6fMJJdzFltJkiT1Da4c3YP+d/Eabv7dEs45ciozJo2odjiSJElSyUrtqkREHACcBuwF7NgxP6WUzipnYP1Na3ueC26by8QRg/i7dzkJlSRJkvqWUgdHfwy4nmycwwqgZYdDdhz/oB1c/+gi5r+yke999FCGNJScr0mSJEm9Qqk12IuAO4BPpJTWVTCefunPa5u5+oHneecB43n3gROKv0CSJEnqZUpNHN4AfNqkoetSSlx8xzwALvnADCIcEC1JkqS+p9TB0Y8BrlTWDffOW86D81fwd+/alz1HNVY7HEmSJKlbSm1xOB+4NSJWA/cBa3c8IKWUL2dg/cGmrW1cetc89n/DMM5527RqhyNJkiR1W6mJw5+Bp4Ef72J/6sK5Boxv37+Av6zfwr+ccQh1Nc58K0mSpL6r1Mr+vwN/DdwOzOf1syppB3OXrueGxxZx+uGTOXTKqGqHI0mSJO2WUhOHk4G/TyldU8lg+ov2fOKC2+cyekg9X3zv/tUOR5IkSdptpfafaQKerWQg/clPf7uYOS+v48ITD2REY121w5EkSZJ2W6mJww3AGZUMpL9YsXELX7/nOd62zxhOPnhStcORJEmSyqLUrkqLgdMj4n7gHnY+q9L15Qysr7r87j+xtS3P5SfPdM0GSZIk9RulJg7XFn5OAY7fyf4EDPjE4ZEFK7lrzjI+d/y+TB83tNrhSJIkSWVTauLgIgRFbGlt56I75jJt7BD+9ti9qx2OJEmSVFYlJQ4ppcWVDqSv++6vXmDx6mZ+/Im3MKiuptrhSJIkSWXlqmRl8MKKTfzbwwv54MGTOGrfsdUOR5IkSSq7klocImIR2TiGXUopTS9LRH1MSokLbnuGwXU1XHDigdUOR5IkSaqIUsc4PMzrE4cxwJHAJuCX5QyqL7n1qaX8dtEavnLKTMYNa6h2OJIkSVJFlDrG4eydPR8RI8mmZ32gjDH1GWubWvjK//yJN08eyemHTa52OJIkSVLF7NYYh5TSOuAbwJfLE07f8rV75rN+cytfPeUgcjnXbJAkSVL/VY7B0VuAPctwnj7l9y+t4T9+/zKfOGoaB0wcXu1wJEmSpIoqdYzD60RELTATuASYV66A+oKWtjwX3PYMk0YM4nPH71vtcCRJkqSKK3VWpTy7nlVpA3Bi2SLqA37w6CIWLN/Ev39sNkMaup17SZIkSX1GqbXey3h94rAFWAz8IqW0vqxR9WIvr2nmmgcX8O4DJ/CuAydUOxxJkiSpR5Q6q9IlFY6jT0gp8eU75pKL4JIPzKh2OJIkSVKP2eXg6IjIRcT7I2JmJ8ccFBHvr0xovc89c1/hV8+t5PPv2o9JIwdXOxxJkiSpx3Q2q9JHgJuBpk6O2QjcHBGnl3rBiDg/Ip6MiK0RcWOpr6u2jVtaueSueRwwcThnHzm12uFIkiRJPapY4nBDSmnRrg5IKb0E/AA4qwvXXAZcAVzfhddU3bfuX8CKjVv56ikzqa0pxyy2kiRJUt/RWQ34EOC+Es7xADC71AumlG5NKd0OrC71NdU2d+l6bnr8Jc44fDJvnjyq2uFIkiRJPa6zxGEYsLaEc6wtHNtvXXb3s4we0sA/vHf/aociSZIkVUVnsyqtAqYAjxY5x+TCsWUVEZ8CPgUwefLkcp++S646bRZ/XtfMiMF1VY1DkiRJqpbOWhwepbSxC2dTPLnospTSdSml2Sml2ePGjSv36btk8phGjtx7bFVjkCRJkqqps8ThauD4iPh2RNTvuDMi6iLiGuA44NuVClCSJElS9e2yq1JK6YmI+ALwTeDMiLiPbKVoyLowvQsYA3whpfSbUi8YEbWF69YANRExCGhLKbV18x4kSZIkVVinK0enlK6OiKeALwKnANtWPdsMPARcmVL6dReveSFwcYffPwJcClzSxfNIkiRJ6iGRUirtwIgcsK2j/+qUUnvFotrB7Nmz05NPPtlTl5MkSZIGkijloE5bHDpKKeWBFd0OR5IkSVKf5RLIkiRJkooycZAkSZJUlImDJEmSpKJMHCRJkiQVZeIgSZIkqSgTB0mSJElFmThIkiRJKsrEQZIkSVJRJg6SJEmSijJxkCRJklSUiYMkSZKkokwcJEmSJBVl4iBJkiSpKBMHSZIkSUWZOEiSJEkqysRBkiRJUlEmDpIkSZKKMnGQJEmSVJSJgyRJkqSiTBwkSZIkFWXiIEmSJKkoEwdJkiRJRZk4SJIkSSrKxEGSJElSUSYOkiRJkooycZAkSZJUlImDJEmSpKJMHCRJkiQVZeIgSZIkqSgTB0mSJElFmThIkiRJKsrEQZIkSVJRJg6SJEmSijJxkCRJklSUiYMkSZKkokwcJEmSJBVl4iBJkiSpKBMHSZIkSUWZOEiSJEkqysRBkiRJUlEmDpIkSZKKMnGQJEmSVJSJgyRJkqSiTBwkSZIkFWXiIEmSJKkoEwdJkiRJRZk4SJIkSSrKxEGSJElSUSYOkiRJkooycZAkSZJUlImDJEmSpKJ6PHGIiNERcVtENEXE4og4o6djkCRJktQ1tVW45neBFmACcDDw84iYk1KaV4VYJEmSJJWgR1scImIIcCpwUUppU0rpUeBO4KM9GYckSZKkrunprkr7Ae0ppQUdnpsDzOjhOCRJkiR1QU93VRoKrN/hufXAsB0PjIhPAZ8q/LopIp6rcGyqvLHAqmoHoYqyjPs/y7j/s4z7P8u4/+tqGd+TUnpvsYN6OnHYBAzf4bnhwMYdD0wpXQdc1xNBqWdExJMppdnVjkOVYxn3f5Zx/2cZ93+Wcf9XqTLu6a5KC4DaiNi3w3OzAAdGS5IkSb1YjyYOKaUm4FbgsogYEhFvA04GftSTcUiSJEnqmmosAPcZYDCwArgZ+FunYh0w7HrW/1nG/Z9l3P9Zxv2fZdz/VaSMI6VUifNKkiRJ6keq0eIgSZIkqY8xcZAkSZJUlImDuiwiGiLiBxGxOCI2RsTTEXFCh/3HR8T8iGiOiF9FxJQdXnt9RGyIiFci4vM7nPvciHghIjZFxD0RMakn702Z3SzjD0fE44V9D+3k3NdFxHMRkY+Is3vmjrSjSpVxRIyNiMciYnVErIuIJwoTYaiHVfh9nCKiqfBZvSkivt9Dt6UOKvg+PrpD2W7bUkSc2oO3Jyr+Pn5/RMwtlO/jEXFgsXhMHNQdtcDLwDHACOAi4GcRMTUixpLNnHURMBp4Erilw2svAfYFpgDvAP4hIt4LEBHHAF8lm2lrNLCIbAC9et7ulPEa4Grgyl2cew7ZJAlPVSZ0lahSZbwJ+DgwDhgFfA24KyJ6et0gVfZ9DDArpTS0sJ1biRtQURUp45TSrzuU7VDgJLL39j2VvBntVEXKOLKlEX4CfBoYCdwF3Fnss9rB0SqLiPgjcCkwBjg7pXRk4fkhZCsXvjmlND8ilgLnpJTuK+y/HNg3pfQ3EXEVMDil9NnCvknAUmCflNLCnr8rdVRqGXc4/lzgIymlY3dxvkeB76eUbqxw6CpRBco4B5wI3AlMSCmtqOwdqJhylXFEJLLP7hd6KnaVptzv48IxNwCklM6pYOgqUTnKOCLOB05IKZ1Y+D0HNAEnpZQe3NW1bXHQbouICcB+ZAv5zSD7RhnYvnbHQmBGRIwCJnXcX3g8Y9upChsdfgeYWZnIVapSy7g60akcyl3GhT9sW8iShu+bNFRfBd7Hj0TW5fTWiJhaxlDVTZX4rI6IRuA04KbyRaruKmMZ76zOFRSpc5k4aLdERB1ZU9dNhex2KLB+h8PWA8MK+9hh/7Z9AP8DfDgi3hQRg4EvAwlorFD4KkEXy1h9UCXKOKX0JmA4cAbwaJlCVTdVoIyPAaYC+wPLgLvtjlZdFfysPpXsW+yHdztI7ZYyl/H9wDERcWxE1ANfAuopUucycVC3FZq1fgS0AOcXnt5EVlnoaDiwsbCPHfZv20ehaexi4L+BxcBLhX1/Ln/0KkU3ylh9TCXLOKW0JaV0M/DFiJi1u7GqeypRximlR1JKLSmldcDngGnAAeWJWF1V4c/qs4AfJvu2V1W5y7iQeJwF/AvwF2As8CxF6lwmDuqWiAjgB8AE4NSUUmth1zxgVofjhgB7A/NSSmvJ/nN2rEDMKrwGgJTSd1NK+6aUxpMlELXA3Erei3auO2Xc40Fqt/RgGdcB03cjVHVTD5Zx4rXdHtRDKlnGEbEXcCzww3LFq66rVBmnlP4rpTQzpTSG7IvbKcDvO3uNiYO661qyb5fen1La3OH524CZEXFqRAwi6270xw6DdH4IXBgRoyJif+CTwI0AETEoImZGZjLZcunXFBIO9bxulXFE1BSerwVyhXKt2/biiKgv7A+grrDfz6LqKHsZR8RbI+KoQjkPjoh/JPtj99uevDFtV4kynhERBxeOGQp8k2wiiz/14H3pVRX5rC74KPC4E5RUXaX+Hh9aOGYc8D3gro6DqncqpeTm1qWNLCNNZAMfN3XYzizsfycwH9gMPARM7fDaBuB6YAOwHPh8h30jgT+Sjep/BfhnoKba9zsQt90s47MLr+243dhh/0M72X9ste95oG2VKmOyvu9zyJrK15D1i357te93IG4VLOPjgOcKn9UrgNvJZliq+j0PtK2Sn9WFY+YDn6j2fQ7krcJ/jx/t8Fn9PWBIsXicjlWSJElSUXYPkCRJklSUiYMkSZKkokwcJEmSJBVl4iBJkiSpKBMHSZIkSUWZOEiSJEkqysRBkiRJUlEmDpIkSZKKMnGQJEmSVNT/B2LpnjsC00bDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 792x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cum_mean = data.sort_values('datetime', ascending = True)['ratingValue'].expanding().mean()\n",
    "cum_ratings =  data.sort_values('datetime', ascending = True).expanding().count()['headline']\n",
    "data_sorted = data.sort_values('datetime', ascending = True)\n",
    "data_sorted['cum_mean'] = cum_mean\n",
    "data_sorted['cum_ratings'] = cum_ratings\n",
    "\n",
    "\n",
    "# Creating variables for plots\n",
    "data_sorted_sept = data_sorted[data_sorted[\"sept14\"] == 1]\n",
    "data_sorted_before = data_sorted[data_sorted['before_sept14'] == 1]\n",
    "data_sortes_after = data_sorted[data_sorted['after_sept14'] == 1]\n",
    "\n",
    "x1 = data_sorted[\"datetime\"] #first figure\n",
    "y1 = data_sorted[\"cum_mean\"]\n",
    "x2 = data_sorted_sept[\"datetime\"] #second figure\n",
    "y2 = data_sorted_sept[\"cum_mean\"]\n",
    "\n",
    "x3 = data_sorted[\"datetime\"] #third figure\n",
    "y3 = data_sorted[\"cum_ratings\"]\n",
    "x4 = data_sorted_sept[\"datetime\"] #fourth figure\n",
    "y4 = data_sorted_sept[\"cum_ratings\"]\n",
    "\n",
    "\n",
    "#creating canvas for figures\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(11,5))\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "\n",
    "\n",
    "ax1.plot(x1,y1)\n",
    "ax1.plot(x2, y2, color = 'red')\n",
    "ax1.set_ylabel(\"Year\", fontsize=16)\n",
    "ax1.set_ylabel(\"Cumulative mean rating\", fontsize=16)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.yticks([ 0, 1, 2, 3, 4, 5])\n",
    "ax1.spines['top'].set_visible(False) #remove borders\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['bottom'].set_visible(True)\n",
    "ax1.spines['left'].set_visible(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "myFmt = mdates.DateFormatter('%d/%m/%y')\n",
    "\n",
    "ax2 = fig.add_axes([0.7, 0.7, 0.25, 0.2])\n",
    "ax2.plot(x2, y2, color = 'red')\n",
    "ax2.xaxis.set_major_formatter(myFmt)\n",
    "ax2.xaxis.set_major_locator(ticker.MaxNLocator(4))\n",
    "\n",
    "ax2.tick_params(\n",
    "    axis='x',          \n",
    "    which='both',     \n",
    "    bottom=True,      \n",
    "    top=False,         \n",
    "    labelbottom=True, direction='out', length=3, width=3, labelrotation=0.12) \n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(\"RatingTimeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAw0AAAFeCAYAAADHZPOQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs/XmcnFWZ8P9/rs7WSbo7na2zLxBCAgmEpZHFBRAUFWdE8VFQFHDBEZ3NZX7O70FFmRlHnXFmVPQZHHVEcWMGZkSdjKAsIgiEJWBC9qRJIEkv6fSW9JKu8/2jqkMTslR6SfXyeb9e9cpd97nPXVe5VNdV51znREoJSZIkSTqUokIHIEmSJGlgM2mQJEmSdFgmDZIkSZIOy6RBkiRJ0mGZNEiSJEk6LJMGSZIkSYdl0iBJkiTpsEwaJEmSJB2WSYMkSZKkwxpZ6AAK6Q1veENavnx5ocOQJEmSCiHyvXBYjzTU1tYWOgRJkiRpwBvWSYMkSZKkIzNpkCRJknRYJg2SJEmSDqtgSUNELIyI1oj4Qbdz74qIqohoiYj/iohJ3domRcSdubaqiHjXAfc7ZF9JkiRJPVfIkYabgce6nkTEEuBfgfcA04A9wDcOuL491/Zu4Ju5Pvn0lSRJktRDBVlyNSKuAHYDDwEn5E6/G7grpfRA7ppPA89GRCmQAS4HlqaUmoEHI+JnZJOETx2ub0qp6Ri+NUmSJGnIOeYjDRFRBnwe+PgBTUuAlV1PUkobyY4snJh7dKaU1nW7fmWuz5H6SpIkSeqFQkxPugn4dkpp6wHnS4CGA841AKVHaDtS35eIiOsiYkVErKipqelB+JIkSdLwckyThog4DbgY+KeDNDcDZQecKwOajtB2pL4vkVK6JaVUmVKqnDp16tG9AUmSJGkYOtY1DRcA84HnIgKyIwQjIuJkYDmwrOvCiDgeGAOsI1vTMDIiFqaU1ucuWQasyh2vOkxfSZIkSb0QKaVj92IR43jpiMAnyCYRHwYqgIeBS4EnyK6GNDKldEWu74+BBHwAOA34JXBeSmlVbvWkQ/Y9lMrKyrRixYo+e3+SJEnSIBL5XnhMpyellPaklHZ0PchOK2pNKdWklFYBfwLcBlSTrUe4vlv364GxubYfAR/O9SGPvpIkSdKAceeT2/jNmp2FDiNvx3SkYaBxpEGSJEnHUiaT+Od71vHV32zgnOMn8ePrzi1kOHmPNBRknwZJkiRpOPqr/3ya/3h8G69dXME3rzqj0OHkzaRBkiRJOgbW7WziPx7fxqWnzuBrV5xOUVHeP/QXXCH2aZAkSZKGndt+X0UE/PUbFw+qhAFMGiRJkqR+19rRyY8f28qFiyqYPXFcocM5aiYNkiRJUj+7+d4NtO3LcNnpswodSo+YNEiSJEn9qGFPB//+0BbOWzCZP142s9Dh9IhJgyRJktSPfvHMdppa9/Hx1y8qdCg9ZtIgSZIk9aPlq3Ywd9I4zphbXuhQesykQZIkSeonrR2d/G5DLZcsmUbE4FoxqTuTBkmSJKmfrNvZRGcmccbciYUOpVdMGiRJkqR+8vS2BgBOnllW4Eh6x6RBkiRJ6ic/e+oFyseNYs4g3JuhO5MGSZIkqR88s62BR7fs4t1nzx10O0AfyKRBkiRJ6gc/f/oFRo8o4kPnLyh0KL1m0iBJkiT1g9XbG1k0vZSy4lGFDqXXTBokSZKkfrC9oZVZ5WMLHUafMGmQJEmS+kFHZ4Yxo4bG1+2h8S4kSZKkAaZjX4ZRI4bG1+2h8S4kSZKkAaa9M5k0SJIkSTq4zkyiqbWDMSOHxtftofEuJEmSpAFk7Y4m2vZlOHnG4N4JuotJgyRJktTHNtY0A3DyTJMGSZIkSQfRlTQsmFpS4Ej6hkmDJEmS1Mfa9mUYNSIYO3pEoUPpEyYNkiRJUh/b15lhZNHQ+ao9dN6JJEmSNEB0dCZGjohCh9FnTBokSZKkPtbStm/ILLcKBUgaIuIHEbE9IhojYl1EfCB3fn5EpIho7vb4dLd+YyLiO7l+OyLiYwfc96KIWBMReyLi3oiYd6zfmyRJkgRQVbeHGRPGFjqMPjOyAK/5BeD9KaW2iFgM3BcRTwJ1ufbylNK+g/S7EVgIzAOmA/dGxOqU0vKImALcAXwAuAu4CfgJcE7/vhVJkiTpRTsaWvnxY8/xWNUuPnz+gkKH02eOedKQUlrV/WnusYAXk4ZDeS9wbUqpHqiPiG8B1wDLgbcBq1JKtwNExI1AbUQsTimt6dt3IEmSJMHe9k6qm1rZVNvCY5t3saG6md+sqWZfJjGlZAzXvvK4QofYZwox0kBEfIPsF/6xwJPAL4EpueaqiEjA3cAnU0q1ETERmAms7HablcBlueMl3dtSSi0RsTF3/iVJQ0RcB1wHMHfu3L59Y5IkSRqyWjs6Wbl1N79dX8vdq3eydmfT/raRRcGM8mLee+583nLaTBZNL6V41NBYbhUKlDSklK6PiD8FzgUuANqAWuAs4ClgMnAzcBtwCdC1K0ZDt9s0AKW54xKg5oCX6d7e/bVvAW4BqKysTL1/N5IkSRpqHtlUtz8x2NXSTn1LOzub2ujMJEYUBafPKefjrzuRGeVjmTmhmDPmTRxSScKBCpI0AKSUOoEHI+Iq4MMppa8CK3LNOyPio8D2iCgDmnPny4DWbsdd6V1z7nl33dslSZKkQ9pWv4c/PN/Amh1NPLp5Fw9tzM6cP3X2BKaVFbN4ehkzJhRz6uwJnLNgMmXFowoc8bFVsKShm5FkaxoO1DUKECml+ojYDiwjO22J3HFXfcQq4OqujhExPnfP7vUTkiRJEgDt+zLct7aax6vquXdtNet2Zn+jjoD5k8fzsdedyNXnzWfC2OGVHBzKMU0aIqICeC3wc2AvcDFwJfCuiDgb2A2sByYCXwXuSyl1TUm6FbghIlYA04APAtfm2u4EvhwRlwO/AD4DPG0RtCRJ0vC0t72T2uY2qpvaqGlqo6Y5+++q5xt4oaGVDdVNdHRmf6M+b8Fk3lE5h7PmT2LhtBLGjR4Iv6sPLMf6P5EEfBj4f2T3iKgC/iKl9N8RcSXwd0AF0Eh2ROHKbn0/C3wz12cv8MWU0nKAlFJNLmH4OvAD4BHgimPyjiRJklRQKSW27trLbzfU8MimXTy9bTdb6va87LoImDx+DFNKRvOBVx/PSTPKeOWCyUwuGVOAqAeXSGn41gJXVlamFStWHPlCSZIkDSh1zW08uKGW+9bWcP+6Gna1tAMwZmQRS2dN4IITpzJtQjEVpWOYmntMGjeakSOGzi7NfSDyvdCxF0mSJA0amUziZytf4DP//QcaW/dRVjySi0+axpnzJ7JsdjknzShjRFHe34WVJ5MGSZIkDUgpJepyy50+v3svD2+s4xfPbGdb/V6Wzirj469bxKsXTnH04BgwaZAkSdKA0JlJrN3RxIqqXTy6eRcrttSzo7F1f3sEXLiogk+8fhF/tGymIwrHkEmDJEmSCqauuY3lq3bw6OZd3Le2hoa9HQBMLyvmrOMmcfqccqaWjmHy+NEsmTmBCeNcArUQTBokSZJ0zLR2dLKzsZXHq+r57u+28Mzz2dX1p5aO4cJFUzl/0VQq501i9sSxRDiSMFCYNEiSJKnfbapp5i9/8hR/eKGRzsyLq3eee/xk/uyihZxz/CSThAHMpEGSJEn96rfra/jsz1axqaaF6y9YwHFTxjNjwljOmFfuRmqDhP8tSZIkqc+llLh/XQ3fuG8jj27exeTxo/mH/7OMt585u9ChqQdMGiRJktRr1Y2trNreyPP1e9nesJe7V+9k3c5mZpWP5YZLT+KKV8ylZIxfPQcr/5uTJElSj9Q0tfGr1TtY/ocd/HZ97f7zI4qC6WXF/P/ftJhrzjuO0SPdR2GwM2mQJElSXjKZxPbGVh5cX8OdTz7Po5t3kUkwf/I4PnzBAs6YO5Gls8qoKC12D4UhxqRBkiRJALTvy9Cwt4PG1g7qW9rZVr+X6qZWNtW0sLGmmVUvNLKnvROAWeVj+ciFJ/CmU2aweHqpKx8NcSYNkiRJw1Amk1hX3cSz2xtZubWBx6vq9++ZcKDycaM4saKUy06fxckzylg2u5yls8pMFIYRkwZJkqRhoH1fhkc21/FE1W4e2VzHU1t37x81GDOyiNPmlPORCxcwrayYCWNHUTZ2FHMmjmVaWTElY0aaIAxzJg2SJElDUCaT2FDTzI8f3cqv1+xke0Mr7fsyAJxQUcJbTpvF0lllLJk5gaUzyxg5wmJlHZpJgyRJ0iD3XN0e1uxopLqpjQ3VzWyubeHhTXX7k4SLFldwyZLpnH3cJM46bhJlxaMKHLEGG5MGSZKkQaKuuY2qXXu4Z/VOnt7WQG1zG7ta2qluatt/zbjRIzhuynhed/I0zpg7kVedMIVF00sLGLWGApMGSZKkASSlRF1LO5tqWlj1QgNVdXuobmqlqm4Pq15o3H9d6ZiRnLNgMstmlzN/ynjOWzCZaWXFTCkZ7VQj9TmTBkmSpAJJKbGxppn71tZw79pqtu7KLnHa2pHZf03JmJFMKxvD9AnFfPKSRZw0o5R5k8ezYGpJASPXcGPSIEmSdIzsamln5bbdrNy6m3vX1lBV18LuPR0AHD9lPKfPLaeidAwzy8cyf/J4Tp5ZxrSy4gJHLZk0SJIk9YvWjk4e27KLhzbW8URVPVV1e9jR2ApABBw3eTynzJrAH506kzPnT+T4KeNd1lQDlkmDJElSH0gp7V+96NaHt3DPs9V0ZhIji4K5k8YxfUIx7z1vHqfPmcgpsydQMsavYRo8/F+rJElSD6x+oZF711azsaaZjTUtbKpupqltHwBFAW89fTaXLJnGqxdOZezoEQWOVuodkwZJkqQjSCmxrX4vv3xmOxuqm3m8qp5NtS0ATC8r5oSKEt52xiwWVJSwYGoJi6aXMqVkTIGjlvqOSYMkSdIBtjfsZeXW3Ty9rYFnns8+ugqWRxQF5xw/ibecNov3nDuPSeNHFzhaqf+ZNEiSpGFrb3snq15o4Kmtu3nm+Qa21e9lQ3UzDXuzCcLIouDEaaW8Ycl0ls6awLLZ5SydVWbBsoadY540RMQPgIuA8cAO4EsppX/LtV0E3AzMBR4BrkkpVeXaxgDfBN4O7Mn1+0q3+x6yryRJUiaT+O+Vz7OhupkdDW2s2dHImh1NdGYSALPKxzJv8jguOqmC2RPH8ZqFU1g6awLFo6xHkPJKGiLiLcCklNJ3c8/nAT8GlgL/S/YLenOer/kF4P0ppbaIWAzcFxFPAlXAHcAHgLuAm4CfAOfk+t0ILATmAdOBeyNidUppeURMOUJfSZI0TG2obuKLy9fy4Ppa9nZ0Atk6hAUV4/mT84/ntDkTWTZnAhWl7ocgHUq+Iw03ALd3e/4VYDZwC/Aesl/oP5HPjVJKq7o/zT0WAGcCq1JKtwNExI1AbUQsTimtAd4LXJtSqgfqI+JbwDXAcuBtR+grSZKGkV0t7fzrAxv59bPVbKhuZkRR8NbTZ1E5byKXnT7L0QPpKOWbNCwAngaIiLHAm4D3ppRuj4hngb8mz6Qhd49vkP3CPxZ4Evgl8LfAyq5rUkotEbERWBIRO4GZ3dtzx5fljpccqi/wkqQhIq4DrgOYO3duviFLkqRBoLltH/+xYiv/8Kt1NLft45UnTOYdlbN549IZzJk0rtDhSYNWvklDMbA3d3xert+vcs/Xkv1Cn7eU0vUR8afAucAFQBtQAtQccGkDUJpr63p+YBtH6Hvga99CdoSEysrKdDRxS5KkgWlLbQtfuXsdv1q9g9aODKfOnsDn/ngJp8+dWOjQpCEh36RhC/Aq4H7gLcDjKaWuL/AVvPTLfF5SSp3AgxFxFfBhoBkoO+CyMqAp19b1vPWANo7QV5IkDVENezr4++Vr+NGjzwHw9jNnc9U58zhtTnmBI5OGlnyThn8F/iEi3gqcRvZLfpdzgdW9jGEBsAq4uutkRIzvOp9Sqo+I7cAy4O7cJctyfThc317EJUmSBqjtDXu559lqvv6b9dQ1t/O202dx/YULOKHiZZMMJPWBvJKGlNK/REQt2dWIvppSurVbcynw3XzuExEVwGuBn5Od7nQxcCXwLuAh4MsRcTnwC+AzwNPdCplvBW6IiBXANOCDwLW5tjuP0FeSJA1yG6qbuXv1Tn67voaHN9WREpw0o4x/e+9ZnDJ7QqHDk4a0SOnYTeuPiKnAf5AdJSgiu8zqV1NK38q1Xwx8neyyql17LWzJtXXfp2Ev8MUD9mk4ZN9DqaysTCtWrOjDdyhJkvpSR2eGm+/dwH8+sY2tu7LllSdUlPDmU2fw5lNnsGBqiRutST2X9/958koaIuLHwG+Ae1NK63sR2IBi0iBJ0sDT3LaPB9fXcvfqnTy2ZRfP7drDrPKxvHZxBVefN58TKkqOfBNJ+cg7aci3pmEO8DVgZK624N7c4zdH+jVfkiTpSFJK/PKZHdz2SBUPbawDoKx4JKfPnchHLlzAO89ymXSpkPKtaXhlRIwDXkO2JuEC4AqgKCKeI5s8vL/fopQkSUNWU2sH19/2BL9dX8u40SO4+tx5vObEqbxq4RTGjHQTNmkg6HFNQ0ScB3wOuAhIKaVB9/9qpydJklQ4zW37+M6Dm7n14Spqm9u45rz53HDpSYwcUVTo0KThos+nJxERJwAX8uJIQwXZpVa/TrbeQZIk6Yg6M4mb793Atx7YRFPbPipKx/DDD57NeQumFDo0SYeQV9KQm4I0C9hAtpbhL8hOSTpwF2ZJkqSDWvVCA7c98hy/21BLVd0eXnXCFP7sooWcOW8iI4pcAUkayPIdaZhFdifmZ8mOLqw2YZAkSflo2NvB5d98iA3VzYwbPYKzj5vEJy9ZxKWnzHC5VGmQyDdpmEh2StKFwPuBf8pt9nYfL66itK4/ApQkSYPXHU9s4xO3ryST4NTZE/j++89mwthRhQ5L0lHKd/WkRuBnuQcRMZlsAvEh4GYg5XsvSZI09DW37eP6257ggXU1jCwKbn3fKzhvwWSKnIYkDUpH9UU/IuaQLYTuKoieDXQCj/d9aJIkabD6jxVbeWBdDVedM5frXr2AuZPHFTokSb2QbyH0LWSThOPIjiqsBH5KdmrSAymlpn6LUJIkDSqtHZ188/6NTC8r5qa3LLVuQRoC8h1pOBf4Bdkk4f6UUn3/hSRJkgazB9bVsLOxjf931ZkmDNIQkW9Nwyn9HYgkSRoantq6m5FFwQWLphY6FEl95GhrGk4FXgNMBv41pbQjt+nbTqcoSZKkTCbxu411zJk0juJRIwodjqQ+km9NwxjgB8DbyG43nYC7gB3Al4B1wKf6KUZJkjRI3PSL1azcupvrL1hQ6FAk9aGiPK/7W+Bi4D3ANLKJQ5f/AS7p47gkSdIg89DGWr77uy287uRpfPKSRYUOR1Ifynd60pXADSmlH0bEgWONm4H5fRqVJEkadP5jxTZKi0fylXcsswBaGmLyHWmYDDx7mHuM6ZtwJEnSYLXqhUbOnDeR0mJ3fJaGmnyThs1kl109mFcAa/smHEmSNFhtb9jL3Elu4iYNRfkmDbcCn4qIdwOjc+dSRFwI/CXwnf4ITpIkDQ47GlppbN3H7IljCx2KpH6Qb9LwJbKbu30f2JU79yBwD7A8pfS1fohNkiQNEmt2NAJw6uzyAkciqT/ku7lbJ3BFRNxMdqWkCqCObMJwfz/GJ0mSBoFfPrOdkUXBwoqSQociqR8c1eZuKaXfAr/tp1gkSdIg9fCmOi46qYLJJa6NIg1F+U5PkiRJOqhdLe1s3bWXZXOcmiQNVYdMGiKiMyJekTvO5J4f6rHv2IUsSZIGkoc31gGwzHoGacg63PSkzwPbuh2n/g9HkiQNNt97eAvl40ZROX9ioUOR1E8OmTSklD7X7fjGYxKNJEkaVNbvbOLRzbv409eewJiRIwodjqR+kldNQ0T8cUQcVdH0Ie4zJiK+HRFVEdEUEU9GxBtzbfMjIkVEc7fHpw/o+52IaIyIHRHxsQPufVFErImIPRFxb0TM6228kiTp4Br2dHDrw1u49GsPMmpEcMUr5hY6JEn9KN9E4L+Auoj4MXBrSumxXrzeVuB84DngTcBPI+KUbteUp5QOViNxI7AQmAdMB+6NiNUppeURMQW4A/gAcBdwE/AT4JwexilJkg6iuqmVf/zfddzx5DY6OhNnHzeJGy49mVnlbuomDWX5Jg3nAlcB7wSuj4gNZHeJ/kFKqSrfF0sptZD98t/l5xGxGTgTePwI3d8LXJtSqgfqI+JbwDXAcuBtwKqU0u0AEXEjUBsRi1NKa/KNT5IkHVxnJvH/7t/IP929jkxKvHHpDN7/6uM4fU45EVHo8CT1s3w3d3sEeCQi/hJ4I/Ae4P8Cn4+IB8mOPnz7aF88IqYBJwKrup2uiogE3A18MqVUGxETgZnAym7XrQQuyx0v6d6WUmqJiI258y9JGiLiOuA6gLlzHUqVJOlQtu7aw92rd/LQxloer6qnfk8H5xw/iS+87VSOmzK+0OFJOoaOdnO3fWSn/9wVEaXA/wE+B/wrcFRJQ0SMAm4DvpdSWhMRJcBZwFPAZODmXPslQNf2kg3dbtEAlOaOS4CaA16ie3v393ALcAtAZWWlK0JJkpRT39LOI5t3sWLLLh6rqmfl1t0AzCofy0UnTeO1iyu4ZMl0RhQ5siANNz0qbs4VGV9FdsRhFrDjKPsXAd8H2oGPAqSUmoEVuUt2RsRHge0RUQY0586XAa3djptyx8255911b5ckSQdR3dTKA+tquW9tNfc8u5PWjgyjRxZx2uxyrr9gAW89fRYLp73sNzhJw0zeSUNETADeQTZReCWwF/hv4M/JTiXK9z5BdlRiGvCmlFLHIS7tGgWIlFJ9RGwHlnV7rWW8OK1pFXB1t9cYDyzgpdOeJEka9lra9vHDR57jdxtreWZbA3Ut7QBMLR3DW5bN4u2Vszl19gSXT5X0EnklDRFxO/BmYBRwP/A+4D9zowNH65vAScDFKaW93V7jbGA3sB6YCHwVuC+l1DUl6VbghohYQTbh+CBwba7tTuDLEXE58AvgM8DTFkFLkvSijV//Nj+4dy3/fsKrOWFaGa9aOIUlM8s4b8EUTp5RRpHTjiQdQqR05Gn9EfEHstOJbkspbTvS9Ye5zzxgC9AGdF9W9UNABvg7oAJoJDui8FcppR25vmPIJhxvJzvK8cWU0le63fti4Otkl2R9BLgmpbTlcPFUVlamFStWHO4SSZKGhObaetrnH8/WKbNp/NWvefWJFYUOSVLh5f1LQb6rJy3teSwvuU8Vhw/uR4fp20Z2hON9h2i/B1jcqwAlSRqiNvzk55zWsptt3/i2CYOko5bXjtCQrUXI7Qz9DxHx3a4dlyPi/IiY2X8hSpKk3mrbvBmAmeecXuBIJA1G+dY0TAR+CZxNdupQKfA1oIpsbcEu4M/6KUZJktRba9ayZ1Qxk453jyJJRy/fkYYvA3PIrpo0hZdOMboHuKiP45IkSX1oZG0NteVTKXJVJEk9kO+Sq28BPpFSejgiDvy0eY5sQiFJkgaoUU2NtI4rOfKFknQQ+Y40lADPH6KtmKOovJYkScfeyD3NdJQcuA+qJOUn36RhLfD6Q7SdDzzTN+FIkqT+MHZvC+3jxhc6DEmDVL7Tk24Gbo6IBuCHuXPlEXEt8FHguv4ITpIk9Y1R+zrIjBpd6DAkDVL57tPwrYhYAHwO+Hzu9N1kN2T7Ukrptn6KT5Ik9YkERXmvtC5JL5HvSAMppU9FxDeB15HdtbkOuDultKm/gpMkSX0jMolk0iCph46YNETEaOCLwA9TSo8B/9bvUUmSpD5VlDIQrlsiqWeO+JNDSqkd+BAwtv/DkSRJ/SFSxpEGST2W76fHk8Ap/RmIJEnqP5GSIw2SeizfpOHjwCci4s0RfuJIkjTYRLKmQVLP5VsIfTswAfhvYF9EVAOpW3tKKc3r6+AkSVLfKEoJwqRBUs/kmzT8mpcmCZIkaRCJlHHJVUk9lu8+Ddf0cxySJKkfWdMgqTf8yUGSpGGgyJEGSb3gp4ckScNAthDakQZJPWPSIEnSMBApOdIgqcf89JAkaRgoShmSqydJ6iE/PSRJGgYiJcLpSZJ6yKRBkqRhIHBzN0k9l/enR0TMioivRMSKiNgUEUtz5/8iIs7uvxAlSVJvubmbpN7I69MjIpYAzwDvAV4A5gGjc83zgD/vl+gkSVKfcMlVSb2R76fHPwLPAscBbwO6T4p8CDinj+OSJEl9yNWTJPVGXjtCA68CrkwpNUfEiAPadgLT+zYsSZLUl4rcEVpSL+T7k0PmMG1TgL353CQixkTEtyOiKiKaIuLJiHhjt/aLImJNROyJiHsjYt4Bfb8TEY0RsSMiPnbAvQ/ZV5Kk4c6RBkm9ke+nx6PAtYdoewfwuzzvMxLYCpwPTAA+Dfw0IuZHxBTgjty5ScAK4Cfd+t4ILCRbQ3Eh8FcR8QaAPPpKkjRspUyGIpL7NEjqsXynJ90E3BMRvwJ+CCTg4oj4c+CtwGvyuUlKqYXsl/8uP4+IzcCZwGRgVUrpdoCIuBGojYjFKaU1wHuBa1NK9UB9RHwLuAZYTrbO4nB9JUkatjKZxAhwnwZJPZbXTw4ppfuBy8gWQn+HbCH03wOvBi5LKT3SkxePiGnAicAqYAmwsttrtgAbgSURMRGY2b09d7wkd3zIvj2JS5KkoSR1dmYPnJ4kqYfyHWkgpfQL4BcRcQJQAdSllNb29IUjYhRwG/C9lNKaiCgBag64rAEoBUq6PT+wjVz7ofoe+LrXAdcBzJ07t6fhS5I0aGQyKXtg0iCph/Ldp+HUruOU0oaU0kO9TBiKgO8D7cBHc6ebgbIDLi0DmnJtHNDe1Xakvi+RUrolpVSZUqqcOnVqT9+CJEmDRiY30uCO0JJ6Kt9Pj6ciYmVEfDwiZvTmBSMigG8D04DLU0oduaZVwLJu140HFpCtVagHtndvzx2vOlLf3sQqSdKQkMktgmjSIKmH8v30uBKoAr4APBcR/xsR746IcT14zW8CJwF/lFLqvlTrncDSiLg8IoqBzwBPdytkvhW4ISImRsRi4IPAv+fZV5KkYatrpCHcp0FSD+VbCP2TlNIfky1G/hhQTnY4kF6UAAAgAElEQVR60Y6I+F5EXJzPfXJ7J3wIOC3Xtzn3eHdKqQa4HPhboB44G7iiW/fPki1urgLuB76cUlqei+9IfSVJGraSNQ2SeinvQmiAlFIt8DXgaxGxEHgP8AHg3fncK6VURXblpUO13wMsPkRbG/C+3OOo+kqSNJxlXD1JUi/16NMjNy3pFblHBbCvL4OSJEl9J9OZq2lwepKkHso7aYis10XErcAOstOTSoGPANP7KT5JktRbTk+S1Et5TU+KiC8D7wJmkK0r+Efg+ymlTf0YmyRJ6gNu7iapt/KtaXgf8FOyicJD/RiPJEnqY9Y0SOqtfJOGGSml9n6NRJIk9YuUsjUNLrkqqafyXXLVhEGSpEEq5TZ3c0doST11yJGGiNgEvDWltDIiNgPpMPdJKaUFfR6dJEnqvdzqSTHCpEFSzxxuetL9QGO348MlDZIkaYDqWnI1wqRBUs8cMmlIKV3b7fiaYxKNJEnqc5n9S65a0yCpZ/L6ySEiPhMRMw/RNiMiPtO3YUmSpD6Tq2mgaERh45A0aOU7TvlZYPYh2mbm2iVJ0gD04pKrjjRI6pl8k4bDfcpMBNr6IBZJktQfcjUNWNMgqYcOt3rSBcBru536UES8+YDLxgKXAqv6PjRJktQX9u/T4OpJknrocKsnnQ/ckDtOwLUHuaYdWA38WR/HJUmS+kgmV9MQ7tMgqYcO+emRUvpcSqkopVREdnrSOV3Puz2KU0pnpJQePnYhS5Kko5G6ahrcEVpSDx1upGG/XOIgSZIGodS15KrTkyT1UF5JQ3cRUQEUH3g+pfRcn0QkSZL6loXQknopr6QhsltI/g3wIaD8EJe5+LMkSQPQi4XQ/qmW1DP5/uTwF8BHgH8kW9/wd2STiM3ARuCD/RKdJEnqta6ahrCmQVIP5Zs0XAt8Hvhi7vmdKaXPAicBzwNz+yE2SZLUBzKduZoGN3eT1EP5Jg3HAytSSp3APrL7M5BS6gD+GXhf/4QnSZJ6LeVWT3J6kqQeyjdpaODF4ucXgEXd2kYCk/oyKEmS1HdSbqQhLISW1EP5rp70JHAy8L+5x+ciYi/ZUYe/BZ7on/AkSVJvpdzmbk5PktRT+SYN/0x2ihLAZ4EzgNtyz6uAj/ZxXJIkqa907QjtPg2Seijfzd3u7na8IyJeASwAxgHP5mobJEnSAJTpWj2pyJoGST1z1Ju7AaSUErChj2ORJEn9YP/0JJdcldRDh0waIuI1R3OjlNIDvQ9HkiT1uZQrhHZ6kqQeOtxIw31AyuMekbsurzHPiPgocA1wCvCjlNI1ufPzyW4W19Lt8i+mlG7KtY8Bvgm8HdgDfCml9JVu970IuJnsnhGPANeklKryiUmSpKEsdXYVQps0SOqZwyUNF/bTa75AdjfpS8jt93CA8pTSvoOcvxFYCMwDpgP3RsTqlNLyiJgC3AF8ALgLuAn4CXBO34cvSdIgk5ueVGTSIKmHDpk0pJTu748XTCndARARlcDso+j6XuDalFI9UB8R3yI7YrEceBuwKqV0e+7eNwK1EbE4pbSmD8OXJGnQcclVSb01EH9yqIqIbRHx3dwIAhExEZgJrOx23UpgSe54Sfe2lFILsLFb+34RcV1ErIiIFTU1Nf31HiRJGjBeLIQeiH/2JQ0Gea2eFBG/OcIlKaV0US9jqQXOAp4CJpOtT7iN7DSmktw1Dd2ubwBKc8clwIEZQPf27oHeAtwCUFlZmU/NhiRJg1un+zRI6p18l1wt4uVF0ZOBRWS/rK/rbSAppWZgRe7pzlzB9PaIKAOac+fLgNZux0254+bc8+66t0uSNGyl3OpJ1jRI6ql8N3e74GDnI2IB8F/A3/VhTPtftutlUkr1EbEdWAZ0bTS3DFiVO14FXN0trvFkN5/rapckadhynwZJvdWrnxxSShuBvwe+nG+fiBgZEcVkl2gdERHFuXNnR8SiiCiKiMnAV4H7UkpdU5JuBW6IiIkRsRj4IPDvubY7gaURcXnu3p8BnrYIWpKk/ds0ECYNknqoL8Ypa4ATj+L6G4C9wKeAq3LHNwDHk10JqQn4A9AGXNmt32fJFjdXAfcDX04pLQdIKdUAlwN/C9QDZwNX9PgdSZI0hKTk6kmSeiffmoaDiohJwMfIfpnPS0rpRrJ7LhzMjw7Trw14X+5xsPZ7gMX5xiFJ0rCRye0I7UiDpB7Kd/Wkzby8EHo0MC13fHlfBiVJkvpO10hDWAgtqYfyHWm4n5cnDa1kpwrdnqttkCRJA1FupKHIfRok9VC+qydd089xSJKkftK15Cru0yCph/z0kCRpiHtxydXCxiFp8Mq7EDoiTgLeDswBig9oTimlq1/eS5IkFVxXIbSrJ0nqoXwLod8LfIdsXUM10H7AJQfWO0iSpAGja/UkJxhI6pl8Rxo+Dfw38P6U0u5+jEeSJPWx5EiDpF7KN2mYDvyJCYMkSYNQcqRBUu/k++nxO+Ck/gxEkiT1k/37NDjSIKln8h1p+ChwR0TUAb8C6g+8IO3fo16SJA0kyR2hJfVSvknDNuBJ4AeHaE9HcS9JknQM7d+nwelJknoo3y/63wLeCfwXsIaXr54kSZIGqq6kwelJknoo36ThLcAnU0r/0p/BSJKkfpBLGopMGiT1UL7jlC3A6v4MRJIk9Y+uHaGtaZDUU/kmDd8F3tWfgUiSpH6yf3qSNQ2Seibf6UlVwJURcTewnIOvnvSdvgxMkiT1kUzX9CSTBkk9k2/S8M3cv/OAiw7SngCTBkmSBqD9q6I7PUlSD+WbNBzXr1FIkqT+k3H1JEm9k1fSkFKq6u9AJElSP8nVNITTkyT1kJ8ekiQNdV1Lrjo9SVIP5TXSEBGbydYtHFJK6fg+iUiSJPWprh2hwx2hJfVQvjUN9/PypGEycB7QDPymL4OSJEl9J+1fcrWwcUgavPKtabjmYOcjopzsEqz39GFMkiSpL2W6RhqcniSpZ3r1m0NKaTfwZeAzfROOJEnqexZCS+qdvvj0aAVm98F9JElSf8hk92kIl1yV1EP51jS8TESMBJYCNwKr+iogSZLUt14shDZpkNQz+a6elOHQqyc1Apfm+4IR8VHgGuAU4Efd6yUi4iLgZmAu8AhwTdceERExhuzO1G8H9gBfSil9JZ++kiQNRR2dGWqb26hrbt//b8PeDvZ2dFL27DMcf/9yxlVt5rxH7gYgRo8pcMSSBqt8Rxo+z8uThlagCviflFLDUbzmC8DfAJcAY7tORsQU4A7gA8BdwE3AT4BzcpfcCCwE5gHTgXsjYnVKaXkefSVJGlTa9nXSsKeDFxpa2dnYSsOeDhpbO2hq3ceaHY2sr27mubo97Msc/De9t676HVf+4t94YdIMnlh6Lo2nVXL+3BnH+F1IGipi/zJsx/qFI/4GmN010hAR15EdHTgv93w8UAucnlJaExHPA9emlH6Va78JWJhSuuJIfQ8VQ2VlZVqxYkX/vUlJkvLQ2NrBfWtr2FLbwlNbd/PEc/Xs3tNxyOvLx42ict4kFk0vYVb5OKaUjGZyyRimlIxmwthRFI8awZjODmLECBg16hi+E0mDTN5zFg850hDZHWAuBTanlP5wiGtOAeanlO466hBfbgmwsutJSqklIjYCSyJiJzCze3vu+LIj9QVekjTkEozrAObOndsHYUuS1HO/fnYn7//eiz9gzZxQzOtOmsb8KeOZMHYUk8aPZvbEsUwaP5qysaMYP3okI/IpaB41oh+jljTcHG560lXAN8jWHhxKE/CjiPhgSulHvYylBKg54FwDUJpr63p+YNuR+r5ESukW4BbIjjT0LmRJknrnq79ez+Txo7npsqVcuKiCsaP9si9p4DnckqtXAd9NKW0+1AUppS3At4Gr+yCWZqDsgHNlZBOT5m7PD2w7Ul9JkgakLbUtrNzWwPtffRxvOmWGCYOkAetwScMZwK/yuMc9QGUfxLIKWNb1JFeXsABYlVKqB7Z3b88drzpS3z6IS5KkfvH3/7OGkUXB60+eXuhQJOmwDpc0lAL1edyjnoNMAzqUiBgZEcXACGBERBTn9ny4E1gaEZfn2j8DPN2tkPlW4IaImBgRi4EPAv+eaztSX0mSBpSOzgwPrK/hnWfN4YSKkiN3kKQCOlzSUEt2edMjmZu7Nl83AHuBT5GdArUXuCGlVANcDvwt2UTkbOCKbv0+C2wku8zr/cCXU0rLAfLoK0nSgPLrZ3eyp72TCxdVFDoUSTqiQy65GhE/ASamlF5/2BtE/AqoTym9sx/i61cuuSpJKpTLbv4du1raufcTF+S3GpIk9b28P3wON9Lwz8BFEfFPETH6Za8QMSoi/gV4LfBPRx+jJEnDU21zG09t3c2Vr5hrwiBpUDjkkqsppYcj4uPAPwLvzo0oVOWa5wGvAyYDH08p/b7fI5UkaYh4cH12Vu/pc8sLHIkk5edw+zSQUvrniHiCbP3BW4Gxuaa9wH3A36eUftuvEUqSNIjtamln9QuN/GZNNSu37aa6qZWtu/ZSWjzSpEHSoHHYpAEgpfQA8EBuh+gpudN1KaXOfo1MkqRBakttC79eU81v1uzk95t20ZlJRMBpc8o5fc5E3nHmHC5ZOp0xI92XQdLgcMSkoUtKKQNU92MskiQNGiklGlv3sbm2hXU7m9i+u5Xnd+/h0c272FK3B4CFFSV86DXHc/bxkzlpeikVZcUFjlqSeibvpEGSpOFmX2eGbfV7Wbezia31e1n1fAM7m1rZ0dDK9oZW9rS/dNB9SskYls4q45rz5vPaxdOYO3lcgSKXpL5l0iBJGtba9nVS29xOTVMbNU1t1Da3sb2hlXU7mrhvXTWtHZn91xaPKqKseBSV8ydy/okVzCwvZvbEcSyeXsqM8mKnG0kaskwaJEnDysqtu/nlM9t5bMsuNta00LC342XXFAXMnjiOi0+axvknTmVBRQnHTR5P+bhRRLhEqqThx6RBkjQkdXRmqGlqY0djK/euqea362tZs6Nx/8hB5byJ/NGyGUwrLWZq6Zj9jykl2cfokYfbykiShheTBknSoPTC7r2s3Lqb6qY2qpta2dnYtn+KUXVTK3Ut7aT04vWTxo/m7WfOZsaEsbzzrDlMKRlTuOAlaZAxaZAkDVj7OjNU7drDE1X1rHqhkU21LexqaWNXczsvNLTuv25EUTClZDQVpcVMn1DMsjnlTCsbw7SyYqaVjWHe5PEcP2W8U4skqYdMGiRJA0JnJvH0tt08u72JLXUtPLSxlnU7mmnvzE4nGjd6BAumljC1ZAyLppWxcFoJ5x4/mZnlY5k0fjQjikwIJKm/mDRIkgrqoQ21/Muv1/Ps9kYaW/cBMGpEcMbciVz7yvmcOK2UJbPKWFhRamIgSQVi0iBJKpiNNc1c/d1HqSgt5tJTZ3DO8ZM5c95EZkwYa4IgSQOISYMkqWDuWb2Tjs7EHdefxzR3S5akAcv15CRJBbOppoWppWNMGCRpgDNpkCQVTOu+TsaPdhdlSRroTBokSQWzrzNZuyBJg4BJgySpYPZlMowa4Z8iSRro/KSWJBWMIw2SNDiYNEiSCqYjkxjpSIMkDXguuSpJOmY6OjPUNLWxcutuHttSzwPrajhr/sRChyVJOgKTBklSv+rMJG76+Wp+/vR2apvb9p8vCjhvwWQ+dP6CAkYnScqHSYMkqU+klNjZ2Ma6nU1sqmnmhYZWNte28OD6WvZ2dDKrfCx/efGJTC4ZzZSS0bzmxKmMG+2fIUkaDPy0liTlLaXE5toWNlQ3s3tPB/V72qnf08Hm2mae2rqbnY0vjiSMHlnErPKxXLJkGhcuruDik6Yxfox/diRpMPLTW5L0MnXNbWyp20Pj3g4aWztoat3H9oa9/Ofjz7OjsfUl144aEcyZOI7KeZN4xXGTWFhRwsJppUwpGU2EKyNJ0lAw4JKGiLgPOAfYlzv1fEppUa7tXcAXgCnA3cD7Ukq7cm2TgG8Drwdqgb9OKf3w2EYvSYNLJpN4fvdefr+pjuqmNnY0tPLC7r08sL6Gjs70kmsj4Kx5k/jIa0/glFkTmDx+NBPHj2b86BEmB5I0xA24pCHnoymlf+t+IiKWAP8KXAo8AdwCfAO4InfJzUA7MA04DfhFRKxMKa06ZlFL0gD39Lbd/PrZajbWNLOxpoXNtc20dmT2t08YO4oZE4q5/IzZXLJkOuXjRlFaPIqy4pGUjR1F8agRBYxeklQoAzVpOJh3A3ellB4AiIhPA89GRCmQAS4HlqaUmoEHI+JnwHuATxUqYEkqtA3Vzdy3NpskPFG1m7U7m4iAORPHsWDqeF65YDILKkqYP3k8S2eVUVo8qtAhS5IGoIGaNHwhIv4eWAv835TSfcAS4KGuC1JKGyOiHTiRbNLQmVJa1+0eK4HzD7xxRFwHXAcwd+7cfnsDklQoKSVWbmvgzie28b2HqwAoHzeKk2eUceMrTuZtZ86mzORAknQUBmLS8P8DVpOdanQFcFdEnAaUAA0HXNsAlAKdh2l7iZTSLWSnNlFZWZkObJekweqprbv5yWPP8ePHtpJyn26vXVzBDZeexHFTxlt3IEnqsQGXNKSUHun29HsRcSXwJqAZKDvg8jKgiexIw6HaJGnIemrrbu5evYPHq+r5/aZdjB89gjcunc4ZcyfyltNmMbV0TKFDlCQNAQMuaTiIBASwCljWdTIijgfGAOvIJg0jI2JhSml97pJluT6SNOT8flMdX/nVOh7dsouRRcEJFSV88pJFXH3efErcC0GS1McG1F+WiCgHzgbuJ7vk6juB1wB/QTbWhyPi1WRXT/o8cEdKqSnX9w7g8xHxAbKrJ70FOO+YvwlJ6idbd+3hn+9Zz71rq9nV0s6UktHccOlJXPGKuSYKkqR+NdD+yowC/gZYTLZOYQ1wWUppLUBE/AlwGzAZuAe4tlvf64HvANVAHfBhl1uVNBTUNrfx9d9s4LZHqiiK4M2nzuSkGaX8nzPnMGGcBc2SpP4XKQ3fWuDKysq0YsWKQochSS/T0Znhsc27+M2aan746HO07cvwjsrZ/NlFC5kxYWyhw5MkDQ15r5Ax0EYaJGlYq2lq46crtvLTFVupqtvDiKLgDUun87HXnciCqSWFDk+SNEyZNEjSAPHIpjo+fNsT7GppZ86ksXztytN57eIKxluvIEkqMP8SSdIA8ORz9bz73x6honQMt//JuZw5dyJFRe6rIEkaGEwaJKnAHtlUx7X//hjjRo/gzo+8kmllxYUOSZKklygqdACSNJz9flMdV3/3UWaWj+WuP32VCYMkaUBypEGSCqS6sZWP/vBJZkwYy0+uO4fJJe7eLEkamEwaJKlAvvA/a6hraeO2D5xtwiBJGtCcniRJBfCl5Wu488nnue7Vx7Noemmhw5Ek6bAcaZCkY2hveyd/98tn+f7vq7j4pGn81RsWFzokSZKOyKRBko6R5rZ9/OkPn+C+dTW875XH8VdvWMQIl1WVJA0CJg2SdAxs3bWHP/vxkzz53G4+/eaTef+rjit0SJIk5c2kQZL62Yotu/jzHz9F/Z52vvKOZbztjNmFDkmSpKNi0iBJ/SSlxO0rtvF//+sZppSM4cfXncOps8sLHZYkSUfNpEGS+sHGmmY+d9dqHlhXw1nzJ/LNq85kisuqSpIGKZMGSepjD6yr4ervPkpK8InXn8j1F5xAkQXPkqRBzKRBkvpISolfP1vNn/zgcWZOGMt3rjnLPRgkSUOCSYMk9ZFv3LeRL//vWqaXFXPH9ecxray40CFJktQnTBokqZc6M4lvP7iJf/jVWs4+bhK3vKeSCeNGFTosSZL6jEmDJPVCSomP/vAJ/ucPO3jdydP4u7eeYsIgSRpyTBokqYeqG1v50v+u5X/+sINPvP5EPnLhCURY8CxJGnpMGiSpB6qbWrnq24+wbmcz15w3nw9fYMIgSRq6TBok6Sg9tKGW677/OO2dGW593yt4zYlTCx2SJEn9yqRBko5g9552/vOJ53nyuXoa9nbw2/W1TC8r5uZ3n8GZ8yYWOjxJkvqdSYMkHWD3nnZWbKnnpyu28uyORrbV7yUlmDNpLJPHj+GKs+bwsdefSEWpS6pKkoYHkwZJw1pKic21LTy9rYFnnm/goY11PLu9EYBxo0dw4eIK3n7GHC46qYKlsyYUOFpJkgrDpEHSsLSvM8N/PfUCN9+7gc21LQCMGVnEsjnlfPKSRSyZWcY5x0+meNSIAkcqSVLhDamkISImAd8GXg/UAn+dUvphYaOSdKyllOjoTHR0ZnKPxN72Tp7cWk9V3R621LZw37oadrW0s2haKX9z2VLOnDeRhRUljBxRVOjwJUkacIZU0gDcDLQD04DTgF9ExMqU0qrChiUNLikl9mUSnV2PlOjszP2beeljXyaRSYl9nbl/My+/JtsvQ2cGOjMZWto6qWtpY097J+37sl/su/frut/BXu/F++VevzOxrX4PLe2dL0kSDqeidAyvPGEKb1w6nUuWTGdEkUulSpJ0OEMmaYiI8cDlwNKUUjPwYET87P9r795jpDrLOI5/fzOzy2UXWi4VSyrFC7UVAl74w1gVvPzRhjY1wTSm2pQqbbQ2Mekf2hjvJl4STTSxMSW0UhpDNIqmGNNojFiRxkg0kKK0KVGirRVbhLLYizCPf5x3ds/Ozh4Gds7MMvv7JCc7877n8rw8zJn3mTkzA9wM3N3T4FoYeek0Tx/PPlwZZBOciGyBrC1y855W7THaF7nbYz3ZvsfaI20YTfsjt7/JjpPflvy2E+JufRwm7K/4OJOOuY2YIqAeY7cjbRhAvR6jxx+3j4B65PvGjjl+m/H7bD7O6TN1Rl46zZl6Lo7IJsH1tE49Imur54+Z9Y+uFzEaU6ONpvuNvNfz+0rxNPYVk/ytR4xO4CdM/uvj/++VrVYRA9UKtYqoVkVVolppseTaaxVRafyVeP0r57Fs4VwGaxUGqtmS3dbo/YGquGzBXK66dD4Lhwa7N0AzM7M+0DdFA3AFcCYinsi17QfW9SieQo8efo7btu/rdRg2BRIIkJT+Qq1SYWhWLU1ms75KBUR2vyKhCX/H+ippR/n7jf1XKmIgTZJH25r2odHtWh0nt08mTtCbJ+L5+0UT+HHbS9SqolqpFK4zd7DKouFBhgZrVPwqv5mZ2bTXT0XDMHCiqe0EMC/fIOl24HaAZcuWdSeyFlZfdhH33PTmFFM2+Wzcbtwb365x6zR+eHa0tWjdFvtjknalyWUjCuU2yLef7ThM0p6Pu53jNH5hN7/OpMdJDY1J8ei4NDZpz0/wlf7RRifVufbRY43ry63jX/41MzOzGaSfioYRYH5T23zgZL4hIrYAWwDWrl3bxYswxlsyfzYbVl/aq8ObmZmZmbWtn74m5AmgJmlFrm0N4A9Bm5mZmZlNQd8UDRFxCtgJfEnSkKSrgRuAB3sbmZmZmZnZha1viobkDmAOcBTYAXzMX7dqZmZmZjY1/fSZBiLiGPC+XsdhZmZmZtZP+u2dBjMzMzMz6zAXDWZmZmZmVshFg5mZmZmZFXLRYGZmZmZmhVw0mJmZmZlZIRcNZmZmZmZWyEWDmZmZmZkVctFgZmZmZmaFFBG9jqFnJP0bONLrOOy8LAae7XUQ1lHOaf9xTvuPc9p/nNP+cy45fTYirmlnxRldNNiFS9K+iFjb6zisc5zT/uOc9h/ntP84p/2nrJz68iQzMzMzMyvkosHMzMzMzAq5aLAL1ZZeB2Ad55z2H+e0/zin/cc57T+l5NSfaTAzMzMzs0J+p8HMzMzMzAq5aDAzMzMzs0IuGqynJM2SdJ+kI5JOSvqTpGtz/e+RdEjSfyX9WtLlTdveL+l5Sc9Iuqtp35slPSlpRNLDkpZ2c2wz1RRzeqOkvalvd4t9b5H0uKS6pE3dGZGVlVNJiyX9TtJzko5LelTS1V0c2oxV8uM0JJ1K594RSVu7NKwZrcTH6TtyuWwsIWljF4c345T8GL1e0mMpl3slvaGdmFw0WK/VgL8D64CLgM8CP5S0XNJiYGdqWwjsA36Q2/YLwArgcuBdwCclXQMgaR3wFeCGtO1fgR1dGI9NLafHgG8BX5tk3/uBO4A/lhO6TaKsnI4AHwYuARYAXwd2SaqVNA4bU+bjFGBNRAynZXMZA7AJSslpRPw2l8th4Dqyx+7DZQ7GysmnpBXA94GPAhcDu4CH2jnv+oPQNu1IOgB8EVgEbIqIt6X2IbJfOHxTRByS9BRwa0T8IvV/GVgRER+Q9A1gTkR8PPUtBZ4CXhcRh7s/qpmt3Zzm1t8MfCgi1k+yvz3A1ojYVnLoNokScloBNgAPAUsi4mi5I7BmncqppCA7Fz/ZrdittU4/TtM63wOIiFtLDN1a6EQ+Jd0JXBsRG9L9CnAKuC4iflV0fL/TYNOKpCXAFcBBYCXZK8sARMQp4DCwUtICYGm+P91e2dhVWsjdB1hVTuQ2mXZz2pvo7Hx0OqfpifBFsoJhqwuG7ivhcfqIsstGd0pa3sFQrU1lnHslzQXeDzzQuUitHR3MZ6v5kWhjfuSiwaYNSQNkb5k9kCrlYeBE02ongHmpj6b+Rh/Az4EbJa2WNAf4HBDA3JLCtxbOMad2ASgjpxGxGpgP3ATs6VCo1qYScroOWA5cCTwN/MyXnHVXiefejWSvaP9mykFa2zqcz18C6yStlzQIfBoYpI35kYsGmxbS22MPAi8Dd6bmEbKJRN584GTqo6m/0Ud6i+3zwI+BI8DfUt8/Oh+9tXIeObVprsycRsSLEbEDuFvSmqnGau0pI6cR8UhEvBwRx4FPAK8GrupMxHY2JZ97bwG2h69t75pO5zMVHbcA3wH+CSwG/kwb8yMXDdZzkgTcBywBNkbE/1LXQWBNbr0h4LXAwYj4D9l/9vzkYk3aBoCIuCciVkTEK8iKhxrwWJljscz55LTrQdo56WJOB4DXTCFUa1MXcxqMvxzCSlJmTiW9ClgPbO9UvFasrHxGxI8iYlVELF0pkBMAAAGTSURBVCJ7gfVy4A9n285Fg00H3yV7Fer6iHgh1/4TYJWkjZJmk11idCD3IZ/twGckLZB0JXAbsA1A0mxJq5RZRvaT6t9OxYaV77xyKqma2mtAJeVxoLGxpMHUL2Ag9fs81h0dz6mkt0p6e8rrHEmfInty/H03BzaDlZHTlZLemNYZBr5J9iUUf+niuGayUs69yc3AXn+ZSFeV9Vz6lrTOJcC9wK78B6gnFRFevPRsIatug+xDkCO55YOp/73AIeAFYDewPLftLOB+4HngX8Bdub6LgQNk3wjwDPBVoNrr8c6EZYo53ZS2zS/bcv27W/Sv7/WY+30pK6dk177vJ3tL/RjZddLv7PV4Z8JSYk7fDTyezr1HgZ+SfZNSz8fc70uZ5960ziHgI70e50xZSn4u3ZM7794LDLUTk79y1czMzMzMCvltfTMzMzMzK+SiwczMzMzMCrloMDMzMzOzQi4azMzMzMyskIsGMzMzMzMr5KLBzMzMzMwKuWgwMzMzM7NCLhrMzMzMzKyQiwYzMzMzMyv0fxhNFwj9g2zmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 792x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = data_sorted[\"datetime\"] #first figure\n",
    "y1 = data_sorted[\"cum_ratings\"]\n",
    "x2 = data_sorted_sept[\"datetime\"] #second figure\n",
    "y2 = data_sorted_sept[\"cum_ratings\"]\n",
    "\n",
    "\n",
    "#creating canvas for figures\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(11,5))\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "\n",
    "\n",
    "ax1.plot(x1,y1)\n",
    "ax1.plot(x2, y2, color = 'red')\n",
    "ax1.set_ylabel(\"Year\", fontsize=16)\n",
    "ax1.set_ylabel(\"Cumulative reviews\", fontsize=16)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "#plt.yticks([ 0, 1, 2, 3, 4, 5])\n",
    "ax1.spines['top'].set_visible(False) #remove borders\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['bottom'].set_visible(True)\n",
    "ax1.spines['left'].set_visible(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(\"CumReviewsTimeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the length of the reviews in characters grouped by rating and time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_cat</th>\n",
       "      <th>rating_binary</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Before Sept 2014</th>\n",
       "      <th>0</th>\n",
       "      <td>733.0</td>\n",
       "      <td>637.889495</td>\n",
       "      <td>584.204344</td>\n",
       "      <td>16.0</td>\n",
       "      <td>222.00</td>\n",
       "      <td>461.0</td>\n",
       "      <td>881.00</td>\n",
       "      <td>3790.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>292.0</td>\n",
       "      <td>232.643836</td>\n",
       "      <td>313.682238</td>\n",
       "      <td>13.0</td>\n",
       "      <td>73.00</td>\n",
       "      <td>145.0</td>\n",
       "      <td>271.00</td>\n",
       "      <td>3770.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Sept 2014</th>\n",
       "      <th>0</th>\n",
       "      <td>1288.0</td>\n",
       "      <td>283.033385</td>\n",
       "      <td>337.119094</td>\n",
       "      <td>12.0</td>\n",
       "      <td>75.00</td>\n",
       "      <td>168.0</td>\n",
       "      <td>354.25</td>\n",
       "      <td>3053.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120.0</td>\n",
       "      <td>203.683333</td>\n",
       "      <td>219.604227</td>\n",
       "      <td>14.0</td>\n",
       "      <td>68.75</td>\n",
       "      <td>130.5</td>\n",
       "      <td>247.00</td>\n",
       "      <td>1344.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">After Sept 2014</th>\n",
       "      <th>0</th>\n",
       "      <td>661.0</td>\n",
       "      <td>680.944024</td>\n",
       "      <td>614.563685</td>\n",
       "      <td>13.0</td>\n",
       "      <td>246.00</td>\n",
       "      <td>514.0</td>\n",
       "      <td>932.00</td>\n",
       "      <td>4069.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>828.0</td>\n",
       "      <td>211.143720</td>\n",
       "      <td>239.732474</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67.75</td>\n",
       "      <td>134.5</td>\n",
       "      <td>265.25</td>\n",
       "      <td>2058.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 count        mean         std   min     25%  \\\n",
       "time_cat         rating_binary                                                 \n",
       "Before Sept 2014 0               733.0  637.889495  584.204344  16.0  222.00   \n",
       "                 1               292.0  232.643836  313.682238  13.0   73.00   \n",
       "Sept 2014        0              1288.0  283.033385  337.119094  12.0   75.00   \n",
       "                 1               120.0  203.683333  219.604227  14.0   68.75   \n",
       "After Sept 2014  0               661.0  680.944024  614.563685  13.0  246.00   \n",
       "                 1               828.0  211.143720  239.732474  12.0   67.75   \n",
       "\n",
       "                                  50%     75%     max  \n",
       "time_cat         rating_binary                         \n",
       "Before Sept 2014 0              461.0  881.00  3790.0  \n",
       "                 1              145.0  271.00  3770.0  \n",
       "Sept 2014        0              168.0  354.25  3053.0  \n",
       "                 1              130.5  247.00  1344.0  \n",
       "After Sept 2014  0              514.0  932.00  4069.0  \n",
       "                 1              134.5  265.25  2058.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['time_cat', 'rating_binary']).describe()['len_rev']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the two days with the largest amount of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jolien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: 'year' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Jolien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: 'month' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Jolien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: 'day' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Afinn_binary</th>\n",
       "      <th>Afinn_score</th>\n",
       "      <th>Company</th>\n",
       "      <th>after_sept14</th>\n",
       "      <th>before_sept14</th>\n",
       "      <th>datePublished</th>\n",
       "      <th>datetime</th>\n",
       "      <th>daymean_afinn</th>\n",
       "      <th>daymean_rating</th>\n",
       "      <th>daysum_afinn</th>\n",
       "      <th>daysum_rating</th>\n",
       "      <th>headline</th>\n",
       "      <th>inLanguage</th>\n",
       "      <th>len_rev</th>\n",
       "      <th>ratingValue</th>\n",
       "      <th>rating_binary</th>\n",
       "      <th>rating_c</th>\n",
       "      <th>reviewBody</th>\n",
       "      <th>sept14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_rev</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <th>2014</th>\n",
       "      <th>9</th>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>www.jensens.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-09-22T22:40:58Z</td>\n",
       "      <td>2014-09-22 22:40:58</td>\n",
       "      <td>-1.327511</td>\n",
       "      <td>-0.792576</td>\n",
       "      <td>-608.0</td>\n",
       "      <td>-363</td>\n",
       "      <td>👍👍👍</td>\n",
       "      <td>da</td>\n",
       "      <td>3053</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Øv oplevelse</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <th>2014</th>\n",
       "      <th>9</th>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>www.jensens.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-09-20T23:52:31Z</td>\n",
       "      <td>2014-09-20 23:52:31</td>\n",
       "      <td>-1.324380</td>\n",
       "      <td>-0.929752</td>\n",
       "      <td>-641.0</td>\n",
       "      <td>-450</td>\n",
       "      <td>Øv med mere øv på.</td>\n",
       "      <td>da</td>\n",
       "      <td>2386</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Øv, Eat all can sparreribs eksistere ikke læng...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Afinn_binary  Afinn_score          Company  \\\n",
       "no_rev year month day                                               \n",
       "458    2014 9     22              1          7.0  www.jensens.com   \n",
       "484    2014 9     20              1          4.0  www.jensens.com   \n",
       "\n",
       "                       after_sept14  before_sept14         datePublished  \\\n",
       "no_rev year month day                                                      \n",
       "458    2014 9     22            0.0            0.0  2014-09-22T22:40:58Z   \n",
       "484    2014 9     20            0.0            0.0  2014-09-20T23:52:31Z   \n",
       "\n",
       "                                 datetime  daymean_afinn  daymean_rating  \\\n",
       "no_rev year month day                                                      \n",
       "458    2014 9     22  2014-09-22 22:40:58      -1.327511       -0.792576   \n",
       "484    2014 9     20  2014-09-20 23:52:31      -1.324380       -0.929752   \n",
       "\n",
       "                       daysum_afinn  daysum_rating            headline  \\\n",
       "no_rev year month day                                                    \n",
       "458    2014 9     22         -608.0           -363                 👍👍👍   \n",
       "484    2014 9     20         -641.0           -450  Øv med mere øv på.   \n",
       "\n",
       "                      inLanguage  len_rev  ratingValue  rating_binary  \\\n",
       "no_rev year month day                                                   \n",
       "458    2014 9     22          da     3053            5              1   \n",
       "484    2014 9     20          da     2386            5              1   \n",
       "\n",
       "                       rating_c  \\\n",
       "no_rev year month day             \n",
       "458    2014 9     22          1   \n",
       "484    2014 9     20          1   \n",
       "\n",
       "                                                              reviewBody  \\\n",
       "no_rev year month day                                                      \n",
       "458    2014 9     22                                        Øv oplevelse   \n",
       "484    2014 9     20   Øv, Eat all can sparreribs eksistere ikke læng...   \n",
       "\n",
       "                       sept14  \n",
       "no_rev year month day          \n",
       "458    2014 9     22      1.0  \n",
       "484    2014 9     20      1.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['no_rev', 'year', 'month', 'day']).max()[-2:]\n",
    "#error because groupby variables are both columns and indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of reviews over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jolien\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  warnings.warn(\"This figure includes Axes that are not compatible \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating variables for plots\n",
    "data_sept = data[data[\"sept14\"] == 1]\n",
    "data_before = data[data['before_sept14'] == 1]\n",
    "data_after = data[data['after_sept14'] == 1]\n",
    "\n",
    "x1 = data[\"datetime\"] #first figure\n",
    "y1 = data[\"no_rev\"]\n",
    "x2 = data_sept[\"datetime\"] #second figure\n",
    "y2 = data_sept[\"no_rev\"]\n",
    "\n",
    "\n",
    "#creating canvas for figures\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(11,5))\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "\n",
    "\n",
    "ax1.plot(x1,y1)\n",
    "ax1.plot(x2, y2, color = 'red')\n",
    "ax1.set_ylabel(\"Year\", fontsize=16)\n",
    "ax1.set_ylabel(\"Review count\", fontsize=16)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax1.spines['top'].set_visible(False) #remove borders\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['bottom'].set_visible(True)\n",
    "ax1.spines['left'].set_visible(True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "myFmt = mdates.DateFormatter('%d/%m/%y')\n",
    "\n",
    "ax2 = fig.add_axes([0.18, 0.6, 0.25, 0.3])\n",
    "ax2.plot(x2, y2)\n",
    "ax2.xaxis.set_major_formatter(myFmt)\n",
    "ax2.plot(x2, y2, color=\"red\")\n",
    "ax2.xaxis.set_major_locator(ticker.MaxNLocator(4))\n",
    "\n",
    "ax2.tick_params(\n",
    "    axis='x',          \n",
    "    which='both',     \n",
    "    bottom=True,      \n",
    "    top=False,         \n",
    "    labelbottom=True, direction='out', length=3, width=3, labelrotation=0.12) \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(\"ReviewTimeline.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the increase/decrease in stars over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_cat</th>\n",
       "      <th>rating_binary</th>\n",
       "      <th>ratingValue</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Before Sept 2014</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>379.0</td>\n",
       "      <td>4.709763</td>\n",
       "      <td>13.738302</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>217.0</td>\n",
       "      <td>8.797235</td>\n",
       "      <td>20.855663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137.0</td>\n",
       "      <td>14.948905</td>\n",
       "      <td>29.136929</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>4</th>\n",
       "      <td>156.0</td>\n",
       "      <td>18.576923</td>\n",
       "      <td>31.644811</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.75</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>136.0</td>\n",
       "      <td>14.169118</td>\n",
       "      <td>27.847350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Sept 2014</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>1209.0</td>\n",
       "      <td>382.898263</td>\n",
       "      <td>143.879165</td>\n",
       "      <td>1.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>458.0</td>\n",
       "      <td>484.00</td>\n",
       "      <td>484.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63.0</td>\n",
       "      <td>337.015873</td>\n",
       "      <td>170.571382</td>\n",
       "      <td>15.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>458.0</td>\n",
       "      <td>458.00</td>\n",
       "      <td>484.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>283.125000</td>\n",
       "      <td>219.433475</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>458.0</td>\n",
       "      <td>458.00</td>\n",
       "      <td>484.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>4</th>\n",
       "      <td>26.0</td>\n",
       "      <td>232.153846</td>\n",
       "      <td>214.102722</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>458.00</td>\n",
       "      <td>458.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>94.0</td>\n",
       "      <td>277.021277</td>\n",
       "      <td>205.998537</td>\n",
       "      <td>3.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>458.0</td>\n",
       "      <td>458.00</td>\n",
       "      <td>484.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">After Sept 2014</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>438.0</td>\n",
       "      <td>2.979452</td>\n",
       "      <td>5.124718</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122.0</td>\n",
       "      <td>5.180328</td>\n",
       "      <td>11.224984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101.0</td>\n",
       "      <td>8.099010</td>\n",
       "      <td>14.514479</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>4</th>\n",
       "      <td>241.0</td>\n",
       "      <td>13.921162</td>\n",
       "      <td>18.439259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>587.0</td>\n",
       "      <td>14.328790</td>\n",
       "      <td>20.400326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.50</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             count        mean         std  \\\n",
       "time_cat         rating_binary ratingValue                                   \n",
       "Before Sept 2014 0             1             379.0    4.709763   13.738302   \n",
       "                               2             217.0    8.797235   20.855663   \n",
       "                               3             137.0   14.948905   29.136929   \n",
       "                 1             4             156.0   18.576923   31.644811   \n",
       "                               5             136.0   14.169118   27.847350   \n",
       "Sept 2014        0             1            1209.0  382.898263  143.879165   \n",
       "                               2              63.0  337.015873  170.571382   \n",
       "                               3              16.0  283.125000  219.433475   \n",
       "                 1             4              26.0  232.153846  214.102722   \n",
       "                               5              94.0  277.021277  205.998537   \n",
       "After Sept 2014  0             1             438.0    2.979452    5.124718   \n",
       "                               2             122.0    5.180328   11.224984   \n",
       "                               3             101.0    8.099010   14.514479   \n",
       "                 1             4             241.0   13.921162   18.439259   \n",
       "                               5             587.0   14.328790   20.400326   \n",
       "\n",
       "                                             min    25%    50%     75%    max  \n",
       "time_cat         rating_binary ratingValue                                     \n",
       "Before Sept 2014 0             1             1.0    1.0    2.0    3.00   87.0  \n",
       "                               2             1.0    1.0    2.0    5.00   87.0  \n",
       "                               3             1.0    2.0    3.0    7.00   87.0  \n",
       "                 1             4             1.0    2.0    3.0   11.75   87.0  \n",
       "                               5             1.0    2.0    3.0    8.00   87.0  \n",
       "Sept 2014        0             1             1.0  255.0  458.0  484.00  484.0  \n",
       "                               2            15.0  255.0  458.0  458.00  484.0  \n",
       "                               3             3.0   17.0  458.0  458.00  484.0  \n",
       "                 1             4             3.0   29.5   92.0  458.00  458.0  \n",
       "                               5             3.0   46.0  458.0  458.00  484.0  \n",
       "After Sept 2014  0             1             1.0    1.0    2.0    3.00   63.0  \n",
       "                               2             1.0    1.0    2.0    4.00   63.0  \n",
       "                               3             1.0    1.0    2.0    9.00   63.0  \n",
       "                 1             4             1.0    3.0    8.0   15.00   63.0  \n",
       "                               5             1.0    2.0    5.0   13.50   63.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['time_cat', 'rating_binary', 'ratingValue']).describe()['no_rev'] #use the count column to get relative percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Implement tokenization using TfidfVectorizor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First split the data into training, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(717,)\n",
      "(308,)\n",
      "(985,)\n",
      "(423,)\n",
      "(1044,)\n",
      "(448,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "\n",
    "data_before_d, data_before_t, y_before_d, y_before_t = train_test_split(data_before, \n",
    "                                                                        data_before[['rating_binary', 'Afinn_binary']],\n",
    "                                                                        test_size=0.3, random_state=1)\n",
    "\n",
    "data_sept_d, data_sept_t, y_sept_d, y_sept_t = train_test_split(data_sept, \n",
    "                                                                data_sept[['rating_binary', 'Afinn_binary']],\n",
    "                                                                test_size=0.3, random_state=1)\n",
    "\n",
    "data_after_d, data_after_t, y_after_d, y_after_t = train_test_split(data_after, \n",
    "                                                                    data_after[['rating_binary', 'Afinn_binary']],\n",
    "                                                                    test_size=0.3, random_state=1)    \n",
    "\n",
    "\n",
    "#Extract the reviews from the split data\n",
    "X_before_dev = data_before_d['reviewBody']\n",
    "X_before_test = data_before_t['reviewBody']\n",
    "X_sept_dev = data_sept_d['reviewBody']\n",
    "X_sept_test = data_sept_t['reviewBody']\n",
    "X_after_dev = data_after_d['reviewBody']\n",
    "X_after_test = data_after_t['reviewBody']\n",
    "\n",
    "#Split y_t and y_d into rating and afinn scores\n",
    "y_dev_b = y_before_d['rating_binary']\n",
    "y_test_b = y_before_t['rating_binary']\n",
    "y_dev_d = y_sept_d['rating_binary']\n",
    "y_test_d = y_sept_t['rating_binary']\n",
    "y_dev_a = y_after_d['rating_binary']\n",
    "y_test_a = y_after_t['rating_binary']\n",
    "\n",
    "y_before_afinn_check_dev = y_before_d['Afinn_binary']\n",
    "y_before_afinn_check_test = y_before_t['Afinn_binary']\n",
    "y_sept_afinn_check_dev = y_sept_d['Afinn_binary']\n",
    "y_sept_afinn_check_test = y_sept_t['Afinn_binary']\n",
    "y_after_afinn_check_dev = y_after_d['Afinn_binary']\n",
    "y_after_afinn_check_test = y_after_t['Afinn_binary']\n",
    "\n",
    "#Check size of data before, during and after sept 2014\n",
    "print(y_dev_b.shape)\n",
    "print(y_test_b.shape)\n",
    "print(y_dev_d.shape)\n",
    "print(y_test_d.shape)\n",
    "print(y_dev_a.shape)\n",
    "print(y_test_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens before September 2014:  6238\n",
      "Number of tokens during September 2014:  5059\n",
      "Number of tokens after September 2014:  6941\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "def tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z]\", \" \", str_input).lower().split() #only include words, no - or numbers or other special signs\n",
    "    return words\n",
    "\n",
    "vectorizer = TfidfVectorizer(preprocessor=None, tokenizer=nltk.word_tokenize)\n",
    "#vectorizer = TfidfVectorizer(preprocessor=None, tokenizer=tokenizer)\n",
    "\n",
    "vectorizer.fit(X_before_dev)\n",
    "X_dev_vec_b = vectorizer.transform(X_before_dev)\n",
    "X_test_vec_b = vectorizer.transform(X_before_test)\n",
    "tokens_b = vectorizer.get_feature_names() #Get the individual tokens\n",
    "print('Number of tokens before September 2014: ', len(tokens_b)) #Number of tokens\n",
    "\n",
    "vectorizer.fit(X_sept_dev)\n",
    "X_dev_vec_d = vectorizer.transform(X_sept_dev)\n",
    "X_test_vec_d = vectorizer.transform(X_sept_test)\n",
    "tokens_d = vectorizer.get_feature_names() #Get the individual tokens\n",
    "print('Number of tokens during September 2014: ', len(tokens_d)) #Number of tokens\n",
    "\n",
    "vectorizer.fit(X_after_dev)\n",
    "X_dev_vec_a = vectorizer.transform(X_after_dev)\n",
    "X_test_vec_a = vectorizer.transform(X_after_test)\n",
    "tokens_a = vectorizer.get_feature_names() #Get the individual tokens\n",
    "print('Number of tokens after September 2014: ', len(tokens_a)) #Number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Define functions for algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to compute accuracy of a fitted model (binary outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(fittedModel, X, y):\n",
    "    \n",
    "    y_pred = fittedModel.predict(X)\n",
    "\n",
    "    TP = sum(np.where(y + y_pred == 2, 1, 0))\n",
    "    TN = sum(np.where(y + y_pred == 0, 1, 0))\n",
    "    FP = sum(np.where((y == 0) & (y_pred == 1), 1, 0))\n",
    "    FN = sum(np.where((y == 1) & (y_pred == 0), 1, 0))\n",
    "    \n",
    "    ACC = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "    return(ACC)\n",
    "\n",
    "def A_afinn(y_pred, y):\n",
    "    \n",
    "    TP = sum(np.where(y + y_pred == 2, 1, 0))\n",
    "    TN = sum(np.where(y + y_pred == 0, 1, 0))\n",
    "    FP = sum(np.where((y == 0) & (y_pred == 1), 1, 0))\n",
    "    FN = sum(np.where((y == 1) & (y_pred == 0), 1, 0))\n",
    "    \n",
    "    ACC = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "    return(ACC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define classifiers (binary outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "logreg_lib = LogisticRegression(penalty = 'l1', C = 5, random_state = 1) #liberal regularization\n",
    "\n",
    "#We have to set dual to False in order to get 'l1' penalty.\n",
    "svc_lib = LinearSVC(penalty = 'l1', C = 5, random_state = 1, dual = False) #liberal regularization\n",
    "\n",
    "NB = BernoulliNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Compute accuracy of Afinn scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy Afinn score before September 2014:  0.6903765690376569\n",
      "Training accuracy Afinn score during September 2014:  0.8903553299492386\n",
      "Training accuracy Afinn score after September 2014:  0.5957854406130269 \n",
      "\n",
      "Test accuracy Afinn score before September 2014:  0.724025974025974\n",
      "Test accuracy Afinn score during September 2014:  0.8983451536643026\n",
      "Test accuracy Afinn score after September 2014:  0.6138392857142857\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy Afinn score before September 2014: ', A_afinn(y_before_afinn_check_dev, y = y_dev_b))\n",
    "print('Training accuracy Afinn score during September 2014: ', A_afinn(y_sept_afinn_check_dev, y = y_dev_d))\n",
    "print('Training accuracy Afinn score after September 2014: ', A_afinn(y_after_afinn_check_dev, y = y_dev_a), '\\n')\n",
    "\n",
    "print('Test accuracy Afinn score before September 2014: ', A_afinn(y_before_afinn_check_test, y = y_test_b))\n",
    "print('Test accuracy Afinn score during September 2014: ', A_afinn(y_sept_afinn_check_test, y = y_test_d))\n",
    "print('Test accuracy Afinn score after September 2014: ', A_afinn(y_after_afinn_check_test, y = y_test_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# Classification before September 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Implement the logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we naïvely fit the logistic regression classifier to the training data. We choose a value of 5 for the regularization parameter `C`. This is actually the inverse of the regularization strength so a lower value means stronger regularization. We use the `l1` penalty because we want to have a sparse solution (we want to set some coefficients to 0) for interpretation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9762900976290098\n"
     ]
    }
   ],
   "source": [
    "#First we naïvely fit the model:\n",
    "sentiment_logreg_lib = logreg_lib.fit(X_dev_vec_b, y_dev_b) \n",
    "\n",
    "#Investigate the accuracy of the training data\n",
    "print('Training Accuracy: ', A(sentiment_logreg_lib, X_dev_vec_b, np.array(y_dev_b))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy value of the training data is very high and there might be a large difference with the test data. Perhaps we should fit a learning curve to investigate the bias and variance of the model and see whether we are over/under fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= logreg_lib,\n",
    "                   X=X_dev_vec_b,\n",
    "                   y=y_dev_b,\n",
    "                   train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                   scoring=A,                 \n",
    "                   cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Train      Test\n",
      "sample size                    \n",
      "64           0.967187  0.776800\n",
      "129          0.970543  0.803286\n",
      "193          0.967358  0.824276\n",
      "258          0.955426  0.820090\n",
      "322          0.959006  0.818564\n"
     ]
    }
   ],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('This learning curve shows too much variance:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the learning curve shows a lot of variance we want to see whether we can optimize the regularization parameter of the logistic regression model. In particular we should consider setting the regularization parameter to a more conservative (thus lower) value.  We do this by making a validation curve plotting the training and test accuracy for several value of `C`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "Cs = np.linspace(0.001, 5, 20)\n",
    "\n",
    "train_scores_val, test_scores_val = \\\n",
    "    validation_curve(estimator= logreg_lib,\n",
    "                     X=X_dev_vec_b,\n",
    "                     y=y_dev_b,\n",
    "                     param_name='C',\n",
    "                     param_range=Cs,\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_val.mean(axis=1),\n",
    "                     'Test':test_scores_val.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(Cs,name='regularization'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(Cs,\n",
    "                train_scores_val.mean(1) + train_scores_val.std(1)*1.96,\n",
    "                train_scores_val.mean(1) - train_scores_val.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the average prediction accuracy (between training and test set) starts decreasing steeply after a `C` value of 1. Lets plot a more detailed validation curve for values lower than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.linspace(0.001, 1, 20)\n",
    "\n",
    "train_scores_val, test_scores_val = \\\n",
    "    validation_curve(estimator= logreg_lib,\n",
    "                     X=X_dev_vec_b,\n",
    "                     y=y_dev_b,\n",
    "                     param_name='C',\n",
    "                     param_range=Cs,\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_val.mean(axis=1),\n",
    "                     'Test':test_scores_val.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(Cs,name='regularization'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(Cs,\n",
    "                train_scores_val.mean(1) + train_scores_val.std(1)*1.96,\n",
    "                train_scores_val.mean(1) - train_scores_val.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a regularization `C` = 0.6 setting for which the test accuracy is in the confidence band of the training accuracy to see whether the learning curves improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_mod = LogisticRegression(penalty = 'l1', C = 0.6, random_state = 1) #moderate regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moderate setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= logreg_mod,\n",
    "                   X=X_dev_vec_b,\n",
    "                   y=y_dev_b,\n",
    "                   train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                   scoring=A,                 \n",
    "                   cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size')) \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('This learning curve looks ok regarding the size of our training data:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit final logistic regression classification model to training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the final model, with `C` = 0.6 to our training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_logreg_mod = logreg_mod.fit(X_dev_vec_b, y_dev_b)\n",
    "print('Training Accuracy: ', A(sentiment_logreg_mod, X_dev_vec_b, y_dev_b))\n",
    "print('Test Accuracy: ', A(sentiment_logreg_mod, X_test_vec_b, y_test_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is much lower and it is reasonably close to the test accuracy. We can now investigate the estimated coeficients to the test data to see which tokens predict positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put tokens, coefficients and odds ratios in one dataframe\n",
    "coef_tokens = pd.concat([pd.Series(tokens_b), pd.DataFrame(sentiment_logreg_mod.coef_.T), pd.DataFrame(np.exp(sentiment_logreg_mod.coef_).T)], axis = 1) \n",
    "coef_tokens.columns = ['Token', 'Coef', 'OR']\n",
    "\n",
    "#Count the amount of tokens with non-zero coefficient value\n",
    "print((coef_tokens['Coef'] != 0).value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef_tokens = coef_tokens[coef_tokens.Coef != 0] #remove tokens with zero coefficient value and exponentiate to get odds ratio\n",
    "coef_tokens_sorted = coef_tokens.sort_values('Coef')\n",
    "print(coef_tokens_sorted[['Token', 'Coef', 'OR']]) #sort and print to get highest and lowest coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_n = 10\n",
    "top_features_p = 8\n",
    "top_features = 18\n",
    "\n",
    "top_pos_coef = coef_tokens_sorted.iloc[-top_features_p:]\n",
    "top_neg_coef = coef_tokens_sorted.iloc[:top_features_n]\n",
    "top = pd.concat([top_pos_coef, top_neg_coef], axis = 0).sort_values('Coef')\n",
    "print(top)\n",
    "colors = ['red' if c > 0 else 'blue' for c in top['Coef']]\n",
    "\n",
    "\n",
    "barplot, ax = plt.subplots(figsize=(10,4))\n",
    "plt.tight_layout()\n",
    "plt.bar(np.arange(top_features), top['Coef'], color=colors)\n",
    "features = np.array(top['Token'])\n",
    "plt.xticks(np.arange(0, top_features), top['Token'], rotation=60, ha='right')\n",
    "plt.yticks([-10, -5, 0, 5, 10])\n",
    "ax.spines['top'].set_visible(False) #remove borders\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "#plt.tick_params(top='off', right='off')#control tick marks\n",
    "ax.axhline(y=0, xmin=0, xmax=20, color = 'black', linestyle = 'dashed', linewidth = 1)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12) #change size of tick labels\n",
    "ax.set_ylabel(\"Coefficient\", fontsize=16)\n",
    "plt.gcf().subplots_adjust(bottom=0.22, left = 0.1)\n",
    "plt.savefig('CoefficientsLogistic_b.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Implement the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we naïvely fit the Naive Bayes classifier to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_NB = NB.fit(X_dev_vec_b.toarray(), y_dev_b) #need to convert to np array, not sparse\n",
    "\n",
    "print('Training Accuracy: ', A(sentiment_NB, X_dev_vec_b.toarray(), np.array(y_dev_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy value of both the training data is low. Perhaps we should fit a learning curve to investigate the bias and variance of the model and see whether we are over/under fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= NB,\n",
    "                   X=X_dev_vec_b.toarray(),\n",
    "                   y=y_dev_b,\n",
    "                   train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                   scoring=A,                 \n",
    "                   cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('There is too much variance and bias according to this learning curve:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the learning curve too much variance and bias we want to see whether we can optimize the prior of the Naive Bayes classifier. This may be a controversial idea. Ideally, when we fit Bayesian models we need to specify a prior, that reflects our belief about the 'truth', in this case the proportion of negative and positive sentiments, before we fit a model. These prior probabilities could for example be derived from our knowledge of the proportions in the wider population, perhaps gained from  the literature. It would thus be odd to optimize a model based on the prior. We may also consider setting the prior to the proportions from the training set; this is what is done by default in the Naive Bayes classifier from sklearn. According to http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.59.2085&rep=rep1&type=pdf putting a uniform prior may actually improve performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_unif = BernoulliNB(class_prior = [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= NB_unif,\n",
    "                   X=X_dev_vec_b.toarray(),\n",
    "                   y=y_dev_b,\n",
    "                   train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                   scoring=A,                 \n",
    "                   cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('This looks very much like the curve above:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since adapting the prior does not see to change the learning curve a lot we could try to optimize the smoothing parameter `alpha`. This parameter determines the smooting of the prior and accounts for features not present in the learning samples and prevents zero probabilities in further computations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(0.001, 5, 20)\n",
    "\n",
    "train_scores_val, test_scores_val = \\\n",
    "    validation_curve(estimator= NB,\n",
    "                     X=X_dev_vec_b,\n",
    "                     y=y_dev_b,\n",
    "                     param_name='alpha',\n",
    "                     param_range=alphas,\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_val.mean(axis=1),\n",
    "                     'Test':test_scores_val.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(alphas,name='smoothing'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(alphas,\n",
    "                train_scores_val.mean(1) + train_scores_val.std(1)*1.96,\n",
    "                train_scores_val.mean(1) - train_scores_val.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "print('There seems to be a decrease in bias for lower alpha values, \\nthe variance however becomes larger:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best `alpha` for which the training accuracy is still within the confidence bands of the training accuracy is about 2.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the final naive Bayes classifier to the training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the final model to our training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_adj_alpha = BernoulliNB(alpha = 2.5)\n",
    "\n",
    "sentiment_NB_adj = NB_adj_alpha.fit(X_dev_vec_b.toarray(), np.array(y_dev_b)) \n",
    "\n",
    "print('Training Accuracy: ', A(sentiment_NB_adj, X_dev_vec_b.toarray(), np.array(y_dev_b)))\n",
    "print('Test Accuracy: ', A(sentiment_NB_adj, X_test_vec_b.toarray(), np.array(y_test_b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put tokens and coefficients in one dataframe\n",
    "feat_prob = pd.DataFrame(np.exp(sentiment_NB.feature_log_prob_).T)\n",
    "feat_prob.columns = ['Negative', \"Positive\"]\n",
    "\n",
    "#compute the ratio of probability of being predicted as a negative vs positive sentiment divided by the sum of probabilities\n",
    "feat_prob_ratio_neg = (feat_prob['Negative']/feat_prob['Positive'])*(feat_prob['Negative'])\n",
    "feat_prob_ratio_pos = (feat_prob['Positive']/feat_prob['Negative'])*(feat_prob['Positive'])\n",
    "\n",
    "coef_tokens = pd.concat([pd.Series(tokens_b), feat_prob, feat_prob_ratio_neg, feat_prob_ratio_pos], axis = 1) \n",
    "coef_tokens.columns = ['Token', 'Negative', \"Positive\", \"RatioNP\", 'RatioPN']\n",
    "\n",
    "\n",
    "#Sort\n",
    "\n",
    "coef_tokens['highest'] = abs(coef_tokens[[\"RatioNP\", \"RatioPN\"]]).max(axis=1)\n",
    "coef_high = coef_tokens.sort_values('highest', ascending = False)\n",
    "coef_high['color'] = (coef_high['RatioNP'] > coef_high['RatioPN']).replace([False, True], ['red', 'blue'])\n",
    "coef_high_sorted_neg = coef_tokens.sort_values(['RatioNP'], ascending = False)\n",
    "coef_high_sorted_pos = coef_tokens.sort_values(['RatioPN'], ascending = False)\n",
    "\n",
    "#Print\n",
    "print(coef_high.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = 20\n",
    "\n",
    "top = coef_high.iloc[:top_features].sort_values(['RatioNP','highest'], ascending = False)\n",
    "colors = top['color']\n",
    "\n",
    "barplot, ax = plt.subplots(figsize=(10,4))\n",
    "plt.tight_layout()\n",
    "plt.bar(np.arange(top_features), top.highest, color = colors)\n",
    "features = top.Token\n",
    "plt.xticks(np.arange(0, top_features), features, rotation=60, ha='right')\n",
    "\n",
    "ax.spines['top'].set_visible(False) #remove borders\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=12) #change size of tick labels\n",
    "ax.set_ylabel(\"Discrimination\", fontsize=16)\n",
    "\n",
    "print(features)\n",
    "plt.gcf().subplots_adjust(bottom=0.22, left = 0.1)\n",
    "plt.savefig('CoefficientsEB_b.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Implement a support vector classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we naïvely fit the support vector classifier to the training data. We choose a value of 5 for the regularization parameter `C`. This is actually the inverse of the regularization strength so a lower value means stronger regularization. We use the `l1` penalty because we want to have a sparse solution (we want to set some coefficients to 0) for interpretation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we naïvely fit the model:\n",
    "sentiment_svc_lib = svc_lib.fit(X_dev_vec_b, y_dev_b) \n",
    "\n",
    "#Investigate the accuracy of the training data\n",
    "print('Training Accuracy: ', A(sentiment_svc_lib, X_dev_vec_b, np.array(y_dev_b))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy value of the training data is very high and there might be a large difference with the test data. Perhaps we should fit a learning curve to investigate the bias and variance of the model and see whether we are over/under fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= svc_lib,\n",
    "                     X=X_dev_vec_b,\n",
    "                     y=y_dev_b,\n",
    "                     train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('This learning has too much variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the learning curve shows a lot of variance we want to see whether we can optimize the regularization parameter of the support vector classifier. In particular we should consider setting the regularization parameter to a more conservative (thus lower) value.  We do this by making a validation curve plotting the training and test accuracy for several value of `C`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.linspace(0.001, 5, 20)\n",
    "\n",
    "train_scores_val, test_scores_val = \\\n",
    "    validation_curve(estimator= svc_lib,\n",
    "                     X=X_dev_vec_b,\n",
    "                     y=y_dev_b,\n",
    "                     param_name='C',\n",
    "                     param_range=Cs,\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_val.mean(axis=1),\n",
    "                     'Test':test_scores_val.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(Cs,name='regularization'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(Cs,\n",
    "                train_scores_val.mean(1) + train_scores_val.std(1)*1.96,\n",
    "                train_scores_val.mean(1) - train_scores_val.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the average prediction accuracy (between training and test set) starts decreasing steeply after a `C` value of 1. Lets plot a more detailed validation curve for values lower than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.linspace(0.001, 1, 20)\n",
    "\n",
    "train_scores_val, test_scores_val = \\\n",
    "    validation_curve(estimator= svc_lib,\n",
    "                     X=X_dev_vec_b,\n",
    "                     y=y_dev_b,\n",
    "                     param_name='C',\n",
    "                     param_range=Cs,\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_val.mean(axis=1),\n",
    "                     'Test':test_scores_val.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(Cs,name='regularization'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(Cs,\n",
    "                train_scores_val.mean(1) + train_scores_val.std(1)*1.96,\n",
    "                train_scores_val.mean(1) - train_scores_val.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a regularization setting `C` = 0.15 for which the test accuracy is still within the confidence bands of the training accuracy to see whether the learning curves improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_mod = LinearSVC(penalty = 'l1', C = 0.15, random_state = 1, dual = False) #moderate regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= svc_mod,\n",
    "                     X=X_dev_vec_b,\n",
    "                     y=y_dev_b,\n",
    "                     train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('Seems ok for our sample size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit final support vector classification model to training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the final model, with `C` = 0.15 to our training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_svc_mod = svc_mod.fit(X_dev_vec_b, y_dev_b)\n",
    "print('Training Accuracy: ', A(sentiment_svc_mod, X_dev_vec_b, y_dev_b))\n",
    "print('Test Accuracy: ', A(sentiment_svc_mod, X_test_vec_b, y_test_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and test accuracy are now much lower and closer together. We can now investigate the estimated coeficients to the test data to see which tokens predict positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put tokens and coefficients in one dataframe\n",
    "coef_tokens = pd.concat([pd.Series(tokens_b), pd.DataFrame(sentiment_svc_mod.coef_.T)], axis = 1) \n",
    "coef_tokens.columns = ['Token', 'Coef']\n",
    "\n",
    "#Count the amount of tokens with non-zero coefficient value\n",
    "print((coef_tokens['Coef'] != 0).value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_tokens = coef_tokens[coef_tokens.Coef != 0] #remove tokens with zero coefficient value\n",
    "coef_tokens_sorted = coef_tokens.sort_values('Coef')#sort and print to get highest and lowest coefficients\n",
    "print(coef_tokens_sorted[['Token', 'Coef']]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_n = 10\n",
    "top_features_p = 8\n",
    "top_features = 18\n",
    "\n",
    "top_pos_coef = coef_tokens_sorted.iloc[-top_features_p:]\n",
    "top_neg_coef = coef_tokens_sorted.iloc[:top_features_n]\n",
    "top = pd.concat([top_pos_coef, top_neg_coef], axis = 0).sort_values('Coef')\n",
    "print(top)\n",
    "colors = ['red' if c > 0 else 'blue' for c in top['Coef']]\n",
    "\n",
    "\n",
    "barplot, ax = plt.subplots(figsize=(10,4))\n",
    "plt.tight_layout()\n",
    "plt.bar(np.arange(top_features), top['Coef'], color=colors)\n",
    "features = np.array(top['Token'])\n",
    "plt.xticks(np.arange(0, top_features), top['Token'], rotation=60, ha='right')\n",
    "plt.yticks([ -4, -2, 0, 2, 4])\n",
    "ax.spines['top'].set_visible(False) #remove borders\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "#plt.tick_params(top='off', right='off')#control tick marks\n",
    "ax.axhline(y=0, xmin=0, xmax=20, color = 'black', linestyle = 'dashed', linewidth = 1)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12) #change size of tick labels\n",
    "ax.set_ylabel(\"Vector Coordinate\", fontsize=16)\n",
    "\n",
    "plt.gcf().subplots_adjust(bottom=0.22, left = 0.1)\n",
    "plt.savefig('CoefficientsSVC_b.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# Classification during September 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Implement the logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we naïvely fit the logistic regression classifier to the training data. We choose a value of 5 for the regularization parameter `C`. This is actually the inverse of the regularization strength so a lower value means stronger regularization. We use the `l1` penalty because we want to have a sparse solution (we want to set some coefficients to 0) for interpretation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we naïvely fit the model:\n",
    "sentiment_logreg_lib = logreg_lib.fit(X_dev_vec_d, y_dev_d) \n",
    "\n",
    "#Investigate the accuracy of the training data\n",
    "print('Training Accuracy: ', A(sentiment_logreg_lib, X_dev_vec_d, np.array(y_dev_d))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy value of the training data is very high and there might be a large difference with the test data. Perhaps we should fit a learning curve to investigate the bias and variance of the model and see whether we are over/under fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= logreg_lib,\n",
    "                   X=X_dev_vec_d,\n",
    "                   y=y_dev_d,\n",
    "                   train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                   scoring=A,                 \n",
    "                   cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('This learning curve shows too much variance:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the learning curve shows a lot of variance we want to see whether we can optimize the regularization parameter of the logistic regression model. In particular we should consider setting the regularization parameter to a more conservative (thus lower) value.  We do this by making a validation curve plotting the training and test accuracy for several value of `C`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "Cs = np.linspace(0.001, 5, 20)\n",
    "\n",
    "train_scores_val, test_scores_val = \\\n",
    "    validation_curve(estimator= logreg_lib,\n",
    "                     X=X_dev_vec_d,\n",
    "                     y=y_dev_d,\n",
    "                     param_name='C',\n",
    "                     param_range=Cs,\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_val.mean(axis=1),\n",
    "                     'Test':test_scores_val.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(Cs,name='regularization'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(Cs,\n",
    "                train_scores_val.mean(1) + train_scores_val.std(1)*1.96,\n",
    "                train_scores_val.mean(1) - train_scores_val.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot a more detailed validation curve for values lower than 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.linspace(0.001, 2, 20)\n",
    "\n",
    "train_scores_val, test_scores_val = \\\n",
    "    validation_curve(estimator= logreg_lib,\n",
    "                     X=X_dev_vec_d,\n",
    "                     y=y_dev_d,\n",
    "                     param_name='C',\n",
    "                     param_range=Cs,\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_val.mean(axis=1),\n",
    "                     'Test':test_scores_val.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(Cs,name='regularization'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(Cs,\n",
    "                train_scores_val.mean(1) + train_scores_val.std(1)*1.96,\n",
    "                train_scores_val.mean(1) - train_scores_val.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with a regularization parameter very close to 0 we get a very high accuracy!! We select a regularization setting `C` = 0.8 for which the test accuracy is still within the training accuray confidence bands to see whether the learning curves improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_mod = LogisticRegression(penalty = 'l1', C = 0.8, random_state = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= logreg_mod,\n",
    "                   X=X_dev_vec_d,\n",
    "                   y=y_dev_d,\n",
    "                   train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                   scoring=A,                 \n",
    "                   cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size')) \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('This learning curve also looks ok for our sample size: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit final logistic regression classification model to training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the final model, with `C` = 0.8 to our training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_logreg_mod = logreg_mod.fit(X_dev_vec_d, y_dev_d)\n",
    "print('Training Accuracy: ', A(sentiment_logreg_mod, X_dev_vec_d, y_dev_d))\n",
    "print('Test Accuracy: ', A(sentiment_logreg_mod, X_test_vec_d, y_test_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is lower and it is reasonably close to the test accuracy. We can now investigate the estimated coeficients to the test data to see which tokens predict positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put tokens, coefficients and odds ratios in one dataframe\n",
    "coef_tokens = pd.concat([pd.Series(tokens_d), pd.DataFrame(sentiment_logreg_mod.coef_.T), pd.DataFrame(np.exp(sentiment_logreg_mod.coef_).T)], axis = 1) \n",
    "coef_tokens.columns = ['Token', 'Coef', 'OR']\n",
    "\n",
    "#Count the amount of tokens with non-zero coefficient value\n",
    "print((coef_tokens['Coef'] != 0).value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef_tokens = coef_tokens[coef_tokens.Coef != 0] #remove tokens with zero coefficient value and exponentiate to get odds ratio\n",
    "coef_tokens_sorted = coef_tokens.sort_values('Coef')\n",
    "print(coef_tokens_sorted[['Token', 'Coef', 'OR']]) #sort and print to get highest and lowest coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_n = 2\n",
    "top_features_p = 10\n",
    "\n",
    "top_features = 12\n",
    "\n",
    "top_pos_coef = coef_tokens_sorted.iloc[-top_features_p:]\n",
    "top_neg_coef = coef_tokens_sorted.iloc[:top_features_n]\n",
    "top = pd.concat([top_pos_coef, top_neg_coef], axis = 0).sort_values('Coef')\n",
    "print(top)\n",
    "colors = ['red' if c > 0 else 'blue' for c in top['Coef']]\n",
    "\n",
    "\n",
    "barplot, ax = plt.subplots(figsize=(10,4))\n",
    "plt.tight_layout()\n",
    "plt.bar(np.arange(top_features), top['Coef'], color=colors)\n",
    "features = np.array(top['Token'])\n",
    "plt.xticks(np.arange(0, top_features), top['Token'], rotation=60, ha='right')\n",
    "plt.yticks([-10, -5, 0, 5, 10])\n",
    "ax.spines['top'].set_visible(False) #remove borders\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "#plt.tick_params(top='off', right='off')#control tick marks\n",
    "ax.axhline(y=0, xmin=0, xmax=20, color = 'black', linestyle = 'dashed', linewidth = 1)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12) #change size of tick labels\n",
    "ax.set_ylabel(\"Coefficient\", fontsize=16)\n",
    "plt.gcf().subplots_adjust(bottom=0.22, left = 0.1)\n",
    "plt.savefig('CoefficientsLogistic_d.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Implement the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we naïvely fit the Naive Bayes classifier to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_NB = NB.fit(X_dev_vec_d.toarray(), y_dev_d) #need to convert to np array, not sparse\n",
    "\n",
    "print('Training Accuracy: ', A(sentiment_NB, X_dev_vec_d.toarray(), np.array(y_dev_d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy value of the training data is quite high. Perhaps we should fit a learning curve to investigate the bias and variance of the model and see whether we are over/under fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= NB,\n",
    "                   X=X_dev_vec_d.toarray(),\n",
    "                   y=y_dev_d,\n",
    "                   train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                   scoring=A,                 \n",
    "                   cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('This learning curve looks ok:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the learning curve does not show a lot of variance we want to see whether we can optimize the prior of the Naive Bayes classifier. This may be a controversial idea. Ideally, when we fit Bayesian models we need to specify a prior, that reflects our belief about the 'truth', in this case the proportion of negative and positive sentiments, before we fit a model. These prior probabilities could for example be derived from our knowledge of the proportions in the wider population, perhaps gained from  the literature. It would thus be odd to optimize a model based on the prior. We may also consider setting the prior to the proportions from the training set; this is what is done by default in the Naive Bayes classifier from sklearn. According to http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.59.2085&rep=rep1&type=pdf putting a uniform prior may actually improve performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_unif = BernoulliNB(class_prior = [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= NB_unif,\n",
    "                   X=X_dev_vec_d.toarray(),\n",
    "                   y=y_dev_d,\n",
    "                   train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                   scoring=A,                 \n",
    "                   cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('This looks very much like the curve above:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since adapting the prior does not see to change the learning curve a lot we could try to optimize the smoothing parameter `alpha`. This parameter determines the smooting of the prior and accounts for features not present in the learning samples and prevents zero probabilities in further computations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(0.001, 5, 20)\n",
    "\n",
    "train_scores_val, test_scores_val = \\\n",
    "    validation_curve(estimator= NB,\n",
    "                     X=X_dev_vec_d,\n",
    "                     y=y_dev_d,\n",
    "                     param_name='alpha',\n",
    "                     param_range=alphas,\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_val.mean(axis=1),\n",
    "                     'Test':test_scores_val.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(alphas,name='smoothing'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(alphas,\n",
    "                train_scores_val.mean(1) + train_scores_val.std(1)*1.96,\n",
    "                train_scores_val.mean(1) - train_scores_val.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "print('There is a decrease in variance for higher alpha values:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the `alpha` parameter to 3 seems to be optimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the final naive Bayes classifier to the training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the final model to our training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_adj_alpha = BernoulliNB(alpha = 3)\n",
    "\n",
    "sentiment_NB_adj = NB_adj_alpha.fit(X_dev_vec_d.toarray(), np.array(y_dev_d)) \n",
    "#note that it set the alpha to 1.0e-10 to prevent numeric errors\n",
    "\n",
    "print('Training Accuracy: ', A(sentiment_NB_adj, X_dev_vec_d.toarray(), np.array(y_dev_d)))\n",
    "print('Test Accuracy: ', A(sentiment_NB_adj, X_test_vec_d.toarray(), np.array(y_test_d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put tokens and coefficients in one dataframe\n",
    "feat_prob = pd.DataFrame(np.exp(sentiment_NB.feature_log_prob_).T)\n",
    "feat_prob.columns = ['Negative', \"Positive\"]\n",
    "\n",
    "#compute the ratio of probability of being predicted as a negative vs positive sentiment divided by the sum of probabilities\n",
    "feat_prob_ratio_neg = (feat_prob['Negative']/feat_prob['Positive'])*(feat_prob['Negative'])\n",
    "feat_prob_ratio_pos = (feat_prob['Positive']/feat_prob['Negative'])*(feat_prob['Positive'])\n",
    "\n",
    "coef_tokens = pd.concat([pd.Series(tokens_d), feat_prob, feat_prob_ratio_neg, feat_prob_ratio_pos], axis = 1) \n",
    "coef_tokens.columns = ['Token', 'Negative', \"Positive\", \"RatioNP\", 'RatioPN']\n",
    "\n",
    "\n",
    "#Sort\n",
    "\n",
    "coef_tokens['highest'] = abs(coef_tokens[[\"RatioNP\", \"RatioPN\"]]).max(axis=1)\n",
    "coef_high = coef_tokens.sort_values('highest', ascending = False)\n",
    "coef_high['color'] = (coef_high['RatioNP'] > coef_high['RatioPN']).replace([False, True], ['red', 'blue'])\n",
    "coef_high_sorted_neg = coef_tokens.sort_values(['RatioNP'], ascending = False)\n",
    "coef_high_sorted_pos = coef_tokens.sort_values(['RatioPN'], ascending = False)\n",
    "\n",
    "#Print\n",
    "print(coef_high.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = 20\n",
    "\n",
    "top = coef_high.iloc[:top_features].sort_values(['color','highest'], ascending = True)\n",
    "colors = top['color']\n",
    "\n",
    "barplot, ax = plt.subplots(figsize=(10,4))\n",
    "plt.tight_layout()\n",
    "plt.bar(np.arange(top_features), top.highest, color = colors)\n",
    "features = top.Token\n",
    "plt.xticks(np.arange(0, top_features), features, rotation=60, ha='right')\n",
    "\n",
    "ax.spines['top'].set_visible(False) #remove borders\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=12) #change size of tick labels\n",
    "ax.set_ylabel(\"Discrimination\", fontsize=16)\n",
    "\n",
    "print(features)\n",
    "plt.gcf().subplots_adjust(bottom=0.22, left = 0.1)\n",
    "\n",
    "plt.savefig('CoefficientsEB_d.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Implement a support vector classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we naïvely fit the support vector classifier to the training data. We choose a value of 5 for the regularization parameter `C`. This is actually the inverse of the regularization strength so a lower value means stronger regularization. We use the `l1` penalty because we want to have a sparse solution (we want to set some coefficients to 0) for interpretation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we naïvely fit the model:\n",
    "sentiment_svc_lib = svc_lib.fit(X_dev_vec_d, y_dev_d) \n",
    "\n",
    "#Investigate the accuracy of the training data\n",
    "print('Training Accuracy: ', A(sentiment_svc_lib, X_dev_vec_d, np.array(y_dev_d))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy value of the training data is very high and there might be a large difference with the test data. Perhaps we should fit a learning curve to investigate the bias and variance of the model and see whether we are over/under fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= svc_lib,\n",
    "                     X=X_dev_vec_d,\n",
    "                     y=y_dev_d,\n",
    "                     train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('This learning curve has too much variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the learning curve shows a lot of variance we want to see whether we can optimize the regularization parameter of the support vector classifier. In particular we should consider setting the regularization parameter to a more conservative (thus lower) value.  We do this by making a validation curve plotting the training and test accuracy for several value of `C`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.linspace(0.001, 5, 20)\n",
    "\n",
    "train_scores_val, test_scores_val = \\\n",
    "    validation_curve(estimator= svc_lib,\n",
    "                     X=X_dev_vec_d,\n",
    "                     y=y_dev_d,\n",
    "                     param_name='C',\n",
    "                     param_range=Cs,\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_val.mean(axis=1),\n",
    "                     'Test':test_scores_val.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(Cs,name='regularization'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(Cs,\n",
    "                train_scores_val.mean(1) + train_scores_val.std(1)*1.96,\n",
    "                train_scores_val.mean(1) - train_scores_val.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the average prediction accuracy (between training and test set) starts decreasing steeply after a `C` value of 1.5. Lets plot a more detailed validation curve for values lower than 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.linspace(0.001, 1.5, 20)\n",
    "\n",
    "train_scores_val, test_scores_val = \\\n",
    "    validation_curve(estimator= svc_lib,\n",
    "                     X=X_dev_vec_d,\n",
    "                     y=y_dev_d,\n",
    "                     param_name='C',\n",
    "                     param_range=Cs,\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_val.mean(axis=1),\n",
    "                     'Test':test_scores_val.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(Cs,name='regularization'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(Cs,\n",
    "                train_scores_val.mean(1) + train_scores_val.std(1)*1.96,\n",
    "                train_scores_val.mean(1) - train_scores_val.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We select a regularization setting `C` = 0.25 for which the the test accuracy is still within the confidence band of the training accuracy to see whether the learning curves improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_mod = LinearSVC(penalty = 'l1', C = 0.25, random_state = 1, dual = False) #moderate regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= svc_mod,\n",
    "                     X=X_dev_vec_d,\n",
    "                     y=y_dev_d,\n",
    "                     train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('Seems ok for our sample size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit final support vector classification model to training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the final model, with `C` = 0.25 to our training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_svc_mod = svc_mod.fit(X_dev_vec_d, y_dev_d)\n",
    "print('Training Accuracy: ', A(sentiment_svc_mod, X_dev_vec_d, y_dev_d))\n",
    "print('Test Accuracy: ', A(sentiment_svc_mod, X_test_vec_d, y_test_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and test accuracy are now much lower and closer together. We can now investigate the estimated coeficients to the test data to see which tokens predict positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put tokens and coefficients in one dataframe\n",
    "coef_tokens = pd.concat([pd.Series(tokens_d),\n",
    "                         pd.DataFrame(sentiment_svc_mod.coef_.T),\n",
    "                         abs(pd.DataFrame(sentiment_svc_mod.coef_.T))], axis = 1) \n",
    "coef_tokens.columns = ['Token', 'Coef', 'absCoef']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Count the amount of tokens with non-zero coefficient value\n",
    "print((coef_tokens['Coef'] != 0).value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_tokens = coef_tokens[coef_tokens.Coef != 0] #remove tokens with zero coefficient value\n",
    "coef_tokens_sorted = coef_tokens.sort_values('absCoef', ascending = False)#sort and print to get highest and lowest coefficients\n",
    "print(coef_tokens_sorted[['Token', 'Coef', 'absCoef']]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = 10\n",
    "\n",
    "top = coef_tokens_sorted.iloc[:20]\n",
    "print(top)\n",
    "top_sorted = top.sort_values('Coef')\n",
    "print(top_sorted)\n",
    "colors = ['red' if c > 0 else 'blue' for c in top_sorted['Coef']]\n",
    "\n",
    "\n",
    "barplot, ax = plt.subplots(figsize=(10,4))\n",
    "plt.tight_layout()\n",
    "plt.bar(np.arange(2 * top_features), top_sorted['Coef'], color=colors)\n",
    "features = np.array(top_sorted['Token'])\n",
    "plt.xticks(np.arange(0, 2 * top_features), top_sorted['Token'], rotation=60, ha='right')\n",
    "plt.yticks([-4, -2, 0, 2, 4])\n",
    "ax.spines['top'].set_visible(False) #remove borders\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "#plt.tick_params(top='off', right='off')#control tick marks\n",
    "ax.axhline(y=0, xmin=0, xmax=11, color = 'black', linestyle = 'dashed', linewidth = 1)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12) #change size of tick labels\n",
    "ax.set_ylabel(\"Coefficient\", fontsize=16)\n",
    "plt.gcf().subplots_adjust(bottom=0.22, left = 0.1)\n",
    "\n",
    "plt.savefig('CoefficientsSVC_d.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Classification after September 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Implement the logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we naïvely fit the logistic regression classifier to the training data. We choose a value of 5 for the regularization parameter `C`. This is actually the inverse of the regularization strength so a lower value means stronger regularization. We use the `l1` penalty because we want to have a sparse solution (we want to set some coefficients to 0) for interpretation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we naïvely fit the model:\n",
    "sentiment_logreg_lib = logreg_lib.fit(X_dev_vec_a, y_dev_a) \n",
    "\n",
    "#Investigate the accuracy of the training data\n",
    "print('Training Accuracy: ', A(sentiment_logreg_lib, X_dev_vec_a, np.array(y_dev_a))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy value of the training data is very high and there might be a large difference with the test data. Perhaps we should fit a learning curve to investigate the bias and variance of the model and see whether we are over/under fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= logreg_lib,\n",
    "                   X=X_dev_vec_a,\n",
    "                   y=y_dev_a,\n",
    "                   train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                   scoring=A,                 \n",
    "                   cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('This learning curve shows too much variance:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the learning curve shows a lot of variance we want to see whether we can optimize the regularization parameter of the logistic regression model. In particular we should consider setting the regularization parameter to a more conservative (thus lower) value.  We do this by making a validation curve plotting the training and test accuracy for several value of `C`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "Cs = np.linspace(0.001, 5, 20)\n",
    "\n",
    "train_scores_val, test_scores_val = \\\n",
    "    validation_curve(estimator= logreg_lib,\n",
    "                     X=X_dev_vec_a,\n",
    "                     y=y_dev_a,\n",
    "                     param_name='C',\n",
    "                     param_range=Cs,\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_val.mean(axis=1),\n",
    "                     'Test':test_scores_val.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(Cs,name='regularization'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(Cs,\n",
    "                train_scores_val.mean(1) + train_scores_val.std(1)*1.96,\n",
    "                train_scores_val.mean(1) - train_scores_val.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the average prediction accuracy (between training and test set) starts decreasing steeply after a `C` value of 1. Lets plot a more detailed validation curve for values lower than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.linspace(0.001, 1, 20)\n",
    "\n",
    "train_scores_val, test_scores_val = \\\n",
    "    validation_curve(estimator= logreg_lib,\n",
    "                     X=X_dev_vec_a,\n",
    "                     y=y_dev_a,\n",
    "                     param_name='C',\n",
    "                     param_range=Cs,\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_val.mean(axis=1),\n",
    "                     'Test':test_scores_val.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(Cs,name='regularization'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(Cs,\n",
    "                train_scores_val.mean(1) + train_scores_val.std(1)*1.96,\n",
    "                train_scores_val.mean(1) - train_scores_val.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a regularization setting for which the test accuracy `C` = 0.6 is still within the confidence bands of the training accuracy to see whether the learning curves improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_mod = LogisticRegression(penalty = 'l1', C = 0.6, random_state = 1) #moderate regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= logreg_mod,\n",
    "                   X=X_dev_vec_a,\n",
    "                   y=y_dev_a,\n",
    "                   train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                   scoring=A,                 \n",
    "                   cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size')) \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('This learning curve looks ok regarding the size of our training data (+/- 1000):')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit final logistic regression classification model to training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the final model, with `C` = 0.6 to our training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_logreg_mod = logreg_mod.fit(X_dev_vec_a, y_dev_a)\n",
    "print('Training Accuracy: ', A(sentiment_logreg_mod, X_dev_vec_a, y_dev_a))\n",
    "print('Test Accuracy: ', A(sentiment_logreg_mod, X_test_vec_a, y_test_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is much lower and it is reasonably close to the test accuracy. We can now investigate the estimated coeficients to the test data to see which tokens predict positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put tokens, coefficients and odds ratios in one dataframe\n",
    "coef_tokens = pd.concat([pd.Series(tokens_a),\n",
    "                         pd.DataFrame(sentiment_logreg_mod.coef_.T),\n",
    "                         abs(pd.DataFrame(sentiment_logreg_mod.coef_.T)),\n",
    "                         pd.DataFrame(np.exp(sentiment_logreg_mod.coef_).T)], axis = 1) \n",
    "coef_tokens.columns = ['Token', 'Coef', 'absCoef', 'OR']\n",
    "\n",
    "#Count the amount of tokens with non-zero coefficient value\n",
    "print((coef_tokens['Coef'] != 0).value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef_tokens = coef_tokens[coef_tokens.Coef != 0] #remove tokens with zero coefficient value and exponentiate to get odds ratio\n",
    "coef_tokens_sorted = coef_tokens.sort_values('absCoef', ascending = False)\n",
    "print(coef_tokens_sorted[['Token', 'Coef','absCoef', 'OR']]) #sort and print to get highest and lowest coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = 10\n",
    "\n",
    "top = coef_tokens_sorted.iloc[:20]\n",
    "print(top)\n",
    "top_sorted = top.sort_values('Coef')\n",
    "print(top_sorted)\n",
    "colors = ['red' if c > 0 else 'blue' for c in top_sorted['Coef']]\n",
    "\n",
    "\n",
    "barplot, ax = plt.subplots(figsize=(10,4))\n",
    "plt.tight_layout()\n",
    "plt.bar(np.arange(2 * top_features), top_sorted['Coef'], color=colors)\n",
    "features = np.array(top_sorted['Token'])\n",
    "plt.xticks(np.arange(0, 2 * top_features), top_sorted['Token'], rotation=60, ha='right')\n",
    "plt.yticks([-10, -5, 0, 5, 10])\n",
    "ax.spines['top'].set_visible(False) #remove borders\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "#plt.tick_params(top='off', right='off')#control tick marks\n",
    "ax.axhline(y=0, xmin=0, xmax=11, color = 'black', linestyle = 'dashed', linewidth = 1)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12) #change size of tick labels\n",
    "ax.set_ylabel(\"Coefficient\", fontsize=16)\n",
    "plt.gcf().subplots_adjust(bottom=0.22, left = 0.1)\n",
    "plt.savefig('CoefficientsLogistic_a.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Implement the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we naïvely fit the Naive Bayes classifier to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_NB = NB.fit(X_dev_vec_a.toarray(), y_dev_a) #need to convert to np array, not sparse\n",
    "\n",
    "print('Training Accuracy: ', A(sentiment_NB, X_dev_vec_a.toarray(), np.array(y_dev_a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy value of the training data is low. Perhaps we should fit a learning curve to investigate the bias and variance of the model and see whether we are over/under fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= NB,\n",
    "                   X=X_dev_vec_a.toarray(),\n",
    "                   y=y_dev_a,\n",
    "                   train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                   scoring=A,                 \n",
    "                   cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('There is too much bias and variance according to this learning curve:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the learning curve shows a lot of bias we want to see whether we can optimize the prior of the Naive Bayes classifier. This may be a controversial idea. Ideally, when we fit Bayesian models we need to specify a prior, that reflects our belief about the 'truth', in this case the proportion of negative and positive sentiments, before we fit a model. These prior probabilities could for example be derived from our knowledge of the proportions in the wider population, perhaps gained from  the literature. It would thus be odd to optimize a model based on the prior. We may also consider setting the prior to the proportions from the training set; this is what is done by default in the Naive Bayes classifier from sklearn. According to http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.59.2085&rep=rep1&type=pdf putting a uniform prior may actually improve performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_unif = BernoulliNB(class_prior = [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= NB_unif,\n",
    "                   X=X_dev_vec_a.toarray(),\n",
    "                   y=y_dev_a,\n",
    "                   train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                   scoring=A,                 \n",
    "                   cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('This looks very much like the curve above:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since adapting the prior does not see to change the learning curve a lot we could try to optimize the smoothing parameter `alpha`. This parameter determines the smooting of the prior and accounts for features not present in the learning samples and prevents zero probabilities in further computations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(0.001, 5, 20)\n",
    "\n",
    "train_scores_val, test_scores_val = \\\n",
    "    validation_curve(estimator= NB,\n",
    "                     X=X_dev_vec_a,\n",
    "                     y=y_dev_a,\n",
    "                     param_name='alpha',\n",
    "                     param_range=alphas,\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_val.mean(axis=1),\n",
    "                     'Test':test_scores_val.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(alphas,name='smoothing'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(alphas,\n",
    "                train_scores_val.mean(1) + train_scores_val.std(1)*1.96,\n",
    "                train_scores_val.mean(1) - train_scores_val.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "print('There is a decrease in bias for lower alpha values, \\nthe variance however becomes larger:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  set the `alpha` parameter to 3 for the best variance-bias tradeoff.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the final naive Bayes classifier to the training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the final model to our training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_adj_alpha = BernoulliNB(alpha = 3)\n",
    "\n",
    "sentiment_NB_adj = NB_adj_alpha.fit(X_dev_vec_a.toarray(), np.array(y_dev_a)) \n",
    "\n",
    "print('Training Accuracy: ', A(sentiment_NB_adj, X_dev_vec_a.toarray(), np.array(y_dev_a)))\n",
    "print('Test Accuracy: ', A(sentiment_NB_adj, X_test_vec_a.toarray(), np.array(y_test_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put tokens and coefficients in one dataframe\n",
    "feat_prob = pd.DataFrame(np.exp(sentiment_NB.feature_log_prob_).T)\n",
    "feat_prob.columns = ['Negative', \"Positive\"]\n",
    "\n",
    "#compute the ratio of probability of being predicted as a negative vs positive sentiment divided by the sum of probabilities\n",
    "feat_prob_ratio_neg = (feat_prob['Negative']/feat_prob['Positive'])*(feat_prob['Negative'])\n",
    "feat_prob_ratio_pos = (feat_prob['Positive']/feat_prob['Negative'])*(feat_prob['Positive'])\n",
    "\n",
    "coef_tokens = pd.concat([pd.Series(tokens_b), feat_prob, feat_prob_ratio_neg, feat_prob_ratio_pos], axis = 1) \n",
    "coef_tokens.columns = ['Token', 'Negative', \"Positive\", \"RatioNP\", 'RatioPN']\n",
    "\n",
    "\n",
    "#Sort\n",
    "\n",
    "coef_tokens['highest'] = abs(coef_tokens[[\"RatioNP\", \"RatioPN\"]]).max(axis=1)\n",
    "coef_high = coef_tokens.sort_values('highest', ascending = False)\n",
    "coef_high['color'] = (coef_high['RatioNP'] > coef_high['RatioPN']).replace([False, True], ['red', 'blue'])\n",
    "coef_high_sorted_neg = coef_tokens.sort_values(['RatioNP'], ascending = False)\n",
    "coef_high_sorted_pos = coef_tokens.sort_values(['RatioPN'], ascending = False)\n",
    "\n",
    "#Print\n",
    "print(coef_high.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = 20\n",
    "\n",
    "top = coef_high.iloc[:top_features].sort_values(['RatioNP'], ascending = False)\n",
    "colors = top['color']\n",
    "\n",
    "barplot, ax = plt.subplots(figsize=(10,4))\n",
    "plt.tight_layout()\n",
    "plt.bar(np.arange(top_features), top.highest, color = colors)\n",
    "features = top.Token\n",
    "plt.xticks(np.arange(0, top_features), features, rotation=60, ha='right')\n",
    "\n",
    "ax.spines['top'].set_visible(False) #remove borders\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=12) #change size of tick labels\n",
    "ax.set_ylabel(\"Discrimination\", fontsize=16)\n",
    "\n",
    "print(features)\n",
    "plt.gcf().subplots_adjust(bottom=0.22, left = 0.1)\n",
    "plt.savefig('CoefficientsEB_a.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Implement a support vector classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we naïvely fit the support vector classifier to the training data. We choose a value of 5 for the regularization parameter `C`. This is actually the inverse of the regularization strength so a lower value means stronger regularization. We use the `l1` penalty because we want to have a sparse solution (we want to set some coefficients to 0) for interpretation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we naïvely fit the model:\n",
    "sentiment_svc_lib = svc_lib.fit(X_dev_vec_a, y_dev_a) \n",
    "\n",
    "#Investigate the accuracy of the training data\n",
    "print('Training Accuracy: ', A(sentiment_svc_lib, X_dev_vec_a, np.array(y_dev_a))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy value of the training data is very high and theremight be a large difference with the test data. Perhaps we should fit a learning curve to investigate the bias and variance of the model and see whether we are over/under fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= svc_lib,\n",
    "                     X=X_dev_vec_a,\n",
    "                     y=y_dev_a,\n",
    "                     train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('This learning plot has too much variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the learning curve shows a lot of variance we want to see whether we can optimize the regularization parameter of the support vector classifier. In particular we should consider setting the regularization parameter to a more conservative (thus lower) value.  We do this by making a validation curve plotting the training and test accuracy for several value of `C`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.linspace(0.001, 5, 20)\n",
    "\n",
    "train_scores_val, test_scores_val = \\\n",
    "    validation_curve(estimator= svc_lib,\n",
    "                     X=X_dev_vec_a,\n",
    "                     y=y_dev_a,\n",
    "                     param_name='C',\n",
    "                     param_range=Cs,\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_val.mean(axis=1),\n",
    "                     'Test':test_scores_val.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(Cs,name='regularization'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(Cs,\n",
    "                train_scores_val.mean(1) + train_scores_val.std(1)*1.96,\n",
    "                train_scores_val.mean(1) - train_scores_val.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the average prediction accuracy (between training and test set) starts decreasing steeply after a `C` value of 0.3. Lets plot a more detailed validation curve for values lower than 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.linspace(0.001, 0.3, 20)\n",
    "\n",
    "train_scores_val, test_scores_val = \\\n",
    "    validation_curve(estimator= svc_lib,\n",
    "                     X=X_dev_vec_a,\n",
    "                     y=y_dev_a,\n",
    "                     param_name='C',\n",
    "                     param_range=Cs,\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_val.mean(axis=1),\n",
    "                     'Test':test_scores_val.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(Cs,name='regularization'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(Cs,\n",
    "                train_scores_val.mean(1) + train_scores_val.std(1)*1.96,\n",
    "                train_scores_val.mean(1) - train_scores_val.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a regularization setting `C` = 0.10 for which the test accuracy is still within the confidence bands of the training accuracy to see whether the learning curves improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_mod = LinearSVC(penalty = 'l1', C = 0.10, random_state = 1, dual = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_learn, train_scores_learn, test_scores_learn = \\\n",
    "    learning_curve(estimator= svc_mod,\n",
    "                     X=X_dev_vec_a,\n",
    "                     y=y_dev_a,\n",
    "                     train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                     scoring=A,                 \n",
    "                     cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = pd.DataFrame({'Train':train_scores_learn.mean(axis=1),\n",
    "                     'Test':test_scores_learn.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes_learn,name='sample size'))    \n",
    "print(acc_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_learn, ax = plt.subplots(figsize=(10,4))\n",
    "acc_.plot(ax=ax)\n",
    "ax.fill_between(train_sizes_learn,\n",
    "                train_scores_learn.mean(1) + train_scores_learn.std(1)*1.96,\n",
    "                train_scores_learn.mean(1) - train_scores_learn.std(1)*1.96, \n",
    "                alpha=0.25, \n",
    "                color='blue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "print('Seems ok for our training size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit final support vector classification model to training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the final model, with `C` = 0.10 to our training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_svc_mod = svc_mod.fit(X_dev_vec_a, y_dev_a)\n",
    "print('Training Accuracy: ', A(sentiment_svc_mod, X_dev_vec_a, y_dev_a))\n",
    "print('Test Accuracy: ', A(sentiment_svc_mod, X_test_vec_a, y_test_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and test accuracy are now much lower and closer together. We can now investigate the estimated coeficients to the test data to see which tokens predict positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put tokens and coefficients in one dataframe\n",
    "coef_tokens = pd.concat([pd.Series(tokens_a), pd.DataFrame(sentiment_svc_mod.coef_.T)], axis = 1) \n",
    "coef_tokens.columns = ['Token', 'Coef']\n",
    "\n",
    "#Count the amount of tokens with non-zero coefficient value\n",
    "print((coef_tokens['Coef'] != 0).value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_tokens = coef_tokens[coef_tokens.Coef != 0] #remove tokens with zero coefficient value\n",
    "coef_tokens_sorted = coef_tokens.sort_values('Coef')#sort and print to get highest and lowest coefficients\n",
    "print(coef_tokens_sorted[['Token', 'Coef']]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = 14\n",
    "top_features_p = 5\n",
    "top_features_n = 9\n",
    "\n",
    "top_pos_coef = coef_tokens_sorted.iloc[-top_features_p:]\n",
    "top_neg_coef = coef_tokens_sorted.iloc[:top_features_n]\n",
    "top = pd.concat([top_pos_coef, top_neg_coef], axis = 0).sort_values('Coef')\n",
    "print(top)\n",
    "colors = ['red' if c > 0 else 'blue' for c in top['Coef']]\n",
    "\n",
    "\n",
    "barplot, ax = plt.subplots(figsize=(10,4))\n",
    "plt.tight_layout()\n",
    "plt.bar(np.arange(top_features), top['Coef'], color=colors)\n",
    "features = np.array(top['Token'])\n",
    "plt.xticks(np.arange(0, top_features), top['Token'], rotation=60, ha='right')\n",
    "plt.yticks([ -4, -2, 0, 2, 4])\n",
    "ax.spines['top'].set_visible(False) #remove borders\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "#plt.tick_params(top='off', right='off')#control tick marks\n",
    "ax.axhline(y=0, xmin=0, xmax=20, color = 'black', linestyle = 'dashed', linewidth = 1)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12) #change size of tick labels\n",
    "ax.set_ylabel(\"Vector Coordinate\", fontsize=16)\n",
    "\n",
    "plt.gcf().subplots_adjust(bottom=0.22, left = 0.1)\n",
    "plt.savefig('CoefficientsSVC_a.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
